"Title","Authors","PublicationName","AlternateName","Abstract","Url","AuthorAddress","Year","DOI","ReferenceType","Keywords","PdfRelativePath","CustomId"
"Data standards can boost metabolomics research and if there is a will there is a way","Rocca-Serra P., Salek R. M., Arita M., Correa E., Dayalan S., Gonzalez-Beltran A., Ebbels T., Goodacre R., Hastings J., Haug K., Koulman A., Nikolski M., Oresic M., Sansone S. A., Schober D., Smith J., Steinbeck C., Viant M. R., Neumann S.","Metabolomics","","Thousands of articles using metabolomics approaches are published every year. With the increasing amounts of data being produced mere description of investigations as text in manuscripts is not sufficient to enable re-use anymore: the underlying data needs to be published together with the findings in the literature to maximise the benefit from public and private expenditure and to take advantage of an enormous opportunity to improve scientific reproducibility in metabolomics and cognate disciplines. Reporting recommendations in metabolomics started to emerge about a decade ago and were mostly concerned with inventories of the information that had to be reported in the literature for consistency. In recent years metabolomics data standards have developed extensively to include the primary research data derived results and the experimental description and importantly the metadata in a machine-readable way. This includes vendor independent data standards such as mzML for mass spectrometry and nmrML for NMR raw data that have both enabled the development of advanced data processing algorithms by the scientific community. Standards such as ISA-Tab cover essential metadata including the experimental design the applied protocols association between samples data files and the experimental factors for further statistical analysis. Altogether they pave the way for both reproducible research and data reuse including meta-analyses. Further incentives to prepare standards compliant data sets include new opportunities to publish data sets but also require a little ""arm twisting"" in the author guidelines of scientific journals to submit the data sets to public repositories such as the NIH Metabolomics Workbench or MetaboLights at EMBL-EBI. In the present article we look at standards for data sharing investigate their impact in metabolomics and give suggestions to improve their adoption. Copyright © 2015 The Author(s).","","","2016","10.1007/s11306-015-0879-3","","","pubmed-26612985.pdf","pubmed-26612985"
"Reporting and Guidelines in Propensity Score Analysis: A Systematic Review of Cancer and Cancer Surgical Studies","Yao X. I., Wang X., Speicher P. J., Hwang E. S., Cheng P., Harpole D. H., Berry M. F., Schrag D., Pang H. H.","Journal of the National Cancer Institute","","Background: Propensity score (PS) analysis is increasingly being used in observational studies especially in some cancer studies where random assignment is not feasible. This systematic review evaluates the use and reporting quality of PS analysis in oncology studies. Methods: We searched PubMed to identify the use of PS methods in cancer studies (CS) and cancer surgical studies (CSS) in major medical cancer and surgical journals over time and critically evaluated 33 CS published in top medical and cancer journals in 2014 and 2015 and 306 CSS published up to November 26 2015 without earlier date limits. The quality of reporting in PS analysis was evaluated. It was also compared over time and among journals with differing impact factors. All statistical tests were two-sided. Results: More than 50% of the publications with PS analysis from the past decade occurred within the past two years. Of the studies critically evaluated a considerable proportion did not clearly provide the variables used to estimate PS (CS 12.1% CSS 8.8%) incorrectly included non baseline variables (CS 3.4% CSS 9.3%) neglected the comparison of baseline characteristics (CS 21.9% CSS 15.6%) or did not report the matching algorithm utilized (CS 19.0% CSS 36.1%). In CSS the reporting of the matching algorithm improved in 2014 and 2015 ( P = .04) and the reporting of variables used to estimate PS was better in top surgery journals ( P = .008). However there were no statistically significant differences for the inclusion of non baseline variables and reporting of comparability of baseline characteristics. Conclusions: The use of PS in cancer studies has dramatically increased recently but there is substantial room for improvement in the quality of reporting even in top journals. Herein we have proposed reporting guidelines for PS analyses that are broadly applicable to different areas of medical research that will allow better evaluation and comparison across studies applying this approach.","","","2017","10.1093/jnci/djw323","","","medline-28376195.pdf","medline-28376195"
"A Lens for Evaluating Genetic Information Governance Models: Balancing Equity Efficiency and Sustainability","Skorve E., Vassilakopoulou P., Aanestad M., Grunfeld T.","Studies in Health Technology & Informatics","","This paper draws from the literature on collective action and the governance of the commons to address the governance of genetic data on variants of specific genes. Specifically the data arrangements under study relate to the BRCA genes (BRCA1 and BRCA2) which are linked to breast and ovarian cancer. These data are stored in global genetic data repositories and accessed by researchers and clinicians from both public and private institutions. The current BRCA data arrangements are fragmented and politicized as there are multiple tensions around data ownership and sharing. Three key principles are proposed for forming and evaluating data governance arrangements in the field. These principles are: equity efficiency and sustainability.","","","2017","","","","medline-28423802.pdf","medline-28423802"
"Multibac: a strategy to remove batch effects between different omic data types","Ugidos M. And Tarazona S. And Prats-Montalban J.m. And Ferrer A. And Conesa A.","Stat. Methods Med. Res","","Diversity of omic technologies has expanded in the last years together with the number of omic data integration strategies. However, multiomic data generation is costly, and many research groups cannot afford research projects where many different omic techniques are generated, at least at the same time. As most researchers share their data in public repositories, different omic datasets of the same biological system obtained at different labs can be combined to construct a multiomic study. However, data obtained at different labs or moments in time are typically subjected to batch effects that need to be removed for successful data integration. While there are methods to correct batch effects on the same data types obtained in different studies, they cannot be applied to correct lab or batch effects across omics. This impairs multiomic meta-analysis. Fortunately, in many cases, at least one omics platform-i.e. gene expression- is repeatedly measured across labs, together with the additional omic modalities that are specific to each study. This creates an opportunity for batch analysis. We have developed multibac (multiomic multiomics batch-effect correction correction), a strategy to correct batch effects from multiomic datasets distributed across different labs or data acquisition events. Our strategy is based on the existence of at least one shared data type which allows data prediction across omics. We validate this approach both on simulated data and on a case where the multiomic design is fully shared by two labs, hence batch effect correction within the same omic modality using traditional methods can be compared with the multibac correction across data types. Finally, we apply multibac to a true multiomic data integration problem to show that we are able to improve the detection of meaningful biological effects.copyright © the author(s) 2020.","","","2020","10.1177/0962280220907365","","","embase-2004451351.pdf","embase-2004451351"
"Social support and caregiver distress: a replication analysis","Miller B., Townsend A., Carpenter E., Montgomery R. V., Stull D., Young R. F.","Journals of Gerontology Series B-Psychological Sciences & Social Sciences","","OBJECTIVES: Prior studies have conceptualized and operationalized social support in different ways making it difficult to determine if the inconsistencies in findings are due to differences in study design samples conceptualization or measurement. The present study examined the replicability of models of social support and caregiver distress across 4 community-based caregiving studies representative of many conducted in the past 10 years. The goal was to identify areas of consistency in findings across the data sets.\\\\\\\\rMETHODS: The authors analyzed 3 models specifying patterns of relationship between social support and depression (main effect mediation effect and moderation effect) separately within data sets using hierarchical ordinary least squares regression. Results were compared across data sets.\\\\\\\\rRESULTS: The replication analysis confirmed the robustness of behavior problems and caregiver health as important contributors to caregiver distress. Results of hypotheses examining the pattern of relationship between social support and distress were inconsistent however. Only 1 type of social support was associated with distress in the expected direction: Less emotional support was associated with higher levels of distress in 2 of the 4 data sets.\\\\\\\\rDISCUSSION: More complex theoretical models that incorporate common measures to represent the linkages between types of stressor types of support and their interactions are needed to foster replicability and generalizability of research results.","","","2001","10.1093/geronb/56.4.s249","","","medline-11445617.pdf","medline-11445617"
"Demonstrating Broadcast Aggregate Keys for Data Sharing in Cloud","Rakshitha K., Rao A. S., Sagar Y., Ramasubbareddy S.","Lecture Notes in Networks and Systems","","In recent years the major issue in cloud computing is providing privacy for getting to re-appropriating information put away on cloud. To store also to share information safely cryptosystem is utilized. In cryptosystem the client needs to scramble the data before securing data on cloud and after that decode the data to get to it. This errand can need numerous keys for information encode just like information decoding. But the problem arises with data integrity where data can be modified or the files can be replaced without the knowledge of the data owner. So to ensure the trustworthiness of the record the information proprietor figures the hash-based message authentication code (HMAC) signature on each encoded document. © 2020 Springer Nature Singapore Pte Ltd.","","","2020","10.1007/978-981-15-2043-3_23","","","scopus-2-s2.0-85081566882.pdf","scopus-2-s2.0-85081566882"
"Developing an extension of the right statement for clinical practice guidelines on acupuncture: right for acupuncture – a protocol","Tang, C. And Lu, L. And Duan, Y. And Zhang, Y. And Zhou, Q. And Luo, X. And Chen, Y. And Xu, N.","European Journal Of Integrative Medicine","","Introduction: the 2017 international standard for reporting items for practice guideline in healthcare (right) published reporting guidelines to enhance transparency and clarity in the process of developing clinical practice guidelines (cpgs). As there are some barriers in the applicability of these guidelines to acupuncture due to its specificity in terms of manipulations, locations and channels compared to other health care interventions, we aim to develop a specific reporting checklist for the development of cpgs on acupuncture. Methods: the study design will refer to the methodology recommended by the enhancing the quality and transparency of health research (equator) network and will be modified as appropriate. We will conduct a literature review and establish an international multidisciplinary team, including a development group, a delphi panellists group and an advisory group. We will run three rounds of modified delphi surveys, face-to-face consensus meetings, consultations with advisors, pilot tests of the draft list of reports and promotion of the checklist. We plan to update regularly. Results: the study is ongoing and there are no complete results. We will update it in time if there is any result. Conclusions: this work will be relevant to a broad range of cpgs addressing questions of reporting standards on cpgs on acupuncture, which will have the following advantages: 1) to provide regulations for guideline developers;  2) to obtain more precise and clear guidelines for readers and clinical practitioners;  and 3) to evaluate reporting quality of cpgs on acupuncture and improve transparency of research reports for editors and reviewers. © 2019","","","2019","10.1016/j.eujim.2019.04.002","","","scopus-2-s2.0-85066927237.pdf","scopus-2-s2.0-85066927237"
"Data availability improvement in peer-to-peer online social networks","Koohpar, F.k. And Fatemi, A. And Raji, F.","Iet Information Security","","One of the main challenges of centralised social networks is having a central provider that stores the data which imposes some limitations to preserve the privacy of users' data. However, one of the decentralised architectures is peer-to-peer network that every user takes the responsibility of storing and managing his/her data. Although the privacy of data is increased in these networks, authorised friends must have access to the shared data when the user is not online in the network. For this purpose, the user selects some friends and copies his/her data in their space. On the other hand, the amount of used space and the total number of replicas must be reduced as much as possible. In this study, the authors provide some solutions to reduce the amount of used space and the total number of replicas to increase data availability. In this way, they segment the user's data and consider the stability of copy-location, i.e. the selected friends who have a copy of the user's data. The performance evaluation of the proposed methods shows that they considerably reduce the amount of used space as well as the total number of replicas in comparison to other approaches. © the institution of engineering and technology 2020.","","","2020","10.1049/iet-ifs.2019.0363","","","scopus-2-s2.0-85083458400.pdf","scopus-2-s2.0-85083458400"
"The campbell collaboration: providing better evidence for a better world","Littell, J.h. And White, H.","Research On Social Work Practice","","In this article, we trace the development of the campbell collaboration and its renewed efforts to build a world library of accurate, synthesized evidence to inform policy and practice and improve human well-being worldwide. Campbell systematic reviews and related evidence synthesis products provide unbiased summaries of entire bodies of empirical evidence, making them uniquely useful sources of information for policy and practice. With recent changes in organizational structure and new leadership, the campbell collaboration is poised to dramatically increase the production, dissemination, and use of rigorous syntheses of research on social, economic, and behavioral interventions. Campbell provides opportunities for social work scholars, practitioners, and consumers to contribute to knowledge about the processes and outcomes of social, behavioral, and economic interventions. © 2017, © the author(s) 2017.","","","2018","10.1177/1049731517703748","","","scopus-2-s2.0-85038403943.pdf","scopus-2-s2.0-85038403943"
"Empirical research on organizational infrastructure model impact to spatial data sharing","Chao, H. And Chou, T.-Y.","Annals Of Gis","","There are many factors in various aspects that affect spatial data sharing (sds), such as organization, personnel, legal system, data use, technique, motives, cost, fairness, power balance and data standards. Early research had mostly dealt with personal experience, which lacked holistic thinking and resolving. In recent years, researchers who believed sds as a social phenomenon began study using social network analysis and observed the role played by organizational networks during the process of sds to explain how organizational networks affected sds. While there exist few studies on sds between units and persons within an organization, and past research lacked effective methods of message gathering and filtering from the targeted organizations, this study proposed the empirical method and dimensions, unlike before, to understand and promote sds from various aspects. After observation on organizational process of sds over a long period, this article gathered empirical data on the behaviours of sds in taichung city government over 17 years and explained the sharing structure model of spatial data in organizational network by social network analysis and graph theory. The importance of the connection between the centre node in such structural model and the routine affairs of the organization was elaborated. This study provided the empirical method and scientifically generated results. © 2013 taylor & francis group.","","","2013","10.1080/19475683.2013.843592","","","scopus-2-s2.0-84889680077.pdf","scopus-2-s2.0-84889680077"
"Mobile data sharing and high availability","Hongisto, M.","Vtt Tiedotteita - Valtion Teknillinen Tutkimuskeskus","","This work introduces a model for decentralised and platform-independent data sharing for highly dynamic networks. The goal is not to deliver a solution that can be considered similar to database systems. Communication and replication challenges are presented and examined from the perspective of nomadic computing. The aim has been to provide highly available data sharing over features over mobile distributed systems. The primary functionality of the data-sharing model introduced here has been validated with simulations. A fully functional data-sharing model has been implemented and tested in a simulated environment. A simulated environment was chosen over real network conditions in order to offer data distribution pattern tracking in several different network topologies and environments. This study provides understanding of the data sharing factors that are important in decentralised distribution models. These factors include location determination for replica storing and control message propagation over weak connections. Smart data distribution is crucial for weakly connected networks to offer access to shared resources. In heterogeneous environments, it is also necessary to consider possible transmission resource savings, and an idea for partitioning data into small elements is introduced. A data distribution algorithm based on this understanding is designed and evaluated. According to technology surveys and the evaluation of data sharing implementation, it is clear that the optimistic replication schema is the choice for data distribution over weakly connected networks. This requires conflict solving for eventual consistency, or any equivalent method to provide the correct operation for applications. A decentralised data management model is the best alternative to operate in weakly connected networks, as it is able to function regardless of network partitions. The data sharing approach presented here provides full adaptability to different network topologies and computing platforms, and it is able to offer data sharing services for any device to some degree. A decentralised data sharing approach introduces new challenges;  these are discussed, and their viability for implementation is estimated. Copyright © vtt 2002.","","","2002","","","","scopus-2-s2.0-84884717935.pdf","scopus-2-s2.0-84884717935"
"Reporting guidelines: Doing better for readers","Moher D.","BMC Medicine","","There is clear guidance on the responsibilities of editors to ensure that the research they publish is of the highest possible quality. Poor reporting is unethical and directly impacts patient care. Reporting guidelines are a relatively recent development to help improve the accuracy clarity and transparency of biomedical publications. They have caught on with hundreds of reporting guidelines now available. Some journals endorse reporting guidelines while a smaller number have used various approaches to implement them. Yet challenges remain - biomedical research is still not optimally reported despite the abundance of reporting guidelines. Electronic algorithms are now being developed to facilitate the choice of correct reporting guideline(s) while other tools are being integrated into journal editorial management processes. Universities need to consider whether it is responsible to advance careers of faculty based on poorly reported research which is of little societal value. If journals embraced auditing of the quality of articles they publish this would give them and their readers essential feedback from which to improve their product. Copyright © 2018 The Author(s).","","","2018","10.1186/s12916-018-1226-0","","","medline-30545364.pdf","medline-30545364"
"Participant acceptability of digital footprint data collection strategies: an exemplar approach to participant engagement and involvement in the alspac birth cohort study","Shiells, K. And Di Cara, N. And Skatova, A. And Davis, O.s.p. And Haworth, C.m.a. And Skinner, A.l. And Thomas, R. And Tanner, A.r. And Macleod, J. And Timpson, N.j. And Boyd, A.","International Journal Of Population Data Science","","Introduction digital footprint records - the tracks and traces amassed by individuals as a result of their interactions with the internet, digital devices and services - can provide ecologically valid data on individual behaviours. These could enhance longitudinal population study databanks;  but few uk longitudinal studies are attempting this. When using novel sources of data, study managers must engage with participants in order to develop ethical data processing frameworks that facilitate data sharing whilst safeguarding participant interests. Objectives this paper aims to summarise the participant involvement approach used by the alspac birth cohort study to inform the development of a framework for using linked participant digital footprint data, and provide an exemplar for other data linkage infrastructures. Methods the paper synthesises five qualitative forms of inquiry. Thematic analysis was used to code transcripts for common themes in relation to conditions associated with the acceptability of sharing digital footprint data for longitudinal research. Results we identified six themes: participant understanding;  sensitivity of location data;  concerns for third parties;  clarity on data granularity;  mechanisms of data sharing and consent;  and trustworthiness of the organisation. For cohort members to consider the sharing of digital footprint data acceptable, they require information about the value, validity and risks;  control over sharing elements of the data they consider sensitive;  appropriate mechanisms to authorise or object to their records being used;  and trust in the organisation. Conclusion realising the potential for using digital footprint records within longitudinal research will be subject to ensuring that this use of personal data is acceptable;  and that rigorously controlled population data science benefiting the public good is distinguishable from the misuse and lack of personal control of similar data within other settings. Participant co-development informs the ethical-governance framework for these novel linkages in a manner which is acceptable and does not undermine the role of the trusted data custodian. © the authors.","","","2020","10.23889/ijpds.v5i3.1728","","","scopus-2-s2.0-85128392043.pdf","scopus-2-s2.0-85128392043"
"Where are the scientific standards for high-quality replication studies?","Fiedler, Klaus","Psychologische Rundschau","","There is wide consensus that replication affords an important instrument for identifying valid findings and solid research approaches. However, if replication research serves a major scientific function, then it must be evaluated in terms of strict methodological rules and clearly articulated scientific criteria. A critical analysis of contemporary replication projects-such as the recently published report by the open science collaboration-reveals, however, that no logically sound methodology for state-of-the art replication research has been developed and applied so far. As a consequence, the validity of inferences drawn from many replication studies remains equivocal. Four aspects of this fundamental problem are discussed: uncertainty about the objective of replication (replicandum);  neglect of specific methodological problems (regressiveness;  reliability;  change measurement);  one-sided focus on the avoidance of allegedly expensive ""false-positives"" in the absence of any serious attempt to run a cost-benefit analysis;  and the sorely neglected goal of implementing excellent replication research that leads to new insights and genuine scientific progress. (Psycinfo database record (c) 2021 apa, all rights reserved) abstract (german) es gibt einen breiten konsens, dass replikation ein wichtiges instrument ist, um valide befunde und solide forschung zu erkennen. Wenn sie aber wissenschaftlich bedeutsam ist, dann muss auch die replikationsforschung an strengen methodischen regeln und an klar artikulierten wissenschaftlichen zielen gemessen werden. Eine kritische beschaftigung mit der aktuellen replikationsforschung-etwa im jungst veroffentlichten bericht der open science collaboration-zeigt jedoch, dass eine strenge und forschungslogisch begrundete methodologie fur replikationsstudien bislang weder angewandt noch entwickelt wurde. Infolgedessen bleibt die validitat der schlusse, die aus replikationsstudien gezogen werden durfen, oftmals unklar. Dieses grundlegende problem wird hier unter vier gesichtspunkten diskutiert: unklarheit des gegenstandes der replikation (replicandum), vernachlassigung einschlagiger methodischer probleme (regressivitat;  reliabilitat der veranderungsmessung), einseitige vermeidung von angeblich kostentrachtigen falsch-positiven"" ohne versuch einer systematischen kosten-nutzen-messung sowie das vernachlassigte ziel, replikationsforschung so zu implementieren, dass sie echte erkenntnisfortschritte bringt und als exzellente forschung anerkannt werden kann. (Psycinfo database record (c) 2021 apa, all rights reserved)","","","2018","10.1026/0033-3042/a000388","","","psychinfo-2018-01295-004.pdf","psychinfo-2018-01295-004"
"Accelerating drug development for Alzheimer's disease through the use of data standards","Neville J., Kopko S., Romero K., Corrigan B., Stafford B., LeRoy E., Broadbent S., Cisneroz M., Wilson E., Reiman E., Vanderstichele H., Arneric S. P., Stephenson D.","Alzheimer's & Dementia : Translational Research & Clinical Interventions","","INTRODUCTION: The exceedingly high rate of failed trials in Alzheimer's disease (AD) calls for immediate attention to improve efficiencies and learning from past ongoing and future trials. Accurate highly rigorous standardized data are at the core of meaningful scientific research. Data standards allow for proper integration of clinical data sets and represent the essential foundation for regulatory endorsement of drug development tools. Such tools increase the potential for success and accuracy of trial results. METHODS: The development of the Clinical Data Interchange Standards Consortium (CDISC) AD therapeutic area data standard was a comprehensive collaborative effort by CDISC and Coalition Against Major Diseases a consortium of the Critical Path Institute. Clinical concepts for AD and mild cognitive impairment were defined and a data standards user guide was created from various sources of input including data dictionaries used in AD clinical trials and observational studies. RESULTS: A comprehensive collection of AD-specific clinical data standards consisting of clinical outcome measures leading candidate genes and cerebrospinal fluid and imaging biomarkers was developed. The AD version 2.0 (V2.0) Therapeutic Area User Guide was developed by diverse experts working with data scientists across multiple consortia through a comprehensive review and revision process. The AD CDISC standard is a publicly available resource to facilitate widespread use and implementation. DISCUSSION: The AD CDISC V2.0 data standard serves as a platform to catalyze reproducible research data integration and efficiencies in clinical trials. It allows for the mapping and integration of available data and provides a foundation for future studies data sharing and long-term registries in AD. The availability of consensus data standards for AD has the potential to facilitate clinical trial initiation and increase sharing and aggregation of data across observational studies and among clinical trials thereby improving our understanding of disease progression and treatment.","","","2017","10.1016/j.trci.2017.03.006","","","pubmed-29067333.pdf","pubmed-29067333"
"Perspectives regarding privacy in clinical research among research professionals from the arab region: an exploratory qualitative study","Adarmouch, L. And Felaefel, M. And Wachbroit, R. And Silverman, H.","Bmc Medical Ethics","","Background: protecting the privacy of research participants is widely recognized as one of the standard ethical requirements for clinical research. It is unknown, however, how research professionals regard concepts of privacy as well as the situations in the research setting that require privacy protections. The aim of this study was to explore the views of research professionals from arab countries regarding concepts and scope of privacy that occur in clinical research. Methods: we adopted an exploratory qualitative approach by the use of focus group discussions. We recruited individuals involved in research from egypt and morocco. We analyzed focus group data via a constant comparison approach, which consisted of close reading of the transcribed interviews followed by coding and then determining themes and subthemes. Results: between august 2016 and july 2018, we conducted nine focus group discussions. Respondents discussed several privacy issues that occurred before the research began (e.g., recruitment practices);  during research (e.g., data collection and physical exams), and after the research (e.g., secondary use of data and data sharing). Respondents revealed their perspectives of patients towards privacy in the clinical and research settings and mentioned that patients are more likely to permit access to their privacy in the clinical setting compared with research setting due to the existence of benefits and trust in clinical care. Respondents also recommended training regarding data protections for individuals involved in research. Conclusions: our study shows that research professionals discussed a range of privacy issues that are present during the different stages of research. We recommend 1) development of standards regarding privacy protections during recruitment efforts;  2) additional training for individuals involved in research regarding best practices with data security in secondary research;  3) a quantitative study involving investigators and rec members to determine their knowledge, attitudes and practices regarding privacy issues that occur in research;  and 4) a quantitative study involving patients to elicit their views regarding their privacy concerns in research. © 2020 the author(s).","","","2020","10.1186/s12910-020-0456-9","","","scopus-2-s2.0-85083412191.pdf","scopus-2-s2.0-85083412191"
"The impact of changing norms on creativity in psychological science","Wai, J. And Halpern, D.f.","Perspectives On Psychological Science","","The open science or credibility revolution has divided psychologists on whether and how the “policy” change of preregistration and similar requirements will affect the quality and creativity of future research. We provide a brief history of how norms have rapidly changed and how news and social media are beginning to “disrupt” academic science. We note a variety of benefits, including more confidence in research findings, but there are possible costs as well, including a reduction in the number of studies conducted because of an increased workload required by new policies. We begin to craft a study to evaluate the short- and long-term impacts of these changing norms on creativity in psychological science, run into some possible roadblocks, and hope others will build on this idea. This policy change can be evaluated in the short term but will ultimately need to be evaluated decades from now. Long-term evaluations are rare, yet this is the ultimate measure of creative scientific advance. Our conclusion supports the goals and procedures for creating a more open science. © 2018, the author(s) 2018.","","","2018","10.1177/1745691618773326","","","scopus-2-s2.0-85049892031.pdf","scopus-2-s2.0-85049892031"
"Quality of reporting for randomised clinical trials published in latin american and spanish journals: a protocol for a systematic survey of three clinical specialities","Bachelet, V.c. And Carrasco, V.a. And Bravo-Córdova, F. And Díaz, R.a. And Lizana, F.j. And Meza-Ducaud, N. And Pardo-Hernandez, H. And Uribe, F.a. And Vergara, A.f. And Villanueva, J. And Navarrete, M.s.","Bmj Open","","Introduction: quality of reporting refers to how published articles communicate how the research was done and what was found. Gaps and imprecisions of reporting hamper the assessment of the methodological quality and internal and external validity. The consolidated standards of reporting trials (consort) are a set of evidence-based recommendations of the minimum elements to be included in the reporting of randomised controlled trials (rcts) to ensure a complete and transparent account of what was done, how it was done and what was found. Few studies have been conducted on the impact of consort on rcts published in latin american and spanish journals. We aim to assess the reporting quality of rcts of three clinical specialities published in spanish and latin american journals, as well as to assess changes over time and associations of quality with journal and country indicators. Methods and analysis: we will conduct a systematic survey of all rcts published in spanish-language journals in three clinical fields (dentistry, neurology and geriatrics) from 1990 to 2018. We will include rcts from previous work that has identified all rcts on these medical fields published in spain and latin america. We will update this work via handsearching of relevant journals. Assessment of quality of reporting will be conducted independently and in duplicate using the consort 2010 statement. We will also extract journal and country indicators. We will conduct descriptive statistics and secondary analyses considering the year, country, and journal of publication, among others. Ethics and dissemination: the universidad de santiago de chile's ethics committee approved the protocol. We will disseminate the results of this work in peer-reviewed scientific journals and conference proceedings. We expect to raise awareness among researchers, journal editors and funders on the importance of training in reporting guidelines and using them from the inception of rct protocols. © author(s) (or their employer(s)) 2020. Re-use permitted under cc by-nc. No commercial re-use. See rights and permissions. Published by bmj.","","","2020","10.1136/bmjopen-2019-036148","","","scopus-2-s2.0-85086792110.pdf","scopus-2-s2.0-85086792110"
"Espacomp medication adherence reporting guideline (emerge)","De Geest, S. And Zullig, L.l. And Dunbar-Jacob, J. And Helmy, R. And Hughes, D.a. And Wilson, I.b. And Vrijens, B.","Annals Of Internal Medicine","","Research on assessing or managing medication adherence applies approaches from observational, interventional, and implementation science that spans many disciplines and demands coherent conceptualization, valid methods, appropriate analyses, and complete and accurate reporting. To ensure such reporting, the european society for patient adherence, compliance, and persistence (espacomp) medication adherence reporting guideline (emerge) recommends standard reporting approaches based on an accepted taxonomy. This guideline is derived from a literature review, a reactive delphi study with 26 medication adherence experts from many countries and disciplines, and feedback from espacomp members. It is designed to supplement existing guidelines for health research reporting and is structured around 4 minimum reporting criteria and 17 items reflecting best reporting practice. By enhancing and harmonizing research reporting, emerge aims to advance research and, ultimately, patient outcomes. © 2018 american college of physicians.","","","2018","10.7326/m18-0543","","","scopus-2-s2.0-85049756982.pdf","scopus-2-s2.0-85049756982"
"Comparison & magnitude credibility: whom to trust when reports are conflicting?","Zhou, S. And Zhang, H. And Shen, B.","Open Communication Journal","","This study used the concepts of comparison credibility and magnitude credibility to assess perceived news media credibility in china. It also investigated which sources people trusted more when they encountered conflicting reports regarding different kinds of stories including entertainment news, disaster news and political news. A random sample from three major metropolises (n = 1,844) were telephone interviewed. Results indicated that television was perceived as the most trustworthy. Regardless of the type of stories, people trusted national chinese media over other media outlets. Implications on credibility research are discussed. © zhou et al.;  Licensee bentham open.","","","2014","10.2174/1874916x01408010001","","","scopus-2-s2.0-84928717222.pdf","scopus-2-s2.0-84928717222"
"Measurements of alveolar bone height at tooth and implant abutments on intraoral radiographs. A comparison of reproducibility of Eggen technique utilized with and without a bite impression","Larheim T. A., Eggen S.","Journal of Clinical Periodontology","","A short review of various radiographic methods for quantitation of marginal bone changes is given. With the method suggested by Eggen (1973) alveolar bone heights on intraoral paralleling radiographs of tooth and implant abutments could be determined directly in ""true' values. A calibrated measuring film (increments adjusted in accordance with the approximately constant magnification) was utilized with a magnifying (x 2) viewer. The reproducibility of this direct method was tested with and without a bite impression and found to be high. The use of an impression did not seem to have any significant influence on the reproducibility of the method when measuring bone heights at teeth. However regarding bone height measurements at implants the impression seemed to bring about a small but significant improvement. The main source of error seemed to be the recognition of the reference points in the alveolar bone. The discussion of available quantitation principles indicated that radiographic bone height should be evaluated in absolute values in clinical studies of comparative nature. The present method seems to be suitable for comparative clinical examinations of alveolar bone height at teeth and implants.","","","1982","10.1111/j.1600-051x.1982.tb02058.x","","","medline-7047578.pdf","medline-7047578"
"Method for Data Quality Assessment of Synthetic Industrial Data","Iantovics L. B., Enachescu C.","Sensors","","Sometimes it is difficult or even impossible to acquire real data from sensors and machines that must be used in research. Such examples are the modern industrial platforms that frequently are reticent to share data. In such situations the only option is to work with synthetic data obtained by simulation. Regarding simulated data a limitation could consist in the fact that the data are not appropriate for research based on poor quality or limited quantity. In such cases the design of algorithms that are tested on that data does not give credible results. For avoiding such situations we consider that mathematically grounded data-quality assessments should be designed according to the specific type of problem that must be solved. In this paper we approach a multivariate type of prediction whose results finally can be used for binary classification. We propose the use of a mathematically grounded data-quality assessment which includes among other things the analysis of predictive power of independent variables used for prediction. We present the assumptions that should be passed by the synthetic data. Different threshold values are established by a human assessor. In the case of research data if all the assumptions pass then we can consider that the data are appropriate for research and can be applied by even using other methods for solving the same type of problem. The applied method finally delivers a classification table on which can be applied any indicators of performed classification quality such as sensitivity specificity accuracy F1 score area under curve (AUC) receiver operating characteristics (ROC) true skill statistics (TSS) and Kappa coefficient. These indicators' values offer the possibility of comparison of the results obtained by applying the considered method with results of any other method applied for solving the same type of problem. For evaluation and validation purposes we performed an experimental case study on a novel synthetic dataset provided by the well-known UCI data repository.","","","2022","10.3390/s22041608","","","medline-35214509.pdf","medline-35214509"
"How to improve the study design of clinical trials in internal medicine: recent advances in the evidence-based methodology","Lund H., Bala M., Blaine C., Brunnhuber K., Robinson K. A.","Polish Archives Of Internal Medicine","","Meta-research has highlighted that up to half of all clinical studies may be redundant and do not add any value. We suggest that such unnecessary studies will continue to be prepared and published unless researchers systematically and transparently identify and consider the existing evidence. This approach of identifying and utilizing the existing knowledge base before and after conducting a new trial is called Evidence-Based Research (EBR) defined as the use of prior research in a systematic and transparent way to inform a new study so that it is answering questions that matter in a valid efficient and accessible manner. This paper describes the issues that have led to the development of the EBR approach suggests what researchers should do to avoid wasteful and unnecessary research and outlines the benefits of conducting evidence-based research. Finally we present the international EBR Network established to support the efforts to minimize waste in research and increase the value of clinical studies.","","","2021","10.20452/pamw.16076","","","medline-34590450.pdf","medline-34590450"
"Insertion of fiocruz’s scientific production in initiatives to promote open access to research data in national and international journals","Martins, M.f.m. And Dos Santos, H.l.c. And Jorge, V.a. And De Oliveira, J.g.","Ciencia Da Informacao","","The study aimed to identify the inclusion of fiocruz’s scientific production in initiatives to promote open access to research data in national and international journals, from 2012 to 2018. It presents reflections on the alignment of editorial policies with international open science guidelines, in order to consider this indicator as a component of the evaluation and qualification of the application for good practices by researchers linked to fiocruz. To carry out the work and fulfill the objectives, a descriptive methodology was adopted, through systematic mapping. To this end, an exploratory research was conducted through bibliographic review, by affiliation, followed by documentary research on the website of the selected journals, which consisted of several steps, resulting in a list of 10 magazine titles for comparison and analysis, according to the ranking. generated by the scientific production indexed in the databases. The results showed that journals should be stimulated to meet the standards demanded by the open science movement, the limits and possibilities of establishing a policy for managing, opening and sharing research data for fiocruz, both as a producer and publisher. The theme needs further studies involving this theme. © 2019, brazilian institute for information in science and technology. All rights reserved.","","","2019","","","","scopus-2-s2.0-85082511666.pdf","scopus-2-s2.0-85082511666"
"Surrogate endpoints in trials: a call for better reporting","Ciani O., Manyara A. M., Chan A. W., Taylor R. S.","Trials [Electronic Resource]","","Using a surrogate endpoint as a substitute for a patient-relevant final outcome enables randomised controlled trials (RCTs) to be conducted more efficiently. However the use of surrogates remains controversial and there is currently no guideline for the reporting of RCTs using surrogate endpoints; therefore we seek to develop SPIRIT (Standard Protocol Items: Recommendations for Interventional Trials) and CONSORT (Consolidated Standards of Reporting Trials) extensions to improve the reporting of these trials. We would like to invite interested individuals (trial methodologists journal editors healthcare industry regulators and payers and patient/public representative groups) particularly those with experience in the use of surrogate endpoints in trials. Copyright © 2022. The Author(s).","","","2022","10.1186/s13063-022-06904-7","","","medline-36503559.pdf","medline-36503559"
"AERA Editorial Policies Regarding Statistical Significance Testing: Three Suggested Reforms","Thompson Bruce","Educational Researcher","","Reviews practices regarding tests of statistical significance and policies of the American Educational Research Association (AERA). Decades of misuse of statistical significance testing are described and revised editorial policies to improve practice are highlighted. Correct interpretation of statistical tests interpretation of effect sizes and exploration of research replicability are essential. (SLD)","","","1996","","","","eric-ej525478.pdf","eric-ej525478"
"The general theory of the quasi-reproducible experiments: how to describe the measured data of complex systems?","Nigmatullin, R.r. And Maione, G. And Lino, P. And Saponaro, F. And Zhang, W.","Communications In Nonlinear Science And Numerical Simulation","","In this paper, we suggest a general theory that enables to describe experiments associated with reproducible or quasi-reproducible data reflecting the dynamical and self-similar properties of a wide class of complex systems. Under complex system we understand a system when the model based on microscopic principles and suppositions about the nature of the matter is absent. This microscopic model is usually determined as ""the best fit"" model. The behavior of the complex system relatively to a control variable (time, frequency, wavelength, etc.) can be described in terms of the so-called intermediate model (im). One can prove that the fitting parameters of the im are associated with the amplitude-frequency response of the segment of the prony series. The segment of the prony series including the set of the decomposition coefficients and the set of the exponential functions (with k = 1,2,. . .,k) is limited by the final mode k. The exponential functions of this decomposition depend on time and are found by the original algorithm described in the paper. This approach serves as a logical continuation of the results obtained earlier in paper [nigmatullin rr, w. Zhang and striccoli d. General theory of experiment containing reproducible data: the reduction to an ideal experiment. Commun nonlinear sci numer simul, 27, (2015), pp 175-192] for reproducible experiments and includes the previous results as a partial case. In this paper, we consider a more complex case when the available data can create short samplings or exhibit some instability during the process of measurements. We give some justified evidences and conditions proving the validity of this theory for the description of a wide class of complex systems in terms of the reduced set of the fitting parameters belonging to the segment of the prony series. The elimination of uncontrollable factors expressed in the form of the apparatus function is discussed.to illustrate how to apply the theory and take advantage of its benefits, we consider the experimental data associated with typical working conditions of the injection system in a common rail diesel engine. In particular, the flow rate of the injected fuel is considered at different reference rail pressures. The measured data are treated by the proposed algorithm to verify the adherence to the proposed general theory. The obtained results demonstrate the undoubted effectiveness of the proposed theory. © 2016 elsevier b.v..","","","2017","10.1016/j.cnsns.2016.05.019","","","scopus-2-s2.0-84973531918.pdf","scopus-2-s2.0-84973531918"
"A secure protocol for sharing trust data in hybrid p2p network","Lin, H. And Zhou, Y.","Journal Of Networks","","The trust data is critical to the trust model of p2p system. In this paper we present an efficient certificateless cryptography scheme and propose a protocol which provides the ability for sharing trust data securely. The protocol avoids the escrow problem identity-based cryptosystem and the secure delivery of private keys. The security of scheme is based on some underlying problems closely related to the bilinear diffie-hellman problem are computationally hard. It tolerates the type i and type ii adversary. The proof of security is presented in the random oracle model. Through security discussion, we show that my secure protocol is extremely secure when encounter a variety of possible attacks. © 2011 academy publisher.","","","2011","10.4304/jnw.6.4.607-614","","","scopus-2-s2.0-79955495442.pdf","scopus-2-s2.0-79955495442"
"EUA Roadmap on Research Assessment in the Transition to Open Science","","European University Association","","The European University Association (EUA) has continuously supported European universities in the transition towards Open Science and in particular to Open Access. Upon recommendation of its Expert Group on Science 2.0/Open Science EUA has developed a variety of initiatives in this area as outlined in the ""EUA Roadmap on Open Access to Research Publications."" That roadmap mainly focused on introducing Open Access as the main model of accessing research publications. Looking ahead to the more global transition towards Open Science with its broader framework beyond accessing research publications the Expert Group is starting to address new models of research assessment and evaluation at all levels as these are instrumental to achieving a fairer more open and transparent system driven by researchers. The present EUA roadmap addresses a selection of topics related to new approaches in research assessment including the assessment of research outcomes researchers and research units and organisations (laboratories research centers and universities). With this document EUA aims to raise awareness and support institutions in the development of research assessment approaches that focus on research quality potential and future impact and that take into account Open Science practices. (ERIC)","","","2018","","","","unknown-1012.pdf","unknown-1012"
"Experimental design for gene expression analysis: Answers are easy is asking the right question difficult?","Fournier M. V., Carvalho P. C., Magee D. D., Da Carvalho M. G. C., Appasani K.","","","More and more array platforms are being used to assess gene expression in a wide range of biological and clinical models. Technologies using arrays have proven to be reliable and affordable for most of the scientific community worldwide. By typing microarrays or proteomics into a search engine such as PubMed thousands of references can be viewed. Nevertheless almost everyone in life science research has a story to tell about array experiments that were expensive did not generate reproducible data or generated meaningless data. Because considerable resources are required for any experiment using arrays it is desirable to evaluate the best method and the best design to ask a certain question. Multiple levels of technical problems such as sample preparation array spotting signal acquisition dye intensity bias normalization or sample-contamination can generate inconsistent results or misleading conclusions. Technical recommendations that offer alternatives and solutions for the most common problems have been discussed extensively in previous work. Less often discussed is the experimental design. A poor design can make array data analysis difficult even if there are no technical problems. This chapter focuses on experimental design choices in terms of controls such as replicates and comparisons for microarray and proteomics. It also covers data validation and provides examples of studies using diverse experimental designs. The overall emphasis is on design efficiency. Though perhaps obvious we also emphasize that design choices should be made so that biological questions are answered by clear data analysis. © 2007 Humana Press Inc.","","","2007","10.1007/978-1-59745-328-8_3","","","scopus-2-s2.0-67650653389.pdf","scopus-2-s2.0-67650653389"
"Electronic health record sharing scheme with searchable attribute-based encryption on blockchain","Niu, S. And Chen, L. And Wang, J. And Yu, F.","Ieee Access","","With the digitization of traditional medical records, medical institutions encounter difficult problems, such as electronic health record storage and sharing. Patients and doctors spend considerable time querying the required data when accessing electronic health records, but the obtained data are not necessarily correct, and access is sometimes restricted. On this basis, this study proposes a medical data sharing scheme based on permissioned blockchains, which use ciphertext-based attribute encryption to ensure data confidentiality and access control of medical data. Under premise of ensuring patient identity privacy, a polynomial equation is used to achieve an arbitrary connection of keywords, and then blockchain technology is combined. In addition, the proposed scheme has keyword-indistinguishability against adaptive chosen keyword attacks under the random oracle model. Analysis shows that the scheme has high retrieval efficiency. © 2013 ieee.","","","2020","10.1109/access.2019.2959044","","","scopus-2-s2.0-85078364018.pdf","scopus-2-s2.0-85078364018"
"Short-term reproducibility of ambulatory blood pressure measurements: a systematic review and meta-analysis of 35 observational studies","Bo Y., Kwok K. O., Chung V. C., Yu C. P., Tsoi K. K., Wong S. Y., Lee E. K.","Journal of Hypertension","","OBJECTIVE: A systematic review on the reproducibility of ambulatory blood pressure measurements (ABPM) has not yet been conducted. This meta-analysis compared 24-h/daytime/night-time SBP and DBP mean values and SBP/DBP nocturnal dipping status from ABPMs in participants with or without hypertension.\\\\\\\\rMETHODS: Ovid MEDLINE EMBASE and CINAHL Complete databases were searched for articles published before 3 May 2019. Eligible studies reporting a 24-h ABPM repeated at least once within 1 month were included. The mean daytime/night-time/24-h BP values percentage of nocturnal dipping and proportion of nondippers were compared between the first and second day of measurements and the proportion of participants with inconsistent dipping status were estimated using a random effect model.\\\\\\\\rRESULTS: Population-based analysis found a 0-1.1 mmHg difference between the first and second ABPM for 24-h/daytime/night-time SBP and DBP and 0-0.5% for percentage of SBP/DBP nocturnal dipping. The proportion of non-dippers was not different between the first and second ABPM. Intra-individual analysis found that the 95% limit of agreements (LOA) for SBP/DBP were wide and the 95% LOA for daytime SBP common reference to diagnose hypertension ranged -16.7 to 18.4 mmHg. Similarly 32% of participants had inconsistent nocturnal dipping status.\\\\\\\\rCONCLUSION: ABPM had excellent reproducibility at the population level favouring its application for research purposes; but reproducibility of intra-individual BP values and dipping status from a 24-h ABPM was limited. The available evidence was limited by the lack of high-quality studies and lack of studies in non-Western populations.","","","2020","10.1097/hjh.0000000000002522","","","medline-32555001.pdf","medline-32555001"
"Solving problems of disclosure risk in an academic setting: using a combination of restricted data and restricted access methods","Rodgers W., Nolte M.","Journal of Empirical Research on Human Research Ethics","","THE HEALTH AND RETIREMENT STUDY collects a vast amount of information about a sample of the U.S. population over age 50 from biennial interviews supplemental questionnaires and through linkages with administrative data including Social Security earnings and benefits records and Medicare claims records. To h onor i ts p ledge to t he r espondents that their data will be kept confidential but at the same time meet its objective of providing useful data to researchers it has develop ed procedures for stripping sensitive information (i.e. information that could facilitate re-identification of sample members) from data sets that are publicly released and also for providing mechanisms for qualified researchers to gain access to a variety of restricted-access data files. These mechanisms include a procedure whereby highly qualified researchersin particular only those who have a current grant from a federal agencycan apply to obtain restricted-access data sets for a limited amount of time with the understanding that they will make no attempt to r e-identify s amp le m embers and t hat they w ill be audited to ensure that they have adhered to the agreedupon safeguards. For those who meet some but not all of the requirements for receiving these data the files can be analyzed in a data enclave (a controlled secure environment in which eligible researchers can perform analyses). This paper focuses on approaches to restricting data access that may need to be considered by investigators who plan to share their data and by their institutional officials who will need to support that effort with appropriate infrastructure and policies. It also provides guidance to investigators and institutional review boards (IRBs) who seek access to restricted data generated and archived elsewhere.","","","2006","10.1525/jer.2006.1.3.85","","","medline-19385825.pdf","medline-19385825"
"Open science and evaluation","Galimberti, P.","Scires-It","","The aim of this contribution is to investigate how open science can influence/support research evaluation and whether and how open science practice can be evaluated in its effort to avoid what is predicted by goodhart's law (when a measure becomes a target, it ceases to be a good measure). The enterprise is not simple, as it focus on giving up points of reference that have been part of common practice of hard sciences for years while we are now trying to implement them in humanities and social sciences. We must accept that the internet has changed the way science is produced, disseminated, validated and evaluated and has multiplied its channels of communication. For this reason the traditional bibliometric indicators, which refer to articles published in peer reviewed journals, preferably in english, as the only viable publication channel, become inapplicable. In an open environment, the role of peer review, in particular the idea of blind (single or double) peer review must also radically change. © 2020. Scientific research and information technology ricerca scientifica e tecnologie dell'informazione. All rights reserved.","","","2020","10.2423/i22394303v10sp65","","","scopus-2-s2.0-85098932047.pdf","scopus-2-s2.0-85098932047"
"Citizen science. One of the eight pillars of open science identified by the european union","Morriello, R.","Jlis.it","","Although it is not a new phenomenon, citizen science is a form of collaborative research whose importance has grown in recent years. There is no single definition of citizen science and although this is the prevailing expression, activities are often referred to by other terms. However, the various terms used emphasize the key element, which is the voluntary participation of non-expert citizens in scientific research. Citizen science encompasses a wide range of activities and practices that can cover the entire life cycle of a research, from data collection to publication of results, and even evaluation. The first part of the article briefly traces the history of citizen science, highlighting the implications of the different facets through which it is defined. Then, it gives an overview of the state of the art, the initiatives, guidelines and good practices, the open issues, and the most representative institutions and organizations. A series of data are also provided regarding its dissemination, and reflections on the impact of this form of research on the scientific community and society, as well as on specific aspects related to open science and sustainable development. Finally, the article focuses on the role of university libraries and public libraries for citizen science. © 2021 the author(s).","","","2021","10.4403/jlis.it-12761","","","scopus-2-s2.0-85128175325.pdf","scopus-2-s2.0-85128175325"
"A qualitative study of big data and the opioid epidemic: recommendations for data governance","Evans, E.a. And Delorme, E. And Cyr, K. And Goldstein, D.m.","Bmc Medical Ethics","","Background: the opioid epidemic has enabled rapid and unsurpassed use of big data on people with opioid use disorder to design initiatives to battle the public health crisis, generally without adequate input from impacted communities. Efforts informed by big data are saving lives, yielding significant benefits. Uses of big data may also undermine public trust in government and cause other unintended harms. Objectives: we aimed to identify concerns and recommendations regarding how to use big data on opioid use in ethical ways. Methods: we conducted focus groups and interviews in 2019 with 39 big data stakeholders (gatekeepers, researchers, patient advocates) who had interest in or knowledge of the public health data warehouse maintained by the massachusetts department of public health. Results: concerns regarding big data on opioid use are rooted in potential privacy infringements due to linkage of previously distinct data systems, increased profiling and surveillance capabilities, limitless lifespan, and lack of explicit informed consent. Also problematic is the inability of affected groups to control how big data are used, the potential of big data to increase stigmatization and discrimination of those affected despite data anonymization, and uses that ignore or perpetuate biases. Participants support big data processes that protect and respect patients and society, ensure justice, and foster patient and public trust in public institutions. Recommendations for ethical big data governance offer ways to narrow the big data divide (e.g., prioritize health equity, set off-limits topics/methods, recognize blind spots), enact shared data governance (e.g., establish community advisory boards), cultivate public trust and earn social license for big data uses (e.g., institute safeguards and other stewardship responsibilities, engage the public, communicate the greater good), and refocus ethical approaches. Conclusions: using big data to address the opioid epidemic poses ethical concerns which, if unaddressed, may undermine its benefits. Findings can inform guidelines on how to conduct ethical big data governance and in ways that protect and respect patients and society, ensure justice, and foster patient and public trust in public institutions. © 2020, the author(s).","","","2020","10.1186/s12910-020-00544-9","","","scopus-2-s2.0-85093843316.pdf","scopus-2-s2.0-85093843316"
"What about ethics? Developing qualitative research in confinement settings","Gomes, S. And Duarte, V.","European Journal Of Criminology","","The main purpose of this article is to discuss some ethical-methodological issues associated with scientific research in confinement settings, particularly those that result from the relationship with the confined individual in the framework of qualitative research. Basing the reflection on empirical research developed by both authors in portuguese confinement settings – prisons and youth educational centres – we examine the significant challenges and dilemmas this type of research entails, exploring the interface between procedural ethics and ethics in practice at three points in the analytical process: before, during and after data collection. This article illustrates the interplay between formal and informal procedures, and between the initial distancing and strangeness when making contact with confinement settings and their social actors and the institutional and relational dynamics that become ingrained in our everyday practice. Our goal is to give visibility to these institutional and relational dynamics and to reflect on the challenges experienced by those who enter confinement settings to do research, in an effort to make the research process more transparent and at the same time more reflexive. We end our reflection advocating more ethically committed and critical scientific research. © the author(s) 2018.","","","2020","10.1177/1477370818801305","","","scopus-2-s2.0-85058652910.pdf","scopus-2-s2.0-85058652910"
"Methodology and reporting of mobile heath and smartphone application studies for schizophrenia","Torous John, Firth Joseph, Mueller Nora, Onnela J., Baker Justin T.","Harvard Review of Psychiatry","","[Correction Notice: An Erratum for this article was reported in Vol 25(4) of Harvard Review of Psychiatry (see record 2017-30914-006). The title of original article should has been printed as ""Methodology and Reporting of Mobile Health and Smartphone Application Studies for Schizophrenia.""] The increasing prevalence of mobile devices among patients of all demographic groups has the potential to transform the ways we diagnose monitor treat and study mental illness. As new tools and technologies emerge clinicians and researchers are confronted with an increasing array of options both for clinical assessment through digital capture of the essential behavioral elements of a condition and for intervention through formalized treatments coaching and other technology-assisted means of patient communication. And yet as with any new set of tools for the assessment or treatment of a medical condition establishing and adhering to reporting guidelines-that is what works and under what conditions-is an essential component of the translational research process. Here using the recently published World Health Organization mHealth Evaluation Reporting and Assessment guidelines for evaluating mobile health applications we review the methodological strengths and weaknesses of existing studies on smartphones and wearables for schizophrenia. While growing evidence supports the feasibility of using mobile tools in severe mental illness most studies to date failed to adequately report accessibility interoperability costs scalability replicability data security usability testing or compliance with national guidelines or regulatory statutes. Future research efforts addressing these specific gaps in the literature will help to advance our understanding and to realize the clinical potential of these new tools of psychiatry. (PsycInfo Database Record (c) 2021 APA all rights reserved)","","","2017","10.1097/hrp.0000000000000133","","","medline-28234658.pdf","medline-28234658"
"Informed consent in a tuberculosis genetic study in cameroon: information overload, situational vulnerability and diagnostic misconception","Mohammed-Ali, A.i. And Gebremeskel, E.i. And Yenshu, E. And Nji, T. And Ntabe, A.c. And Wanji, S. And Tangwa, G.b. And Munung, N.s.","Research Ethics","","Concerns around comprehension and recall of consent information by research participants have typically been associated with low health and research literacy levels. In genomics research, this concern is heightened as the scientific and ethical complexities of genetics research, such as biobanking, genetic susceptibility, data sharing, and incidental findings may be more difficult for potential research participants to understand. However, challenges to research participants’ comprehension of consent information may be compounded by factors beyond health and research literacy levels. To identify factors that may impact research participants’ understanding and recall of consent information, we designed a qualitative study to explore whether participants enrolled in a tuberculosis genetics study (tbgen-africa) in cameroon understood the objectives of the study, the risks and benefits and certain key aspects of the study such as biobanking and data sharing. The results showed that research participants had limited understanding and/or recall of the tbgen-africa study goals and methods. Some participants were of the opinion that tbgen-africa was not a genetics study because tuberculosis is not an inheritable condition. Factors that may have hindered understanding and/or recall of study information are diagnostic misconception (research participants consider research as part of medical diagnosis), and information overload and situational vulnerability (consent at a time of physical and emotional distress). There is a need for improved practices to support research participants’ understanding of consent information in genetics studies including designing the consent process in ways that minimize psychological distress and diagnostic/therapeutic misconception. © the author(s) 2022.","","","2022","10.1177/17470161221106674","","","scopus-2-s2.0-85131863179.pdf","scopus-2-s2.0-85131863179"
"Assessment Rubrics: Towards Clearer and More Replicable Design Research and Practice","Dawson Phillip","Assessment & Evaluation in Higher Education","","""Rubric"" is a term with a variety of meanings. As the use of rubrics has increased both in research and practice the term has come to represent divergent practices. These range from secret scoring sheets held by teachers to holistic student-developed articulations of quality. Rubrics are evaluated mandated embraced and resisted based on often imprecise and inconsistent understandings of the term. This paper provides a synthesis of the diversity of rubrics and a framework for researchers and practitioners to be clearer about what they mean when they say ""rubric."" Fourteen design elements or decision points are identified that make one rubric different from another. This framework subsumes previous attempts to categorise rubrics and should provide more precision to rubric discussions and debate as well as supporting more replicable research and practice.","","","2017","10.1080/02602938.2015.1111294","","","eric-ej1129724.pdf","eric-ej1129724"
"Decolonizing open science: southern interventions","Dutta, M. And Ramasubramanian, S. And Barrett, M. And Elers, C. And Sarwatay, D. And Raghunath, P. And Kaur, S. And Dutta, D. And Jayan, P. And Rahman, M. And Tallam, E. And Roy, S. And Falnikar, A. And Johnson, G.m. And Mandal, I. And Dutta, U. And Basnyat, I. And Soriano, C. And Pavarala, V. And Sreekumar, T.t. And Ganesh, S. And Pandi, A.r. And Zapata, D.","Journal Of Communication","","Hegemonic open science, emergent from the circuits of knowledge production in the global north and serving the economic interests of platform capitalism, systematically erase the voices of the subaltern margins from the global south and the southern margins inhabiting the north. Framed within an overarching emancipatory narrative of creating access for and empowering the margins through data exchanged on the global free market, hegemonic open science processes co-opt and erase southern epistemologies, working to create and reproduce new enclosures of extraction that serve data colonialism-capitalism. In this essay, drawing on our ongoing negotiations of community-led culture-centered advocacy and activist strategies that resist the racist, gendered, and classed structures of neocolonial knowledge production in the metropole in the north, we attend to southern practices of openness that radically disrupt the whiteness of hegemonic open science. These decolonizing practices foreground data sovereignty, community ownership, and public ownership of knowledge resources as the bases of resistance to the colonial-capitalist interests of hegemonic open science. © the author(s) 2021.","","","2021","10.1093/joc/jqab027","","","scopus-2-s2.0-85125146052.pdf","scopus-2-s2.0-85125146052"
"Comparison of registered and published primary outcomes in randomized controlled trials of gastroenterology and hepatology","Li X. Q., Yang G. L., Tao K. M., Zhang H. Q., Zhou Q. H., Ling C. Q.","Scandinavian Journal of Gastroenterology","","OBJECTIVES . The need for trial registration as well as the benefits it has brought for the transparency of medical research has been recognized for years. Trial registration has turned from an exception to a mandatory guideline in recent years. The present study aimed to examine the characteristics of registered randomized controlled trials (RCTs) in a sample of recently published gastroenterology RCTs and to assess the consistency of registered and published primary outcome (PO) in RCTs. METHODS. Articles published in the top five ""general and internal journals"" and top five ""gastroenterology and hepatology journals"" categories between 2009 and 2012 were searched in PubMed. Basic characteristics and the registration information were identified and extracted from the included RCTs. PO consistency analysis was conducted to compare between the registered and published format. RESULTS . A total of 305 RCTs were included; among them 252 could be identified with a registration number. Nearly half of these RCTs were funded solely by industry (141/305 46.3%). ClinicalTrials.gov was the most popular registry for these RCTs (214/252 84.9%). A total of 155 RCTs were included in the PO consistency analysis. Among them 22 (14.2%) RCTs had discrepancies between POs registered in the trial registry compared to the published article. CONCLUSIONS . Based on the results of the present study selective outcome reporting of gastroenterology RCTs published in leading medical journals has been much improved over the past years. However there might be a sampling bias to say that consistency of registered and published POs of gastroenterology RCTs has been better than before.","","","2013","10.3109/00365521.2013.845909","","","medline-24131272.pdf","medline-24131272"
"Data-pe: a framework for evaluating data publication policies at scholarly journals","Moles, N.","Data Science Journal","","With the growing importance of data to the scholarly record and the critical role journals play in facilitating data sharing, the complex landscape of scholarly journal data publication policies has become an obstacle for research. This paper outlines data-pe, a framework for evaluating these policies. It takes the form of a conceptual foundation, comprising twelve criteria for evaluation, operationalized through an evaluation tool. Its objective is to function as a flexible means for a variety of stakeholders to appraise individual policies. Examples of the use of the framework are provided and means for the validation of the tool are discussed. © 2015, committee on data for science and technology. All rights reserved.","","","2015","10.2481/dsj.14-047","","","scopus-2-s2.0-85047286029.pdf","scopus-2-s2.0-85047286029"
"Research guidelines in the era of large-scale collaborations: An analysis of genome-wide association study consortia","Austin M. A., Hair M. S., Fullerton S. M.","American Journal of Epidemiology","","Scientific research has shifted from studies conducted by single investigators to the creation of large consortia. Genetic epidemiologists for example now collaborate extensively for genome-wide association studies (GWAS). The effect has been a stream of confirmed disease-gene associations. However effects on human subjects oversight data-sharing publication and authorship practices research organization and productivity and intellectual property remain to be examined. The aim of this analysis was to identify all research consortia that had published the results of a GWAS analysis since 2005 characterize them determine which have publicly accessible guidelines for research practices and summarize the policies in these guidelines. A review of the National Human Genome Research Institute's Catalog of Published Genome-Wide Association Studies identified 55 GWAS consortia as of April 1 2011. These consortia were comprised of individual investigators research centers studies or other consortia and studied 48 different diseases or traits. Only 14 (25%) were found to have publicly accessible research guidelines on consortia websites. The available guidelines provide information on organization governance and research protocols; half address institutional review board approval. Details of publication authorship data-sharing and intellectual property vary considerably. Wider access to consortia guidelines is needed to establish appropriate research standards with broad applicability to emerging forms of large-scale collaboration. © 2012 The Author.","","","2012","10.1093/aje/kwr441","","","medline-22491085.pdf","medline-22491085"
"Analysis and Evaluation of Chinese Open Government Agricultural Data","Enbo J., Na L.","Journal of Library and Information Science in Agriculture","","[Purpose/Significance] The Ministry of Agriculture and Rural Affairs of China and the agricultural admin istrative departments of various provinces play a major role in promoting agricultural and rural big data develop-ment. This paper aims to learn analyze and evaluate the data publishing sharing standardization and applications by investigating the open governmental agricultural data in 31 provinces autonomous regions and municipalities in the mainland of China and propose suggestions for improvement accordingly. [Method/Process] We investigated the open government platforms built by 31 provinces (autonomous regions and municipalities) sorted out the content published in these platforms and standardized and summarized the investigation data. We made an analysis based on the update time of data license of data applications and download of data. Meanwhile combined with FAIR data principles we evaluated the data from the findability accessibility interoperability and reuse of the data. [Results/Conclusions] At present the management publishing and sharing of agricultural data in China is at an elementary stage. Although various governments formulated regulations and policies the implementation of these regulations and policies is unsatisfactory. The Ministry of Agriculture and Rural Affairs of China should join hands with the agricultural administrative departments of various provinces to promote the opening and use of government agricultural data. Meanwhile they should be in line with the construction mode of the international open government data and improve their capabilities in data publishing management and services. © 2020 Lithologic Reservoirs","","","2020","10.13998/j.cnki.issn1002-1248.2020.10.20-0454","","","scopus-2-s2.0-85150375309.pdf","scopus-2-s2.0-85150375309"
"Auto-FedRL: Federated Hyperparameter Optimization for Multi-institutional Medical Image Segmentation","Guo P., Yang D., Hatamizadeh A., Xu A., Xu Z., Li W., Zhao C., Xu D., Harmon S., Turkbey E., Turkbey B., Wood B., Patella F., Stellato E., Carrafiello G., Patel V. M., Roth H. R., Avidan S., Brostow G., Cissé M., Farinella G. M., Hassner T.","","","Federated learning (FL) is a distributed machine learning technique that enables collaborative model training while avoiding explicit data sharing. The inherent privacy-preserving property of FL algorithms makes them especially attractive to the medical field. However in case of heterogeneous client data distributions standard FL methods are unstable and require intensive hyperparameter tuning to achieve optimal performance. Conventional hyperparameter optimization algorithms are impractical in real-world FL applications as they involve numerous training trials which are often not affordable with limited compute budgets. In this work we propose an efficient reinforcement learning (RL)-based federated hyperparameter optimization algorithm termed Auto-FedRL in which an online RL agent can dynamically adjust hyperparameters of each client based on the current training progress. Extensive experiments are conducted to investigate different search strategies and RL agents. The effectiveness of the proposed method is validated on a heterogeneous data split of the CIFAR-10 dataset as well as two real-world medical image segmentation datasets for COVID-19 lesion segmentation in chest CT and pancreas segmentation in abdominal CT. © 2022 The Author(s) under exclusive license to Springer Nature Switzerland AG.","","","2022","10.1007/978-3-031-19803-8_26","","","scopus-2-s2.0-85142677131.pdf","scopus-2-s2.0-85142677131"
"Current state of microplastic pollution research data: trends in availability and sources of open data","Jenkins, T. And Persaud, B.d. And Cowger, W. And Szigeti, K. And Roche, D.g. And Clary, E. And Slowinski, S. And Lei, B. And Abeynayaka, A. And Nyadjro, E.s. And Maes, T. And Thornton Hampton, L. And Bergmann, M. And Aherne, J. And Mason, S.a. And Honek, J.f. And Rezanezhad, F. And Lusher, A.l. And Booth, A.m. And Smith, R.d.l. And Van Cappellen, P.","Frontiers In Environmental Science","","The rapid growth in microplastic pollution research is influencing funding priorities, environmental policy, and public perceptions of risks to water quality and environmental and human health. Ensuring that environmental microplastics research data are findable, accessible, interoperable, and reusable (fair) is essential to inform policy and mitigation strategies. We present a bibliographic analysis of data sharing practices in the environmental microplastics research community, highlighting the state of openness of microplastics data. A stratified (by year) random subset of 785 of 6,608 microplastics articles indexed in web of science indicates that, since 2006, less than a third (28.5%) contained a data sharing statement. These statements further show that most often, the data were provided in the articles’ supplementary material (38.8%) and only 13.8% via a data repository. Of the 279 microplastics datasets found in online data repositories, 20.4% presented only metadata with access to the data requiring additional approval. Although increasing, the rate of microplastic data sharing still lags behind that of publication of peer-reviewed articles on environmental microplastics. About a quarter of the repository data originated from north america (12.8%) and europe (13.4%). Marine and estuarine environments are the most frequently sampled systems (26.2%);  sediments (18.8%) and water (15.3%) are the predominant media. Of the available datasets accessible, 15.4% and 18.2% do not have adequate metadata to determine the sampling location and media type, respectively. We discuss five recommendations to strengthen data sharing practices in the environmental microplastic research community. Copyright © 2022 jenkins, persaud, cowger, szigeti, roche, clary, slowinski, lei, abeynayaka, nyadjro, maes, thornton hampton, bergmann, aherne, mason, honek, rezanezhad, lusher, booth, smith and van cappellen.","","","2022","10.3389/fenvs.2022.912107","","","scopus-2-s2.0-85134236394.pdf","scopus-2-s2.0-85134236394"
"Privacy challenges and research opportunities for genomic data sharing","Bonomi L., Huang Y., Ohno-Machado L.","Nature Genetics","","The sharing of genomic data holds great promise in advancing precision medicine and providing personalized treatments and other types of interventions. However these opportunities come with privacy concerns and data misuse could potentially lead to privacy infringement for individuals and their blood relatives. With the rapid growth and increased availability of genomic datasets understanding the current genome privacy landscape and identifying the challenges in developing effective privacy-protecting solutions are imperative. In this work we provide an overview of major privacy threats identified by the research community and examine the privacy challenges in the context of emerging direct-to-consumer genetic-testing applications. We additionally present general privacy-protection techniques for genomic data sharing and their potential applications in direct-to-consumer genomic testing and forensic analyses. Finally we discuss limitations in current privacy-protection methods highlight possible mitigation strategies and suggest future research opportunities for advancing genomic data sharing. Copyright © 2020 Springer Nature America Inc.","","","2020","10.1038/s41588-020-0651-0","","","medline-32601475.pdf","medline-32601475"
"Methodological considerations in risk assessment research","Fazel, Seena And Bjorkly, Stal","","","There has been increasing awareness in scientific research of the importance of accounting for possible biases and the need for transparency. This ""research on research"" has been driven in part by the problems of publication bias in treatment and observational research, and also by the lack of validation for risk factors, associations, and biomarkers in many areas of science, including psychology (baker, 2015). Furthermore, much research is not applied in clinical practice, sometimes because interventions are not detailed sufficiently in publications to allow for their implementation. This has led some prominent commentators to estimate that more than 90% of all scientific research may be wasted as a consequence. (Psycinfo database record (c) 2021 apa, all rights reserved)","","","2016","10.1093/acprof:oso/9780199386291.003.0002","","","psychinfo-2016-40455-002.pdf","psychinfo-2016-40455-002"
"The credibility chasm in policy research from academics, think tanks, and advocacy organizations","Doberstein, C.","Canadian Public Policy","","How do key policy professionals inside government view various sources of policy research? Are there systematic differences in the perceptions of the quality and credibility of research derived from different sources? This is a replication of and expansion on doberstein (2017), which presented a randomized controlled survey experiment using policy analysts to systematically test the source effects of policy research. Doberstein's experimental findings provide evidence for the hypothesis that academic research is perceived to be substantially more credible to government policy analysts than think tank or advocacy organization research, regardless of its content, and that sources perceived as more ideological are much less credible. This study replicates that experiment in three additional canadian provincial governments to verify whether the relationship found in the original study persists in a larger sample and in conjunction with further randomization procedures. This study corroborates the original study's findings, confirming that external policy advice systems are subject to powerful heuristics that bureaucrats use to sift through evidence and advice.","","","2017","10.3138/cpp.2016-067","","","scopus-2-s2.0-85039156695.pdf","scopus-2-s2.0-85039156695"
"Balancing access to health data and privacy: a review of the issues and approaches for the future. [Review]","Lane J., Schur C.","Health Services Research","","BACKGROUND: There has been a dramatic increase in the types of microdata and this holds great promise for health services research. However legislative efforts to protect individual privacy have reduced the flow of health care data for research purposes and increased costs and delays affecting the quality of analysis. AIM: This paper provides an overview of the challenges raised by concerns about data confidentiality in the context of health services research the current methodologies used to ensure data security and a description of one successful approach to balancing access and privacy. Materials and Methods. We analyze the issues of access and privacy using a conceptual framework based on balancing the risk of reidentification with the utility associated with data analysis. The guiding principle should be to generate released data that are as close to the maximum acceptable risk as possible. HIPAA and other privacy measures can perhaps be seen as having had the effect of lowering the ""maximum acceptable risk"" level and rendering some data unreleasable. RESULTS: We discuss the levels of risk and utility associated with different types of data used in health services research and the ability to link data from multiple sources as well as current models of data sharing and their limitations. DISCUSSION: One particularly compelling approach is to establish a remote access ""data enclave"" where statistical protections are applied to the data technical protections ensure compliance with data-sharing requirements and operational controls limit researchers' access to the data they need for their specific research questions. CONCLUSION: We recommend reducing delays in access to data for research increasing the use of remote access data enclaves and disseminating knowledge and promulgating standards for best practices related to data protection.","","","2010","10.1111/j.1475-6773.2010.01141.x","","","medline-21054366.pdf","medline-21054366"
"Elevating the quality of disability and rehabilitation research: mandatory use of the reporting guidelines","Chan L., Heinemann A. W., Roberts J.","American Journal of Occupational Therapy","","Note from the AJOT Editor-in-Chief: Since 2010 the American Journal of Occupational Therapy (AJOT) has adopted reporting standards based on the Consolidated Standards of Reporting Trials (CONSORT) Statement and American Psychological Association (APA) guidelines in an effort to publish transparent clinical research that can be easily evaluated for methodological and analytical rigor (APA Publications and Communications Board Working Group on Journal Article Reporting Standards 2008; Moher Schulz & Altman 2001). AJOT has now joined 28 other major rehabilitation and disability journals in a collaborative initiative to enhance clinical research reporting standards through adoption of the EQUATOR Network reporting guidelines described below. Authors will now be required to use these guidelines in the preparation of manuscripts that will be submitted to AJOT. Reviewers will also use these guidelines to evaluate the quality and rigor of all AJOT submissions. By adopting these standards we hope to further enhance the quality and clinical applicability of articles to our readers. Copyright © 2014 by the American Occupational Therapy Association Inc.","","","2014","10.5014/ajot.2014.682004","","","medline-24581397.pdf","medline-24581397"
"The lack of cross-validation can lead to inflated results and spurious conclusions: a re-analysis of the macarthur violence risk assessment study","Bokhari, E. And Hubert, L.","Journal Of Classification","","Cross-validation is an important evaluation strategy in behavioral predictive modeling;  without it, a predictive model is likely to be overly optimistic. Statistical methods have been developed that allow researchers to straightforwardly cross-validate predictive models by using the same data employed to construct the model. In the present study, cross-validation techniques were used to construct several decision-tree models with data from the macarthur violence risk assessment study (monahan et al., 2001). The models were then compared with the original (non-cross-validated) classification of violence risk assessment tool. The results show that the measures of predictive model accuracy (auc, misclassification error, sensitivity, specificity, positive and negative predictive values) degrade considerably when applied to a testing sample, compared with the training sample used to fit the model initially. In addition, unless false negatives (that is, incorrectly predicting individuals to be nonviolent) are considered more costly than false positives (that is, incorrectly predicting individuals to be violent), the models generally make few predictions of violence. The results suggest that employing cross-validation when constructing models can make an important contribution to increasing the reliability and replicability of psychological research. © 2018, classification society of north america.","","","2018","10.1007/s00357-018-9252-3","","","scopus-2-s2.0-85044436708.pdf","scopus-2-s2.0-85044436708"
"How qualitative research methods can be leveraged to strengthen mixed methods research in public policy and public administration?","Hendren, K. And Newcomer, K. And Pandey, S.k. And Smith, M. And Sumner, N.","Public Administration Review","","Recently, there have been a variety of arguments voiced to encourage that more attention be given to the role qualitative methods can play in mixed methods research in public policy and public administration. This article discusses these claims and describes the benefits of qualitative approaches, and how qualitative research methods can be leveraged to strengthen mixed methods research in public administration. We also provide a guide for improving the credibility of mixed methods research through increasing transparency and discussions of all methodological decisions. This study is based on a systematic content analysis of 186 mixed methods studies published in public policy and public administration journals between 2010 and 2018. We found that findings from the quantitative methods dominated the mixed methods studies, little diversity in data collection and analysis methods, and frequent failure to integrate insights from both methods. We also analyzed the 36 qualitative-dominant studies in the sample, and illuminated seven different ways that authors of qualitative-dominant studies leveraged the qualitative strand to strengthen mixed methods research. We developed lessons from our analysis of the qualitative-dominant articles on how to incorporate qualitative methods in a thoughtful manner, articulate a role for each strand, and effectively support findings with one or more strands. © 2022 american society for public administration.","","","2023","10.1111/puar.13528","","","scopus-2-s2.0-85132606461.pdf","scopus-2-s2.0-85132606461"
"Knowledge sharing in multicultural organizations: evidence from pakistan","Raza, I. And Awang, Z.","Higher Education, Skills And Work-Based Learning","","Purpose: taking higher educational institutes (heis) operating in islamabad metropolitan, and pakistan as research context, the purpose of this paper is to identify the antecedents of knowledge sharing behavior (ksb) and to check their causal effect in perspective of culturally diverse academic staff. In addition, the authors suggest certain policies for heis that can raise knowledge sharing practices in multicultural environment. Design/methodology/approach: it is a cross-sectional study, quantitative in nature, and has used a self-administered questionnaire for data collection. With proportionate stratified random sampling technique, 278 academic employees working in three faculties from six public sector universities operating in islamabad metropolitan have recorded their responses. This research also applied confirmatory factor analysis and structural equation modeling to examine the proposed hypothesis of this inquiry. Findings: the empirical results indicate significant and positive effect of cultural diversity management, interpersonal trust, and leader-empowering behavior on ksb, whereas knowledge technology has insignificant effect on ksb of culturally diverse academic staff. Moreover, proposed model has explained 54 percent variation in endogenous construct. Practical implications: the present research aids academic leadership in designing policies and strategies to enhance knowledge sharing among faculty members and to create a supportive knowledge sharing culture. Originality/value: this study fills the empirical gap that exists in literature by exploring the antecedents and their effect on ksb of multicultural academic staff associated in public sector heis in islamabad metropolitan, pakistan. © 2019, emerald publishing limited.","","","2020","10.1108/heswbl-09-2019-0114","","","scopus-2-s2.0-85079172406.pdf","scopus-2-s2.0-85079172406"
"Implementation of GenePattern within the Stanford Microarray Database","Hubble J., Demeter J., Jin H., Mao M., Nitzberg M., Reddy T. B., Wymore F., Zachariah Z. K., Sherlock G., Ball C. A.","Nucleic Acids Research","","Hundreds of researchers across the world use the Stanford Microarray Database (SMD; http://smd.stanford.edu/) to store annotate view analyze and share microarray data. In addition to providing registered users at Stanford access to their own data SMD also provides access to public data and tools with which to analyze those data to any public user anywhere in the world. Previously the addition of new microarray data analysis tools to SMD has been limited by available engineering resources and in addition the existing suite of tools did not provide a simple way to design execute and share analysis pipelines or to document such pipelines for the purposes of publication. To address this we have incorporated the GenePattern software package directly into SMD providing access to many new analysis tools as well as a plug-in architecture that allows users to directly integrate and share additional tools through SMD. In this article we describe our implementation of the GenePattern microarray analysis software package into the SMD code base. This extension is available with the SMD source code that is fully and freely available to others under an Open Source license enabling other groups to create a local installation of SMD with an enriched data analysis capability.","","","2009","10.1093/nar/gkn786","","","medline-18953035.pdf","medline-18953035"
"Portuguese authorship in published clinical trials: differences in industry and investigator initiated trials","Pinheiro Andrade, M. And Matias, D. And Batuca, J. And Gouveia, N. And Mota-Filipe, H. And Carreira Monteiro, E. And Madeira, C.","Acta Medica Portuguesa","","Introduction: the aim of this study was to investigate the portuguese authorship in publications resulting from trials initiated by the industry or investigators and run in portugal. Material and methods: clinical trials with portuguese institutions as sponsor or recruiting centers, and registered in four clinical trial registries, in the last 14 years, were assessed. Publications of completed trials, from both the initiative of the industry and investigators were screened and compared. Results: the percentage of published trials initiated by industry and investigators was similar (28.0%). However, the percentage of completed investigator-initiated trials (43.6%) was lower when compared to industry trials (69.7%). There was a higher percentage of portuguese authorship in published investigator-initiated trials when compared with industry-initiated trials (47.1% vs 8.5%, respectively). Moreover, industry-initiated trials with portuguese authors were published in journals with lower journal impact factor when compared with those published without authorship of portuguese investigators. Oncology was the therapeutic area with the highest number of clinical trial registrations and publications. However, in publications with portuguese authors, industry initiated trials mainly focused on neurology while investigator-initiated trials had a higher number of papers in the fields of gastroenterology and infection diseases. Published trials with portuguese authorship, initiated by the industry or investigators, also targeted different populations and had different purposes. In both cases, no significant differences were observed in terms of the journal impact factor or in the alignment of the published randomized trials with the respective reporting guidelines. Discussion: when compared with previous publications, this study showed an increasing trend in the number of clinical trials in portugal, published within similar timeframes, after trial conclusion. Even though both industry and investigator trials are published within the standards for reporting trials, the low number of portuguese authorships in industry publications might underline the need for invigorating these independent clinical trials in portugal by capacitating and empowering national clinical research teams. Conclusion: this study confirmed that even though all registered trials had the involvement of portuguese institutions as a recruiting center, not all the published trials had portuguese investigators as authors, mainly those initiated by the industry. © 2021 celom. All rights reserved.","","","2021","10.20344/amp.14554","","","scopus-2-s2.0-85119040093.pdf","scopus-2-s2.0-85119040093"
"Data sharing between home care professionals: a feasibility study using the RAI Home Care instrument","Guthrie D. M., Pitman R., Fletcher P. C., Hirdes J. P., Stolee P., Poss J. W., Papaioannou A., Berg K., Ezekiel H. J.","BMC Geriatrics","","BACKGROUND: Across Ontario home care professionals collect standardized information on each client using the Resident Assessment for Home Care (RAI-HC). However this information is not consistently shared with those professionals who provide services in the client's home. In this pilot study we examined the feasibility of sharing data from the RAI-HC between care coordinators and service providers.\\\\\\\\rMETHODS: All participants were involved in a one-day training session on the RAI-HC. The care coordinators shared specific outputs from the RAI-HC including the embedded health index scales with their contracted physiotherapy and occupational therapy service providers. Two focus groups were held one with care coordinators (n = 4) and one with contracted service providers (n = 6). They were asked for their opinions on the positive aspects of the project and areas for improvement.\\\\\\\\rRESULTS: The focus groups revealed a number of positive outcomes related to the project including the use of a falls prevention brochure and an increased level of communication between professionals. The participants also cited multiple areas for improvement related to data sharing (e.g. time constraints data being sent in a timely fashion) and to their standard practices in the community (e.g. busy workloads difficulties in data sharing duplication of assessments between professionals).\\\\\\\\rCONCLUSIONS: Home care professionals were able to share select pieces of information generated from the RAI-HC system and this project enhanced the level of communication between the two groups of professionals. However a single information session was not adequate training for the rehabilitation professionals who do not use the RAI-HC as part of normal practice. Better education ongoing support and timely access to the RAI-HC data are some ways to improve the usefulness of this information for busy home care providers.","","","2014","10.1186/1471-2318-14-81","","","medline-24975375.pdf","medline-24975375"
"A Decade of GigaScience: GigaDB and the Open Data Movement","Armit C., Tuli M. A., Hunter C. I.","GigaScience","","The increasingly multidisciplinary nature of scientific research necessitates a need for Open Data repositories that can archive data in support of publications in scientific journals. Recognising this need even before GigaScience launched in 2012 GigaDB was already in place and taking data for a year before (making it 11 this year). Since GigaDB launched there has been a consistent growth in this resource in terms of data volume data discoverability and data re-use. In this commentary we provide a retrospective of key changes over the last decade and the role of Data Curation in enhancing the user experience. Furthermore we explore a much needed emphasis on enabling researchers to interact with and explore datasets prior to data download. Copyright © The Author(s) 2022. Published by Oxford University Press GigaScience.","","","2022","10.1093/gigascience/giac053","","","medline-35701374.pdf","medline-35701374"
"Information technology principles for management reporting and research. [Review] [51 refs]","Gillam M., Rothenhaus T., Smith V., Kanhouwa M.","","","Information technology holds the promise to enhance the ability of individuals and organizations to manage emergency departments improve data sharing and reporting and facilitate research. The Society for Academic Emergency Medicine (SAEM) Consensus Committee has identified nine principles to outline a path of optimal features and designs for current and future information technology systems. The principles roughly summarized include the following: utilize open database standards with clear data dictionaries provide administrative access to necessary data appoint and recognize individuals with emergency department informatics expertise allow automated alert and proper identification for enrollment of cases into research provide visual and statistical tools and training to analyze data embed automated configurable alarm functionality for clinical and nonclinical systems allow multiexport standard and format configurable reporting strategically acquire mission-critical equipment that is networked and capable of automated feedback regarding functional status and location and dedicate resources toward informatics research and development. The SAEM Consensus Committee concludes that the diligent application of these principles will enhance emergency department management reporting and research and ultimately improve the quality of delivered health care. [References: 51]","","","2004","","","","unknown-1537.pdf","unknown-1537"
"The state of social and personality science: rotten to the core, not so bad, getting better, or getting worse?","Motyl M. And Demos A.p. And Carsel T.s. And Hanson B.e. And Melton Z.j. And Mueller A.b. And Prims J.p. And Sun J. And Washburn A.n. And Wong K.m. And Yantis C. And Skitka L.j.","J Pers Soc Psychol","","The scientific quality of social and personality psychology has been debated at great length in recent years. Despite research on the prevalence of questionable research practices (qrps) and the replicability of particular findings, the impact of the current discussion on research practices is unknown. The current studies examine whether and how practices have changed, if at all, over the last 10 years. In study 1, we surveyed 1,166 social and personality psychologists about how the current debate has affected their perceptions of their own and the field's research practices. In study 2, we coded the research practices and critical test statistics from social and personality psychology articles published in 2003-2004 and 2013-2014. Together, these studies suggest that (a) perceptions of the current state of the field are more pessimistic than optimistic;  (b) the discussion has increased researchers' intentions to avoid qrps and adopt proposed best practices, (c) the estimated replicability of research published in 2003-2004 may not be as bad as many feared, and (d) research published in 2013-2014 shows some improvement over research published in 2003-2004, a result that suggests the field is evolving in a positive direction. (Psycinfo database recordcopyright (c) 2017 apa, all rights reserved).","","","2017","10.1037/pspa0000084","","","embase-623063127.pdf","embase-623063127"
"Evaluating Research Transparency and Openness in Communication Sciences and Disorders Journals","Schroeder S. R., Gaeta L., El Amin M., Chow J. C., Borders J. C.","Journal of Speech Language & Hearing Research","","PURPOSE: To improve the credibility reproducibility and clinical utility of research findings many scientific fields are implementing transparent and open research practices. Such open science practices include researchers making their data publicly available and preregistering their hypotheses and analyses. A way to enhance the adoption of open science practices is for journals to encourage or require submitting authors to participate in such practices. Accordingly the American Speech-Language-Hearing Association's Journals Program has recently announced their intention to promote open science practices. Here we quantitatively assess the extent to which several journals in communication sciences and disorders (CSD) encourage or require participation in several open science practices by using the Transparency and Openness Promotion (TOP) Factor metric.\\\\\\\\rMETHOD: TOP Factors were assessed for 34 CSD journals as well as several journals in related fields. TOP Factors measure the level of implementation across 10 open science-related practices (e.g. data transparency analysis plan preregistration and replication) for a total possible score of 29 points.\\\\\\\\rRESULTS: Collectively CSD journals had very low TOP Factors (M = 1.4 range: 0-8). The related fields of Psychology (M = 4.0) Rehabilitation (M = 3.2) Linguistics (M = 1.7) and Education (M = 1.6) also had low scores though Psychology and Rehabilitation had higher scores than CSD.\\\\\\\\rCONCLUSION: CSD journals currently have low levels of encouraging or requiring participation in open science practices which may impede adoption. Open Science Form: https://doi.org/10.23641/asha.21699458.","","","2023","10.1044/2022_jslhr-22-00330","","","medline-36516469.pdf","medline-36516469"
"Improving outcome reporting in clinical trial reports and protocols: study protocol for the instrument for reporting planned endpoints in clinical trials (inspect)","Butcher, N.j. And Monsour, A. And Mew, E.j. And Szatmari, P. And Pierro, A. And Kelly, L.e. And Farid-Kapadia, M. And Chee-A-Tow, A. And Saeed, L. And Monga, S. And Ungar, W. And Terwee, C.b. And Vohra, S. And Fergusson, D. And Askie, L.m. And Williamson, P.r. And Chan, A.-W. And Moher, D. And Offringa, M.","Trials","","Background: inadequate and poor quality outcome reporting in clinical trials is a well-documented problem that impedes the ability of researchers to evaluate, replicate, synthesize, and build upon study findings and impacts evidence-based decision-making by patients, clinicians, and policy-makers. To facilitate harmonized and transparent reporting of outcomes in trial protocols and published reports, the instrument for reporting planned endpoints in clinical trials (inspect) is being developed. The final product will provide unique inspect extensions to the spirit (standard protocol items: recommendations for interventional trials) and consort (consolidated standards of reporting trials) reporting guidelines. Methods: the inspect spirit and consort extensions will be developed in accordance with the methodological framework created by the equator (enhancing the quality and transparency of health research quality) network for reporting guideline development. Development will consist of (1) the creation of an initial list of candidate outcome reporting items synthesized from expert consultations and a scoping review of existing guidance for reporting outcomes in trial protocols and reports;  (2) a three-round international delphi study to identify additional candidate items and assess candidate item importance on a 9-point likert scale, completed by stakeholders such as trial report and protocol authors, systematic review authors, biostatisticians and epidemiologists, reporting guideline developers, clinicians, journal editors, and research ethics board representatives;  and (3) an in-person expert consensus meeting to finalize the set of essential outcome reporting items for trial protocols and reports, respectively. The consensus meeting discussions will be independently facilitated and informed by the empirical evidence identified in the primary literature and through the opinions (aggregate rankings and comments) collected via the delphi study. An integrated knowledge translation approach will be used throughout inspect development to facilitate implementation and dissemination, in addition to standard post-development activities. Discussion: inspect will provide evidence-informed and consensus-based standards focused on outcome reporting in clinical trials that can be applied across diverse disease areas, study populations, and outcomes. Inspect will support the standardization of trial outcome reporting, which will maximize trial usability, reduce bias, foster trial replication, improve trial design and execution, and ultimately reduce research waste and help improve patient outcomes. © 2019 the author(s).","","","2019","10.1186/s13063-019-3248-0","","","scopus-2-s2.0-85062586422.pdf","scopus-2-s2.0-85062586422"
"How to know what works in alleviating poverty: Learning from experimental approaches in qualitative research","Hartman A., Kern F. G.","World Development","","Experimental studies of poverty alleviation have stimulated an interdisciplinary discussion on what constitutes robust evidence to inform policy and benefit the poor. These studies emphasize research transparency and reporting standards pre-registration data sharing replication and aggregated evidence. Though imperfect such practices help to identify what works under what conditions. We argue that researchers should also explore how similar practices could be tailored for qualitative research on the politics of poverty alleviation. We outline a research framework motivated by the experiment focused Metaketa initiative that incorporates the strengths of qualitative inquiry. We present the eleven pillars of a qualitative Metaketa. © 2019 Elsevier Ltd","","","2020","10.1016/j.worlddev.2019.104804","","","scopus-2-s2.0-85076840308.pdf","scopus-2-s2.0-85076840308"
"Distributed Quasi-Poisson regression algorithm for modeling multi-site count outcomes in distributed data networks","Edmondson M. J., Luo C., Nazmul Islam M., Sheils N. E., Buresh J., Chen Z., Bian J., Chen Y.","Journal of Biomedical Informatics","","BACKGROUND: Observational studies incorporating real-world data from multiple institutions facilitate study of rare outcomes or exposures and improve generalizability of results. Due to privacy concerns surrounding patient-level data sharing across institutions methods for performing regression analyses distributively are desirable. Meta-analysis of institution-specific estimates is commonly used but has been shown to produce biased estimates in certain settings. While distributed regression methods are increasingly available methods for analyzing count outcomes are currently limited. Count data in practice are commonly subject to overdispersion exhibiting greater variability than expected under a given statistical model.\\\\\\\\rOBJECTIVE: We propose a novel computational method a one-shot distributed algorithm for quasi-Poisson regression (ODAP) to distributively model count outcomes while accounting for overdispersion.\\\\\\\\rMETHODS: ODAP incorporates a surrogate likelihood approach to perform distributed quasi-Poisson regression without requiring patient-level data sharing only requiring sharing of aggregate data from each participating institution. ODAP requires at most three rounds of non-iterative communication among institutions to generate coefficient estimates and corresponding standard errors. In simulations we evaluate ODAP under several data scenarios possible in multi-site analyses comparing ODAP and meta-analysis estimates in terms of error relative to pooled regression estimates considered the gold standard. In a proof-of-concept real-world data analysis we similarly compare ODAP and meta-analysis in terms of relative error to pooled estimatation using data from the OneFlorida Clinical Research Consortium modeling length of stay in COVID-19 patients as a function of various patient characteristics. In a second proof-of-concept analysis using the same outcome and covariates we incorporate data from the UnitedHealth Group Clinical Discovery Database together with the OneFlorida data in a distributed analysis to compare estimates produced by ODAP and meta-analysis.\\\\\\\\rRESULTS: In simulations ODAP exhibited negligible error relative to pooled regression estimates across all settings explored. Meta-analysis estimates while largely unbiased were increasingly variable as heterogeneity in the outcome increased across institutions. When baseline expected count was 0.2 relative error for meta-analysis was above 5% in 25% of iterations (250/1000) while the largest relative error for ODAP in any iteration was 3.59%. In our proof-of-concept analysis using only OneFlorida data ODAP estimates were closer to pooled regression estimates than those produced by meta-analysis for all 15 covariates. In our distributed analysis incorporating data from both OneFlorida and the UnitedHealth Group Clinical Discovery Database ODAP and meta-analysis estimates were largely similar while some differences in estimates (as large as 13.8%) could be indicative of bias in meta-analytic estimates.\\\\\\\\rCONCLUSIONS: ODAP performs privacy-preserving communication-efficient distributed quasi-Poisson regression to analyze count outcomes using data stored within multiple institutions. Our method produces estimates nearly matching pooled regression estimates and sometimes more accurate than meta-analysis estimates most notably in settings with relatively low counts and high outcome heterogeneity across institutions. Copyright © 2022 Elsevier Inc. All rights reserved.","","","2022","10.1016/j.jbi.2022.104097","","","medline-35643272.pdf","medline-35643272"
"Communicating results in post-Belmont era biomonitoring studies: Lessons from genetics and neuroimaging research","Morello-Frosch R., Varshavsky J., Liboiron M., Brown P., Brody J. G.","Environmental Research","","Background: Biomonitoring is a critical tool to assess the effects of chemicals on health as scientists seek to better characterize life-course exposures from diverse environments. This trend coupled with increased institutional support for community-engaged environmental health research challenge established ethical norms related to biomonitoring results communication and data sharing between scientists study participants and their wider communities. Method(s): Through a literature review participant observation at workshops and interviews we examine ethical tensions related to reporting individual data from chemical biomonitoring studies by drawing relevant lessons from the genetics and neuroimaging fields. Result(s): In all three fields ethical debates about whether/how to report-back results to study participants are precipitated by two trends. First changes in analytical methods have made more data accessible to stakeholders. For biomonitoring improved techniques enable detection of more chemicals at lower levels and diverse groups of scientists and health advocates now conduct exposure studies. Similarly innovations in genetics have catalyzed large-scale projects and broadened the scope of who has access to genetic information. Second increasing public interest in personal medical information has compelled imaging researchers to address demands by participants to know their personal data despite uncertainties about their clinical significance. Four ethical arenas relevant to biomonitoring results communication emerged from our review: tensions between participants' right-to-know their personal results versus their ability or right-to-act to protect their health; whether and how to report incidental findings; informed consent in biobanking; and open-access data sharing. Conclusion(s): Ethically engaging participants in biomonitoring studies requires consideration of several issues including scientific uncertainty about health implications and exposure sources the ability of participants to follow up on potentially problematic results tensions between individual and community research protections governance and consent regarding secondary use of tissue samples and privacy challenges in open access data sharing. Copyright © 2014 Elsevier Inc.","","","2015","10.1016/j.envres.2014.10.001","","","medline-25460657.pdf","medline-25460657"
"Access to linked administrative healthcare utilization data for pharmacoepidemiology and pharmacoeconomics research in Canada: anti-viral drugs as an example","Rawson N. S.","Pharmacoepidemiology & Drug Safety","","PURPOSE: Administrative healthcare utilization data from Canadian provinces have been used for pharmacoepidemiology and pharmacoeconomics research but limited transparency exists about opportunities for data access who can access them and processes to obtain data. An attempt was made to obtain data from all 10 provinces to evaluate access and its complexity.\\\\\\\\rMETHODS: An initial enquiry about the process and requirements to obtain data on individual anonymized patients dispensed any of four anti-viral drugs in the ambulatory setting linked with data from hospital and physician service claims was sent to each province. Where a response was encouraging a technical description of the data of interest was submitted.\\\\\\\\rRESULTS: Data were unavailable from the provinces of New Brunswick Newfoundland and Labrador and Prince Edward Island and inaccessible from British Columbia Manitoba and Ontario due to policies that prohibit collaborative work with pharmaceutical industry researchers. In Nova Scotia patient-level data were available but only on site. Data were accessible in Alberta Quebec and Saskatchewan although variation exists in the currency of the data time to obtain data approval requirements and insurance coverage eligibility.\\\\\\\\rCONCLUSIONS: As Canada moves towards a life-cycle management approach to drug regulation more post-marketing studies will be required potentially using administrative data. Linked patient-level drug and healthcare data are presently accessible to pharmaceutical industry researchers in four provinces although only logistically realistic in three and limited to seniors and low-income individuals in two. Collaborative endeavours to improve access to provincial data and to create other data resources should be encouraged. Copyright (c) 2009 John Wiley & Sons Ltd.","","","2009","10.1002/pds.1822","","","medline-19650154.pdf","medline-19650154"
"Differential privacy based on data provenance publishing method","Ni, W.-W. And Shen, T. And Yan, D.","Jisuanji Xuebao/Chinese Journal Of Computers","","Data provenance describes the mechanism and process of data generation and evolution, which records information about the node module executions used to produce concrete data items, as well as those intermediate data items acting as parameters passed between nodes' executions. Data provenance plays an important role in research and applications of data quality assessment, data recovery and data analysis. With increasingly deepening of data sharing, the need for publishing and sharing workflow of provenance, which is the main representation structure of data provenance, becomes increasingly urgent. However, the provenance workflow often contains private or confidential data. For instance, the node modules included in the provenance workflow, as well as the temporal relations among those nodes, may involve the privacy of the data owner. Direct release of provenance workflow will inevitably bring privacy protection issues. Privacy-preserving data provenance publication becomes an urgent problem to be solved. That is to say, the utility of published provenance workflow needs well maintaining under the premise that data privacy should not be disclosed. The existing research mainly focuses on the maintenance of the local mapping relationship of provenance workflow. For example, privacy protection process is implemented to the provenance workflow, which ensures that the degree of a single node would not be leaked, or sensitive mapping relationship would not be leaked in parallel with effective maintaining of provenance workflow's overall input and output mapping relations. For another important manifestation of provenance workflow's utility, i.e. temporal dependence among those front and corresponding back task nodes of the provenance workflow, the maintenance effect is relatively poor. As for the privacy of adjacent nodes distribution in the provenance workflow, the protection ability of existing methods is also far insufficient. To solve the above problems, the definition of input and output degree sequence with scale i model is introduced to describe the degree distribution of provenance workflow nodes. It provides a carrier for describing the utility and privacy-sensitive information of provenance workflow. As a by-product, it can also accommodate the extraction of directional characteristics of the provenance workflow well. The definition of previous-next sequence is further devised to describe the substructure distribution characteristics of the workflow. This structure can reduce the possible loss of workflow's temporal relations in adding differential noise. By constructing schema of previous-next sequence, the substructure characteristics of those nodes and their adjacent nodes in workflow are captured, and the temporal constraints in workflow are maintained during the reconstruction process. On this basis, a differential privacy-based on privacy-preserving provenance workflow publishing method dpripp is proposed to implement a weak background knowledge-dependent privacy-preserving provenance workflow publication, it can also provide well maintenance to temporal dependance relations of the provenance workflow. Targeted experiments are designed to verify the effectiveness and privacy protection effect of our proposal. The theoretical analysis and experimental results demonstrate that the proposed algorithm can effectively maintain both the local and global temporal dependence relations of nodes in the workflow, while protecting the directional distribution privacy of the locally adjacent nodes in the provenance workflow well. © 2020, science press. All right reserved.","","","2020","10.11897/sp.j.1016.2020.00573","","","scopus-2-s2.0-85083763182.pdf","scopus-2-s2.0-85083763182"
"Moving sport and exercise science forward: a call for the adoption of more transparent research practices","Caldwell A.r. And Vigotsky A.d. And Tenan M.s. And Radel R. And Mellor D.t. And Kreutzer A. And Lahart I.m. And Mills J.p. And Boisgontier M.p.","Sports Med","","The primary means of disseminating sport and exercise science research is currently through journal articles. However, not all studies, especially those with null findings, make it to formal publication. This publication bias towards positive findings may contribute to questionable research practices. Preregistration is a solution to prevent the publication of distorted evidence resulting from this system. This process asks authors to register their hypotheses and methods before data collection on a publicly available repository or by submitting a registered report. In the registered report format, authors submit a stage 1 manuscript to a participating journal that includes an introduction, methods, and any pilot data indicating the exploratory or confirmatory nature of the study. After a stage 1 peer review, the manuscript can then be offered in-principle acceptance, rejected, or sent back for revisions to improve the quality of the study. If accepted, the project is guaranteed publication, assuming the authors follow the data collection and analysis protocol. After data collection, authors re-submit a stage 2 manuscript that includes the results and discussion, and the study is evaluated on clarity and conformity with the planned analysis. In its final form, registered reports appear almost identical to a typical publication, but give readers confidence that the hypotheses and main analyses are less susceptible to bias from questionable research practices. From this perspective, we argue that inclusion of registered reports by researchers and journals will improve the transparency, replicability, and trust in sport and exercise science research. The preprint version of this work is available on sportr[formula: see text]iv: https://osf.io/preprints/sportrxiv/fxe7a/.","","","2020","10.1007/s40279-019-01227-1","","","embase-630830108.pdf","embase-630830108"
"Reporting format for economic evaluation. Part II: Focus on modelling studies","Nuijten M. J., Pronk M. H., Brorens M. J., Hekster Y. A., Lockefeer J. H., de Smet P. A., Bonsel G., van der Kuy A.","PharmacoEconomics","","This article presents the first version of a reporting format for modelling studies which is based on a general reporting format by our taskforce which was published in the previous issue of this journal. The use of decision-analytical models for economic evaluations is increasing because in practice it is not always possible to derive information from prospective studies. However the acceptance of modelling studies is generally lower than prospective studies not only because of the use of secondary data but also because the reports of modelling studies do not always have sufficient transparency. Hence a standardised reporting format may improve the transparency and consequently the acceptance of modelling studies. This article presents an example of a reporting format for economic evaluation based on modelling studies which may facilitate the development of future guidelines for modelling studies. The format consists of a number of headings which are followed by a brief recommendation on the content. This format does not deal with methodology and data management but especially addresses validation and quality assurance which may increase the transparency of the report.","","","1998","","","","medline-10186465.pdf","medline-10186465"
"A common data model for harmonization in the nordic pregnancy drug safety studies (norpress)","Cohen, J.m. And Cesta, C.e. And Kjerpeseth, L. And Leinonen, M.k. And Hálfdánarson, Ó. And Karlstad, Ø. And Karlsson, P. And Andersen, M. And Furu, K. And Hjellvik, V.","Norsk Epidemiologi","","It is necessary to carry out large observational studies to generate robust evidence about the safety of drugs used during pregnancy. In the nordic countries, nationwide population-based health registers that document all births and dispensed prescribed drugs are valuable resources for such studies. A common data model (cdm) is a data harmonization and structuring tool that enables a unified and streamlined analytic approach for studies including data from multiple countries or databases. We describe a cdm developed for the nordic pregnancy drug safety studies (norpress), including details on data sources and structure of the data tables. We also provide an overview of the advantages and disadvantages of the approach (e.g. sharing of data analysis programs versus extra initial work to create cdm datasets from raw data). © 2021, norwegian epidemiological society. All rights reserved.","","","2021","10.5324/nje.v29i1-2.4053","","","scopus-2-s2.0-85113346235.pdf","scopus-2-s2.0-85113346235"
"Influence of parameter settings in voxel-based morphometry 8. Using DARTEL and region-of-interest on reproducibility in gray matter volumetry","Goto M., Abe O., Aoki S., Hayashi N., Miyati T., Takao H., Matsuda H., Yamashita F., Iwatsubo T., Mori H., Kunimatsu A., Ino K., Yano K., Ohtomo K.","Methods of Information in Medicine","","OBJECTIVES: To investigate whether reproducibility of gray matter volumetry is influenced by parameter settings for VBM 8 using Diffeomorphic Anatomical Registration Through Exponentiated Lie Algebra (DARTEL) with region-of-interest (ROI) analyses.\\\\\\\\rMETHODS: We prepared three-dimensional T1-weighted magnetic resonance images (3D-T1WIs) of 21 healthy subjects. All subjects were imaged with each of five MRI systems. Voxel-based morphometry 8 (VBM 8) and WFU PickAtlas software were used for gray matter volumetry. The bilateral ROI labels used were those provided as default settings with the software: Frontal Lobe Hippocampus Occipital Lobe Orbital Gyrus Parietal Lobe Putamen and Temporal Lobe. All 3D-T1WIs were segmented to gray matter with six parameters of VBM 8 with each parameter having between three and eight selectable levels. Reproducibility was evaluated as the standard deviation (mm3) of measured values for the five MRI systems.\\\\\\\\rRESULTS: Reproducibility was influenced by 'Bias regularization (BiasR)' 'Bias FWHM' and 'De-noising filter' settings but not by 'MRF weighting' 'Sampling distance' or 'Warping regularization' settings. Reproducibility in BiasR was influenced by ROI. Superior reproducibility was observed in Frontal Lobe with the BiasR1 setting and in Hippocampus Parietal Lobe and Putamen with the BiasR3* BiasR1 and BiasR5 settings respectively.\\\\\\\\rCONCLUSION: Reproducibility of gray matter volumetry was influenced by parameter settings in VBM 8 using DARTEL and ROI. In multi-center studies the use of appropriate settings in VBM 8 with DARTEL results in reduced scanner effect.","","","2015","10.3414/me14-01-0049","","","medline-25345402.pdf","medline-25345402"
"Can citizen science increase trust in research? A case study of delineating polish metropolitan areas","Bedessem, B. And Gawrońska-Novak, B. And Lis, P.","Journal Of Contemporary European Research","","We assess the relationship between citizens’ participation in scientific research and public trust in research results within social sciences. We conduct an online citizen science quasiexperiment concerning the delineation of metropolitan areas of poland’s two major cities. It consists of two stages. In stage one, participants in one region are exposed to citizen science and directly involved in delineating the boundaries of their local metropolitan area. In stage two, we add another region in which participants are not involved in the research process. In both regions we ask the participants to evaluate the level of their trust in the presented maps of respective metropolitan areas: based on citizen science in one region and historical data regression analysis in the other region. Our contribution to the literature lies in two areas. First, we demonstrate how citizen science can be used in urban studies to delineate boundaries of urban and metropolitan areas exhibiting strong functional connections. Second, we show that the participation of local residents in the research process increases public trust in the study results compared to non-participatory ‘traditional academic’ research. These results confirm that citizen science programs deserve to be strongly supported by european institutions as a possible means to resolving the credibility crisis of science, research and evidence-based policies. © 2021, journal of contemporary european research. All rights reserved.","","","2021","10.30950/jcer.v17i2.1185","","","scopus-2-s2.0-85109433137.pdf","scopus-2-s2.0-85109433137"
"A protocol for the development of the STROCSS guideline: Strengthening the Reporting of Cohort Studies in Surgery","Agha R. A., Borrelli M. R., Vella-Baldacchino M., Thavayogan R., Orgill D. P.","International Journal of Surgery Protocols","","INTRODUCTION: Strengthening the reporting of observational studies in epidemiology (STROBE) coined in 2007 highlighted the importance of improving the quality of observational research by providing an item checklist in order to avoid inadequate reporting of research. However currently there are no reporting guidelines specific to surgical cohort studies which have an extremely important role within the surgical literature. The recent development of surgery specific guidelines has underscored how surgical and procedural interventions require additional detail for readers to have a complete clear transparent and reproducible understanding. The objective of this research is to conduct a Delphi consensus exercise to develop the STROCSS guideline (Strengthening the Reporting of Cohort Studies in Surgery). METHODS AND ANALYSIS: Current guidelines for case series (PROCESS) Cohort Studies (STROBE) and randomised controlled trials (CONSORT) will be analysed to compile items to form baseline material for developing cohort guidelines in the Delphi consensus exercise. The Delphi questionnaire will be administered via Google Forms and conducted using standard Delphi Methodology. Surgeons and individuals with significant experience of reviewing cohort studies as well as those with experience in developing reporting guidelines will be invited to participate. In the first round existing items from PROCESS and STROBE will be put forward and participants will be invited to augment them or contribute further items for consideration. The provisional guidelines will then be updated in successive rounds using the nine-point Likert scale as proposed by the Grading Recommendations Assessment Development and Evaluations (GRADE) working group. This process will be used to agree Standard definitions for the outcomes. DISSEMINATION: The work will be published in a peer-reviewed journal and presented at national and international meetings. Findings will be disseminated to interested parties and journals will be encouraged to endorse the reporting guidelines.","","","2017","10.1016/j.isjp.2017.08.001","","","pubmed-31851747.pdf","pubmed-31851747"
"Proposal for a standardized PSA doubling-time calculation","Ponholzer A., Popper N., Breitenecker F., Schmid H. P., Albrecht W., Loidl W., Madersbacher S., Schramek P., Semjonow A., Rauchenwald M.","Anticancer Research","","PURPOSE: Prostate-specific antigen (PSA) doubling-time (PSA-DT) is an important indicator of progression and survival in men with prostate cancer. Three major limitations regarding PSA-DT determination may lead to inconsistent results: the variety of mathematical methods currently applied the non-standardized handling of input variables and the potential lack of accuracy due to PSA variability. The aim of this project was to develop a reproducible PSA-DT determination tool which simultaneously provides a PSA-DT error estimation.\\\\\\\\rMATERIALS AND METHODS: An internet-based PSA-DT calculation tool via nonlinear optimization implementing the least squares error method using the most recent three PSA values was developed. PSA-DT calculation error is estimated via randomly disturbed measurement data streams (n=65) based on a variable (5-25%) PSA variability.\\\\\\\\rRESULTS: According to a simulation in five men PSA-DT was calculated to be between 1.7 and 15 month (mean: 6.3 month) and determined with another standard tool between 1.3 and 14.5 month (mean: 4.2 month).\\\\\\\\rCONCLUSION: We present a defined open and reproducible PSA-DT calculation and PSA-DT error estimation tool based on a standardized PSA data input. This tool is not better compared to other methods but is scientifically standardized and freely accessible via the following internet address: http://adam.drahtwarenhandlung.at/webapp/mg2008/chapter_prostata4/example_psa.","","","2010","","","","medline-20592353.pdf","medline-20592353"
"Look beyond financial conflicts of interest in evaluating industry-academia collaborations in burden-of-illness and outcomes research studies in dermatology","Prendergast, M.m. And Abramovits, W. And Boguniewicz, M. And Lebwohl, M. And Tokar, M. And Tong, K.b.","Journal Of Investigative Dermatology","","Financial relationships exist among industry, scientific investigators, and academic medical centers. These relationships can foster research in the basic sciences, clinical trials, health economics evaluations, and other outcomes assessment studies. To govern the conduct of burden-of-illness and outcomes research studies involving collaborations between industry and academia, we propose voluntary standards related to: 1) the development of and adherence to standards for research conduct and reporting;  2) disclosure, discussion, and management of potential impacts of financial conflicts of interest;  and 3) transparency in research methods and open access to study results.","","","2004","10.1111/j.0022-202x.2004.09116.x","","","scopus-2-s2.0-4143070415.pdf","scopus-2-s2.0-4143070415"
"The role of metadata in reproducible computational research","Leipzig J., Nust D., Hoyt C. T., Ram K., Greenberg J.","Patterns","","Reproducible computational research (RCR) is the keystone of the scientific method for in silico analyses packaging the transformation of raw data to published results. In addition to its role in research integrity improving the reproducibility of scientific studies can accelerate evaluation and reuse. This potential and wide support for the FAIR principles have motivated interest in metadata standards supporting reproducibility. Metadata provide context and provenance to raw data and methods and are essential to both discovery and validation. Despite this shared connection with scientific data few studies have explicitly described how metadata enable reproducible computational research. This review employs a functional content analysis to identify metadata standards that support reproducibility across an analytic stack consisting of input data tools notebooks pipelines and publications. Our review provides background context explores gaps and discovers component trends of embeddedness and methodology weight from which we derive recommendations for future work.","","","2021","10.1016/j.patter.2021.100322","","","pubmed-34553169.pdf","pubmed-34553169"
"Multidimensional study on users’ evaluation of the kraken personal data sharing platform","Gabrielli, S. And Rizzi, S. And Mayora, O. And More, S. And Baun, J.c.p. And Vandevelde, W.","Applied Sciences (Switzerland)","","Background: recent advances in the design of blockchain-based personal data sharing platforms bring the benefit of empowering users with more control and privacy-preserving measures in sharing data products. However, so far very little is known about users’ intentions to adopt such platforms for providing or consuming data products. Objective: this study aims to investigate users’ main expectations, preferences, and concerns regarding the adoption of blockchain-based personal data sharing platforms in the health and education domains. Methods: fifteen participants were involved in a multidimensional evaluation of a prototyped release of the kraken blockchain-based data sharing platform and asked to assess it in the health or education pilot domains. Data collected during online group interviews with participants were analyzed by applying the micro interlocutor technique to provide a descriptive overview of participant responses. Results: participants showed a marginal acceptance of the prototype usability, asking for some improvements of the user experience and for a more transparent presentation of the platform security and privacy preserving capabilities. Participants expressed interest in using the platform as data providers and consumers as well as setting privacy policies for sharing data products with third parties, including the possibility of revoking access to data. Conclusions: blockchain-based data sharing platforms are more likely to engage target users when technical design is informed by a deeper knowledge of their needs, expectations, and relevant concerns. © 2022 by the authors. Licensee mdpi, basel, switzerland.","","","2022","10.3390/app12073270","","","scopus-2-s2.0-85127516542.pdf","scopus-2-s2.0-85127516542"
"Maximize transparency of clinical trial from trial registry to results disclosure. [Chinese]","Chen J. C.","Chinese Journal of New Drugs","","Clinical trial results disclosure and data sharing can fulfill the value of clinical trial. This paper introduce the history and current trends of the global clinical trial disclosure including the policies development of EMA FDA ICMJE and WHO. Given the advantage of the full transparency of clinical trial China should put more efforts on the clinical trial disclosure and improve the transparency of clinical trial such as the CFDA establish the related policies to guide the clinical trial results disclosure on official registry platform and the National Fund Committee enforce clinical data sharing as the requirement for the fund of academic clinical trials in order to further improve the transpareney of domestic clinical trials. Copyright © 2018 Chinese Journal of New Drugs Co. Ltd. All right reserved.","","","2018","","","","unknown-1281.pdf","unknown-1281"
"Creating an academic research organization to efficiently design conduct coordinate and analyze clinical trials: The Center for Clinical Trials & Data Coordination","Abebe K. Z., Althouse A. D., Comer D., Holleran K., Koerbel G., Kojtek J., Weiss J., Spillane S.","Contemporary Clinical Trials Communications","","When properly executed the randomized controlled trial is one of the best vehicles for assessing the effectiveness of one or more interventions. However numerous challenges may emerge in the areas of study startup recruitment data quality cost and reporting of results. The use of well-run coordinating centers could help prevent these issues but very little exists in the literature describing their creation or the guiding principles behind their inception. The Center for Clinical Trials & Data Coordination (CCDC) was established in 2015 through institutional funds with the intent of 1) providing relevant expertise in clinical trial design conduct coordination and analysis; 2) advancing the careers of clinical investigators and CCDC-affiliated faculty; and 3) obtaining large data coordinating center (DCC) grants. We describe the organizational structure of the CCDC as well as the homegrown clinical trial management system integrating nine crucial elements: electronic data capture eligibility and randomization drug and external data tracking safety reporting outcome adjudication data and safety monitoring statistical analysis and reporting data sharing and regulatory compliance. Lastly we share numerous lessons that can be taken from our experience. Specifically we focus on 1) funding for DCCs 2) the importance of DCCs to clinical researchers 3) the expertise of DCC personnel and 4) continually striving to improve. In conclusion the CCDC strives to provide high-quality support for the design conduct coordination and analyses of clinical trials and we hope this paper will serve as a blueprint for future clinical trialists involved in DCCs.","","","2019","10.1016/j.conctc.2019.100488","","","pubmed-31763494.pdf","pubmed-31763494"
"Comparability and reproducibility of biomedical data","Huang, Y. And Gottardo, R.","Briefings In Bioinformatics","","With the development of novel assay technologies, biomedical experiments and analyses have gone through substantial evolution. Today, a typical experiment can simultaneously measure hundreds to thousands of individual features (e.g. genes) in dozens of biological conditions, resulting in gigabytes of data that need to be processed and analyzed. Because of the multiple steps involved in the data generation and analysis and the lack of details provided, it can be difficult for independent researchers to try to reproduce a published study. With the recent outrage following the halt of a cancer clinical trial due to the lack of reproducibility of the published study, researchers are now facing heavy pressure to ensure that their results are reproducible. Despite the global demand, too many published studies remain non-reproducible mainly due to the lack of availability of experimental protocol, data and/or computer code. Scientific discovery is an iterative process, where a published study generates new knowledge and data, resulting in new follow-up studies or clinical trials based on these results. As such, it is important for the results of a study to be quickly confirmed or discarded to avoid wasting time and money on novel projects. The availability of high-quality, reproducible data will also lead to more powerful analyses (or meta-analyses) where multiple data sets are combined to generate new knowledge. In this article, we review some of the recent developments regarding biomedical reproducibility and comparability and discuss some of the areas where the overall field could be improved. © the author 2012. Published by oxford university press.","","","2013","10.1093/bib/bbs078","","","scopus-2-s2.0-84889004965.pdf","scopus-2-s2.0-84889004965"
"Sharing of individual participant data from clinical trials: general comparison and hiv use case","Mayer, C.s. And Williams, N. And Gold, S. And Fung, K.w. And Huser, V.","Amia ... Annual Symposium Proceedings. Amia Symposium","","Sharing of individual participant data is encouraged by the international committee of medical journal editors. We analyzed clinical trial registry data from clinicaltrials.gov (ctg) and determined the proportion of trials sharing de-identified individual participant data (ipd). We looked at 3,138 medical conditions (as medical subject heading terms). Overall, 10.8% of trials with first registration date after december 1, 2015 answered 'yes' to plan to share de-identified ipd data. This sharing rate ranges between 0% (biliary tract neoplasms) to 72.2% (meningitis, meningococcal) when analyzed by disease that is focus of a study. Via a predictive model, we found that studies that deposited basic summary results data to ctg results registry, large studies and phase 3 interventional studies are most likely to declare intent to share ipd data. As part of an hiv common data element analysis project, we further compared a body of hiv trials (24% sharing rate) to other diseases. ©2019 amia - all rights reserved.","","","2019","","","","scopus-2-s2.0-85083812089.pdf","scopus-2-s2.0-85083812089"
"Towards a paradigm for open and free sharing of scientific data on global change science in china","Peng, C. And Song, X. And Jiang, H. And Zhu, Q. And Chen, H. And Chen, J.m. And Gong, P. And Jie, C. And Xiang, W. And Yu, G. And Zhou, X.","Ecosystem Health And Sustainability","","Despite great progress in data sharing that has been made in china in recent decades, cultural, policy, and technological challenges have prevented chinese researchers from maximizing the availability of their data to the global change science community. To achieve full and open exchange and sharing of scientific data, chinese research funding agencies need to recognize that preservation of, and access to, digital data are central to their mission, and must support these tasks accordingly. The chinese government also needs to develop better mechanisms, incentives, and rewards, while scientists need to change their behavior and culture to recognize the need to maximize the usefulness of their data to society as well as to other researchers. The chinese research community and individual researchers should think globally and act personally to promote a paradigm of open, free, and timely data sharing, and to increase the effectiveness of knowledge development. © 2016, © 2016 peng et al.","","","2016","10.1002/ehs2.1225","","","scopus-2-s2.0-85066468635.pdf","scopus-2-s2.0-85066468635"
"System for quality-assured data analysis: flexible, reproducible scientific workflows","Fowler J. And San Lucas F.a. And Scheet P.","Genet. Epidemiol","","The reproducibility of scientific processes is one of the paramount problems of bioinformatics, an engineering problem that must be addressed to perform good research. The system for quality-assured data analysis (syqada), described here, seeks to address reproducibility by managing many of the details of procedural bookkeeping in bioinformatics in as simple and transparent a manner as possible. Syqada has been used by persons with backgrounds ranging from expert programmer to unix novice, to perform and repeat dozens of diverse bioinformatics workflows on tens of thousands of samples, consuming over 80 cpu-months of computing on over 300,000 individual tasks of scores of projects on laptops, computer servers, and computing clusters. Syqada is especially well-suited for paired-sample analyses found in cancer tumor-normal studies. Syqada executable source code, documentation, tutorial examples, and workflows used in our lab is available from http://scheet.org/software.html.copyright © 2018 wiley periodicals, inc.","","","2019","10.1002/gepi.22178","","","embase-625571835.pdf","embase-625571835"
"Ensuring distributed accountability for data sharing in cloud","Karthick, K. And Jennifer, P. And Muthukumaravel, A.","Middle - East Journal Of Scientific Research","","Cloud computing enables highly scalable services to be easily consumed over the internet on an as-needed basis. A major feature of the cloud services is that users' data are usually processed remotely in unknown machines that users do not own or operate. While enjoying the convenience brought by this new emerging technology, users' fears of losing control of their own data (particularly, financial and health data) can become a significant barrier to the wide adoption of cloud services. To address this problem, in this paper, we propose a novel highly decentralized information accountability framework to keep track of the actual usage of the users' data in the cloud. In particular, we propose an object-centered approach that enables enclosing our logging mechanism together with users' data and policies. We leverage the jar programmable capabilities to both create a dynamic and traveling object and to ensure that any access to users' data will trigger authentication and automated logging local to the jars. To strengthen user's control, we also provide distributed auditing mechanisms. We provide extensive experimental studies that demonstrate the efficiency and effectiveness of the proposed approaches. © idosi publications, 2014.","","","2014","10.5829/idosi.mejsr.2014.20.06.11389","","","scopus-2-s2.0-84894623202.pdf","scopus-2-s2.0-84894623202"
"The sharing economy and its paradoxes: a sociological study of sharing communities in russia","Shmidt, M.","Mir Rossii","","Over the past decade there has been an enormous rise in alternative forms of economic organization, such as the sharing economy - an under-theorized and contradictory empirical phenomenon. The paper studies the variety of interaction practices and motivations for participation and identifies common and specific features of self-organization by comparing three platforms: darudar (sharing goods), bank vremeni [time bank] (sharing time and services) and couchsurfing (sharing accommodation and leisure). The data, which was triangulated, includes: (i) 25 in-depth interviews conducted with experts and active users of the platforms, (ii) ethnography from participant observation of users' offline meetings, (iii) systematic online observation. This study employs a blended ethnography/ netnography approach - studying the sharing economy communities both online and face-to-face to provide 'thick' description of community-building. We theorize that sharing in the sharing economy is a separate principle of resource allocation, which is characterized by the priority of goods over the structure of relations between parties. In contrast to the reciprocity principle, the recipient in sharing is selected with respect to a fixed amount of resources which the donor possesses. Sharing is moving far beyond the boundaries of kindred, friend, partner or other personal relationships, as far as the counterparty is selected among the participants of an extended network of social contacts. The circle of people who can enjoy the benefits of a joint resource expands to the many thousands of users of the virtual sharing platform. What motivates the well-resourced users of the sharing economy platforms, who possess economic and cultural capital, to become practitioners of sharing? Aspiration for communitybuilding, deriving from the extrapolation of the self to the aggregate level: the 'extended self'. Sharing contributes to a sense of an imaginary community, making ourselves an integral part. Practically, sharing transforms into a ritual chain: from the preparation of resources for exchange to the choice of counteragent, communication before the act of sharing, during and after, all of which create a full part of social life. When offering to share material and immaterial objects, participants of the platforms offer a part of themselves - talents and opportunities, communication and empathy, belonging to cultural tradition. In return, they receive a means of reducing loneliness and overcoming social alienation. © hse, 1992-2018. В фокусе исследования находится экономика совместного потребления (sharing economy) - явление, получившее неоднозначную трактовку в существующей ли- тературе. То, что выводит экономику совместного потребления на совершенно новый уровень - это диджитализация. Интернет-платформы - медиаторы между частными пользователями - анонимизируют участников сделок и переносят обмен в мир расширенных сетей, соединяющих людей с большим количеством незна- комцев, тем самым создавая сообщество, в котором никто не знает друг друга по- именно, но обладает правом вкладывать ресурс и пользоваться ресурсами других. Вопрос, которым мы задаемся в предложенном исследовании, - почему при всех рисках, связанных с декоммерциализацией рыночных отношений в экономике со- вместного потребления, количество ее пользователей неуклонно увеличивается? Что мотивирует ресурсообеспеченных пользователей, обладающих высоким уров- нем не только экономического, но и культурного капитала, включаться в экономи- ку совместного потребления? Мы ставим перед собой следующую цель: сравнить разнообразие практик взаимодействия и мотивацию пользователей трех платформ совместного потребления (дарудар, банк времени, couchsurfing), а также выявить общие и специфические черты самоорганизации сообществ. Исследование опирается на методологию этнографического подхода и нет- нографии, использующейся для анализа культур цифровых сообществ. Методом сбора данных является проведение 25 глубинных интервью, а также включенное наблюдение на общих встречах участников сообществ. Стремясь уйти от конвенциональной рамки рассмотрения внеэкономических форм обмена через стратегию выживания, основанную на подключении личных связей, мы предположили, что «совместность» в экономике совместного потребле- ния - это обособленный принцип распределения ресурсов, характеризующийся первичностью блага, а не характерных особенностей отношений между донором и реципиентом. Входным билетом в сообщества становится предложение соб- ственности или труда. Три кейса, которые мы отобрали для полевого исследова- ния, отвечают этой специфике: дарудар, площадка встречи спроса и предложения на отчуждаемое от себя благо;  couchsurfing, платформа, создающая глобальный «жилищный фонд», временный доступ к которому получает любой зарегистри- рованный участник;  и, наконец, банк времени, способ организации трудовой дея- тельности на безвозмездной основе. Качественно разные по своей организации случаи показали: тесное перепле- тение формальных правил и неформальных практик определяет индивидуальные стратегии взаимодействия. Принцип совместности приводится в жизнь тремя структурными категориями: первая - это накопление критической массы, точки, в которой система становится достаточно инертной для того, чтобы поддержи- ваться за счет достаточного количества участников и разнообразия ресурсного по- тенциала;  вторая - гетерогенный (с точки зрения стиля жизни) социальный пор- трет участия;  третья - способность к саморегулированию. Участники четко осознают свою позицию в сообществе, понимая, что ситу- ация обмена, в которую они себя поставили, отличается как от рынка, так и от дачного кооператива. Что мотивирует ресурсообеспеченных пользователей ста- новиться практиками совместного потребления? Очевидно, что это стремление к сообществлению, заключающемуся в выводу своего «я» на агрегатный уро- вень, включению в него того, с кем участник разделяет благо. И акт разделения, и чувство совместного обладания совершенствуют ощущение принадлежности к воображаемому сообществу потребления, делают наше «я» его неотъемлемой ча- стью. В практическом смысле совместное потребление превращается в цепочку повторяющихся манипуляций: подготовку ресурсов к обмену, выбору реципиента, а также коммуникацию до, во время и после непосредственного обмена, форми- рующих полноценную часть социальной жизни. Делая предложение поделиться материальными и нематериальными объектами, участник движения предлагает реципиентам часть себя - таланты и возможности, способность к коммуникации, эмпатию, принадлежность к куль урной традиции, взамен получая способ скра- сить одиночество и укрепить чувство единения.","","","2019","10.17323/1811-038x-2019-28-2-148-171","","","scopus-2-s2.0-85065098886.pdf","scopus-2-s2.0-85065098886"
"Impacts of data synthesis: a metric for quantifiable data standards and performances","Chandra, G. And Siirtola, P. And Tamminen, S. And Knip, M.j. And Veijola, R. And Röning, J.","Data","","Clinical data analysis could lead to breakthroughs. However, clinical data contain sensitive information about participants that could be utilized for unethical activities, such as blackmailing, identity theft, mass surveillance, or social engineering. Data anonymization is a standard step during data collection, before sharing, to overcome the risk of disclosure. However, conventional data anonymization techniques are not foolproof and also hinder the opportunity for personalized evaluations. Much research has been done for synthetic data generation using generative adversarial networks and many other machine learning methods;  however, these methods are either not free to use or are limited in capacity. This study evaluates the performance of an emerging tool named synthpop, an r package producing synthetic data as an alternative approach for data anonymization. This paper establishes data standards derived from the original data set based on the utilities and quality of information and measures variations in the synthetic data set to evaluate the performance of the data synthesis process. The methods to assess the utility of the synthetic data set can be broadly divided into two approaches: general utility and specific utility. General utility assesses whether synthetic data have overall similarities in the statistical properties and multivariate relationships with the original data set. Simultaneously, the specific utility assesses the similarity of a fitted model’s performance on the synthetic data to its performance on the original data. The quality of information is assessed by comparing variations in entropy bits and mutual information to response variables within the original and synthetic data sets. The study reveals that synthetic data succeeded at all utility tests with a statistically non-significant difference and not only preserved the utilities but also preserved the complexity of the original data set according to the data standard established in this study. Therefore, synthpop fulfills all the necessities and unfolds a wide range of opportunities for the research community, including easy data sharing and information protection. © 2022 by the authors.","","","2022","10.3390/data7120178","","","scopus-2-s2.0-85144599100.pdf","scopus-2-s2.0-85144599100"
"Not Normal: the uncertainties of scientific measurements","Bailey D. C.","Royal Society Open Science","","Judging the significance and reproducibility of quantitative research requires a good understanding of relevant uncertainties but it is often unclear how well these have been evaluated and what they imply. Reported scientific uncertainties were studied by analysing 41 000 measurements of 3200 quantities from medicine nuclear and particle physics and interlaboratory comparisons ranging from chemistry to toxicology. Outliers are common with 5sigma disagreements up to five orders of magnitude more frequent than naively expected. Uncertainty-normalized differences between multiple measurements of the same quantity are consistent with heavy-tailed Student's t-distributions that are often almost Cauchy far from a Gaussian Normal bell curve. Medical research uncertainties are generally as well evaluated as those in physics but physics uncertainty improves more rapidly making feasible simple significance criteria such as the 5sigma discovery convention in particle physics. Contributions to measurement uncertainty from mistakes and unknown problems are not completely unpredictable. Such errors appear to have power-law distributions consistent with how designed complex systems fail and how unknown systematic errors are constrained by researchers. This better understanding may help improve analysis and meta-analysis of data and help scientists and the public have more realistic expectations of what scientific results imply.","","","2017","10.1098/rsos.160600","","","medline-28280557.pdf","medline-28280557"
"Reproducibility in clinical psychology","Hopwood, Christopher J And Vazire, Simine","","","This chapter describes the importance of reproducibility in the scientific method. It reviews recent issues in the social sciences that contributed to the recognition of reproducibility problems, and presents the best practices for conducting reproducible research in clinical psychology. The scientific method is one way of explaining phenomena. It can be contrasted with other methods, such as explanation via tradition or metaphysics, by a few foundational principles. The chapter briefly reviews the distinguishing principles of the scientific method, with a focus on reproducibility. It describes a way to do psychological research that is more likely to lead to reproducible findings. In this alternative approach, scientists think through and preregister studies. They make all methods and data open to the public to the extent possible. Studies have adequate power and use maximally precise measures. Null effects are valued as much as positive effects, and the question shifts from ""is this effect real?"" To ""how substantial is the effect and how precisely can we estimate it?"" Effects are routinely replicated. Journals and universities adjust their incentive structures to reward reproducible science, and the popular press and media follow suit. (Psycinfo database record (c) 2022 apa, all rights reserved)","","","2020","10.1017/9781316995808.035","","","psychinfo-2020-10992-028.pdf","psychinfo-2020-10992-028"
"Comparison of radiosotopic T3 and T4 kits","Ravel R., Donnellan A. M.","Amer","","Reproducibility data on various T 3 and T 4 kits are presented. This information is useful in evaluating new kits and interpreting patient results. A plea is made for more attention to reproducibility studies by publications and by clinical or nuclear medicine laboratories.","","","1973","","","","unknown-2158.pdf","unknown-2158"
"Guidelines for Proper Reporting of Clinical Significance Including Minimal Clinically Important Difference Patient Acceptable Symptomatic State Substantial Clinical Benefit and Maximal Outcome Improvement","Harris J. D., Brand J. C., Cote M., Waterman B., Dhawan A.","Arthroscopy","","Patient-reported outcome measures (PROM) need to be responsive reliable and validated for the specific condition or treatment. PROMs also need to exhibit a dose-dependent response across a diverse patient population unlimited by floor and ceiling effects. Statistically significant differences between compared groups might not always represent clinically important differences. Measures of clinical significance reflect a spectrum of patient satisfaction after an intervention. A noticeable difference to the patient is assessed with minimal clinically important difference (MCID) patient satisfaction by patient acceptable symptomatic state (PASS) and a ""considerable"" improvement by substantial clinical benefit (SCB). Clinical relevance measured by these clinically significant outcomes (CSO) are limited by ceiling effects. Maximal outcome improvement (MOI) might more accurately account for patients with higher baseline or preoperative PROMs thereby limiting ceiling effects. The acts of measuring (and reporting) patient-centered endpoints may actually be of greater importance than collecting objective clinician-measured data. As the old surgeon's aphorism goes ""nothing ruins good results like good follow-up."" Copyright © 2022 Arthroscopy Association of North America. Published by Elsevier Inc. All rights reserved.","","","2023","10.1016/j.arthro.2022.08.020","","","medline-36603987.pdf","medline-36603987"
"What drives developing countries to select free open source software for national spatial data infrastructure?","Choi, J. And Hwang, M.-H. And Kim, H. And Ahn, J.","Spatial Information Research","","While there are many discussions that free open source software for geospatial could provide a cost-effective solution for the construction and management of national spatial data infrastructures, little empirical evidence exists of what actually leads developing countries to adopt such software. To fill this void in the literature, this study fulfills an empirical assessment of the main factors that affect the adoption of free open source geospatial software in developing countries. In particular, the study uses the analytical hierarchy process method to evaluate how the functional, economic and public values of free open source geospatial software contribute to the software selection of developing countries. A survey for 10 respondents from 9 asian and latin american countries revealed that economic values were the most important, functional values the second most, and public values the least important factor. The survey also showed that the adoption rate of free open source geospatial software would be similar for different purposes of national spatial data infrastructures such as data management, sharing, utilization, and servicing. The results of the study mean that developing countries, to date, want to introduce free open source software for national spatial data infrastructures mainly from economic motivations. This finding was possible since the study took a comprehensive approach to the adoption of free open source geospatial software, in contrast to other studies that often focused only on software engineering aspects. © 2016, korean spatial information society.","","","2016","10.1007/s41324-016-0051-9","","","scopus-2-s2.0-85080857846.pdf","scopus-2-s2.0-85080857846"
"Reporting quality of social and psychological intervention trials: A systematic review of reporting guidelines and trial publications","Grant Sean P., Mayo-Wilson Evan, Melendez-Torres G., Montgomery Paul","PLoS ONE Vol 8(5) 2013 ArtID e65442","","Background: Previous reviews show that reporting guidelines have improved the quality of trial reports in medicine yet existing guidelines may not be fully suited for social and psychological intervention trials. Objective/Design: We conducted a two-part study that reviewed (1) reporting guidelines for and (2) the reporting quality of social and psychological intervention trials.","","","2013","10.1371/journal.pone.0065442","","","medline-23734256.pdf","medline-23734256"
"Creating context for the experiment record. User-defined metadata: investigations into metadata usage in the LabTrove ELN","Willoughby C., Bird C. L., Coles S. J., Frey J. G.","Journal of Chemical Information & Modeling","","The drive toward more transparency in research the growing willingness to make data openly available and the reuse of data to maximize the return on research investment all increase the importance of being able to find information and make links to the underlying data. The use of metadata in Electronic Laboratory Notebooks (ELNs) to curate experiment data is an essential ingredient for facilitating discovery. The University of Southampton has developed a Web browser-based ELN that enables users to add their own metadata to notebook entries. A survey of these notebooks was completed to assess user behavior and patterns of metadata usage within ELNs while user perceptions and expectations were gathered through interviews and user-testing activities within the community. The findings indicate that while some groups are comfortable with metadata and are able to design a metadata structure that works effectively many users are making little attempts to use it thereby endangering their ability to recover data in the future. A survey of patterns of metadata use in these notebooks together with feedback from the user community indicated that while a few groups are comfortable with metadata and are able to design a metadata structure that works effectively many users adopt a ""minimum required"" approach to metadata. To investigate whether the patterns of metadata use in LabTrove were unusual a series of surveys were undertaken to investigate metadata usage in a variety of platforms supporting user-defined metadata. These surveys also provided the opportunity to investigate whether interface designs in these other environments might inform strategies for encouraging metadata creation and more effective use of metadata in LabTrove.","","","2014","10.1021/ci500469f","","","medline-25405258.pdf","medline-25405258"
"Reproducibility of small-format laboratory cells","Luc, P.-M. And Buchwald, F. And Kowal, J.","Energies","","For the research and development of new battery materials, achieving high reproducibility of the performance parameters in the laboratory test cells is of great importance. Therefore, in the present work, three typical small-format lithium-ion cells (coin cell, swagelok cell and el-cell ecc-pat-core) were tested and compared with regard to the reproducibility of their performance parameters (discharge capacity, internal resistance and coulombic efficiency). A design of experiments (doe) with the two factors separator type and anode–cathode ratio (n/p ratio) was carried out for all cells. For the quality features discharge capacity, internal resistance and coulombic efficiency, the coefficient of variation is used as a measure of reproducibility. The statistical evaluation shows that in 83% of all cases, higher reproducibility is achieved when the freudenberg separator is used instead of the celgard separator. In addition, higher reproducibility is achieved in 78% of all cases if the anode and cathode are the same size. A general statement about which test cell format has the highest reproducibility cannot be made. Rather, the format selection should be adapted to the requirements. The examined factors seem to have an influence on the reproducibility but are more insignificant than other still-unknown factors. Since the production of small-format test cells is a manual process, the competence of the assembler seems to prevail. In order to mitigate the influence of as many unknown variables as possible, assembly instructions are proposed for each cell type. © 2022 by the authors.","","","2022","10.3390/en15197333","","","scopus-2-s2.0-85139907516.pdf","scopus-2-s2.0-85139907516"
"Spase 2.0: a standard data model for space physics","King, T. And Thieman, J. And Roberts, D.a.","Earth Science Informatics","","Spase-for space physics archive search and extract-is a group with a charter to promote collaboration and sharing of data for the space plasma physics community. A major activity is the definition of the spase data model which defines the metadata necessary to describe resources in the broader heliophysics data environment. The spase data model is primarily a controlled vocabulary with hierarchical relationships and with the ability to form associations between described resources. It is the result of many years of effort by an international collaboration (see http://www. spase-group. org) to unify and improve on existing space and solar physics data models. The genesis of the spase group can be traced to 1998 when a small group of individuals saw a need for a data model. Today spase has a large international participation from many of the major space research organizations. The design of the data model is based on a set of principles derived from evaluation of the existing heliophysics data environment. The development guidelines for the data model are consistent with iso-2788 (expanded in ansi/niso z39.19) and the administration for the data model is comparable to that described in the iso standards iso-11179 and iso-20943. Since the release of version 1.0 of the data model in 2005, the model has undergone a series of evolutions. Spase released version 2.0 of its data model in april 2009. This version presents a significant change from the previous release. It includes the capability to describe a wider range of data products and to describe expert annotations which can be associated with a resource. Additional improvements include an enhanced capability to describe resource associations and a more unified approach to describing data products. Version 2.0 of the spase data model provides a solid foundation for continued integration of worldwide research activities and the open sharing of data. © 2010 springer-verlag.","","","2010","10.1007/s12145-010-0053-4","","","scopus-2-s2.0-77955677172.pdf","scopus-2-s2.0-77955677172"
"Messing with Merton: The intersection between open science practices and Mertonian values","Hosseini M., Senabre Hidalgo E., Horbach Spjm, Guttinger S., Penders B.","Accountability in Research","","Although adherence to Mertonian values of science (i.e. communism universalism organized skepticism disinterestedness) is desired and promoted in academia such adherence can cause friction with the normative structures and practices of Open Science. Mertonian values and Open Science practices aim to improve the conduct and communication of research and are promoted by institutional actors. However Mertonian values remain mostly idealistic and contextualized in local and disciplinary cultures and Open Science practices rely heavily on third-party resources and technology that are not equally accessible to all parties. Furthermore although still popular Mertonian values were developed in a different institutional and political context. In this article we argue that new normative structures for science need to look beyond nostalgia and consider aspirations and outcomes of Open Science practices. To contribute to such a vision we explore the intersection of several Open Science practices with Mertonian values to flesh out challenges involved in upholding these values. We demonstrate that this intersection becomes complicated when the interests of numerous groups collide and contrast. Acknowledging and exploring such tensions informs our understanding of researchers' behavior and supports efforts that seek to improve researchers' interactions with other normative structures such as research ethics and integrity frameworks.","","","2022","10.1080/08989621.2022.2141625","","","medline-36303330.pdf","medline-36303330"
"The problem of irreproducible bioscience research","Flier, J.s.","Perspectives In Biology And Medicine","","Over recent decades, progress in bioscience research has been remarkable, but alongside the many transformative advances is a growing concern that a surprisingly high fraction of published research cannot be reproduced by the scientific community. Though experimental and interpretive errors are unavoidable features of the scientific process, recent evidence suggests that irreproducibility is a serious issue requiring analysis, understanding, and remediation. This article reviews the meaning of research reproducibility, examines ongoing efforts to estimate its prevalence, and considers the factors that contribute to it. Two recent case studies illustrate the disparate responses that researchers may take when facing serious claims that a high-profile research finding is irreproducible and may be false. Finally, the article examines potential interventions to counter the current level of irreproducibility, aimed at increasing the efficiency and impact of society’s substantial and critically important investment in bioscience research. © 2022 by johns hopkins university press.","","","2022","10.1353/pbm.2022.0032","","","scopus-2-s2.0-85137714947.pdf","scopus-2-s2.0-85137714947"
"Implementation of the Radiological Society of North America Expert Consensus Guidelines on Reporting Chest CT Findings Related to COVID-19: A Multireader Performance Study","Som A., Lang M., Yeung T., Carey D., Garrana S., Mendoza D. P., Flores E. J., Li M. D., Sharma A., McDermott S., Shepard J. O., Little B. P.","Radiology Cardiothoracic Imaging","","BACKGROUND: RSNA expert consensus guidelines provide a framework for reporting CT findings related to COVID-19 but have had limited multireader validation.\\\\\\\\rPURPOSE: To assess the performance of the RSNA guidelines and quantify interobserver variability in application of the guidelines in patients undergoing chest CT for suspected COVID-19 pneumonia.\\\\\\\\rMATERIALS AND METHODS: A retrospective search from 1/15/20 to 3/30/20 identified 89 consecutive CT scans whose radiological report mentioned COVID-19. One positive or two negative RT-PCR tests for COVID-19 were considered the gold standard for diagnosis. Each chest CT scan was evaluated using RSNA guidelines by 9 readers (6 fellowship trained thoracic radiologists and 3 radiology resident trainees). Clinical information was obtained from the electronic medical record.\\\\\\\\rRESULTS: There was strong concordance of findings between radiology training levels with agreement ranging from 60 to 86% among attendings and trainees (kappa 0.43 to 0.86). Sensitivity and specificity of ""typical"" CT findings for COVID-19 per the RSNA guidelines were on average 86% (range 72%-94%) and 80.2% (range 75-93%) respectively. Combined ""typical"" and ""indeterminate"" findings had a sensitivity of 97.5% (range 94-100%) and specificity of 54.7% (range 37-62%). A total of 163 disagreements were seen out of 801 observations (79.6% total agreement). Uncertainty in classification primarily derived from difficulty in ascertaining peripheral distribution multiple dominant disease processes or minimal disease.\\\\\\\\rCONCLUSION: The ""typical appearance"" category for COVID-19 CT reporting has an average sensitivity of 86% and specificity rate of 80%. There is reasonable interreader agreement and good reproducibility across various levels of experience. Copyright 2020 by the Radiological Society of North America Inc.","","","2020","10.1148/ryct.2020200276","","","medline-33778625.pdf","medline-33778625"
"Sharing confidential and sensitive data","Boruch, Robert F And Reiss, Albert Jr. And Garner, Joel And Larntz, Kinley And Freels, Sally","","","Deals with issues, options, and policy in sharing data that bear on criminal and civil justice research / main vehicle for illustration is the spouse assault replication program, a set of randomized field experiments . . . to understand how to reduce domestic violence topical coverage includes: (1) the spouse assault replication program and data sharing;  (2) privacy of research participants;  (3) data sharing, privacy, and the proprietary interests of researchers;  (4) data sharing and the interests of police departments and other agencies;  [and] (5) data sharing issues and their priorities in longitudinal surveys versus experiments (psycinfo database record (c) 2022 apa, all rights reserved)","","","1991","10.4135/9781483325620.n4","","","psychinfo-1991-97523-003.pdf","psychinfo-1991-97523-003"
"Reproducibility and reliability of hypoglycaemic episodes recorded with Continuous Glucose Monitoring System (CGMS) in daily life","Hoi-Hansen T., Pedersen-Bjergaard U., Thorsteinsson B.","Diabetic Medicine","","AIM: Continuous glucose monitoring may reveal episodes of unrecognized hypoglycaemia. We evaluated reproducibility and reliability of hypoglycaemic episodes recorded in daily life by the Medtronic MiniMed Continuous Glucose Monitoring System (CGMS).\\\\\\\\rMETHODS: Twenty-nine adult patients with Type 1 diabetes underwent 6 days of continuous subcutaneous glucose monitoring applying one CGMS on each side of the abdomen. Blood glucose was measured by HemoCue B-Glucose Analyzers six times daily and two different 4-point calibration sets were generated (set A and B). Using these calibration sets CGMS raw data were recalibrated generating four different CGMS data sets [left-A (left side of abdomen calibration set A) left-B right-A and right-B]. Agreement between CGMS data sets was evaluated during hypoglycaemic events comparing CGMS readings = 2.2 mmol/l with nadir values from corresponding CGMS data sets. CGMS readings were also compared with independent self-monitored blood glucose (SMBG) values.\\\\\\\\rRESULTS: With hypoglycaemia (CGMS readings = 2.2 mmol/l) in calibration set left-A values below 3.5 mmol/l were present in 99% (95% CI: 95-100%) of samples in left-B 91% (95% CI: 84-96%) of samples in right-A and 90% (95% CI: 83-95%) of samples in right B. In 84% of these episodes (95% CI: 59-96%) independent SMBG values were below 3.5 mmol/l. Difference in duration was observed with a median difference of 20 min; (left-A vs. right-B).\\\\\\\\rCONCLUSION: Hypoglycaemic episodes recorded by CGMS are reproducible and agreement with independent SMBG values is acceptable for retrospective recording of hypoglycaemic events with CGMS.","","","2005","10.1111/j.1464-5491.2005.01552.x","","","medline-15975099.pdf","medline-15975099"
"Dual-quorum: a highly available and consistent replication system for edge services","Gao, L. And Dahlin, M. And Zheng, J. And Alvisi, L. And Iyengar, A.","Ieee Transactions On Dependable And Secure Computing","","This paper introduces dual-quorum replication, a novel data replication algorithm designed to support internet edge services. Edge services allow clients to access internet services via distributed edge servers that operate on a shared collection of underlying data. Although it is generally difficult to share data while providing high availability, good performance, and strong consistency, replication algorithms designed for specific access patterns can offer nearly ideal trade-offs among these metrics. In this paper, we focus on the key problem of sharing read/write data objects across a collection of edge servers when the references to each object 1) tend not to exhibit high concurrency across multiple nodes and 2) tend to exhibit bursts of read-dominated or write-dominated behavior. Dual-quorum replication combines volume leases and quorum-based techniques to achieve excellent availability, response time, and consistency for such workloads. In particular, through both analytical and experimental evaluations, we show that the dual-quorum protocol can (for the workloads of interest) approach the optimal performance and availability of read-one/write-all-asynchronously (rowa-a) epidemic algorithms without suffering the weak consistency guarantees and resulting design complexity inherent in rowa-a systems. © 2006 ieee.","","","2010","10.1109/tdsc.2008.36","","","scopus-2-s2.0-77952741967.pdf","scopus-2-s2.0-77952741967"
"Reproducibility of a peripheral quantitative computed tomography scan protocol to measure the material properties of the second metatarsal","Chaplais E., Greene D., Hood A., Telfer S., du Toit V., Singh-Grewal D., Burns J., Rome K., Schiferl D. J., Hendry G. J.","BMC Musculoskeletal Disorders","","BACKGROUND: Peripheral quantitative computed tomography (pQCT) is an established technology that allows for the measurement of the material properties of bone. Alterations to bone architecture are associated with an increased risk of fracture. Further pQCT research is necessary to identify regions of interest that are prone to fracture risk in people with chronic diseases. The second metatarsal is a common site for the development of insufficiency fractures and as such the aim of this study was to assess the reproducibility of a novel scanning protocol of the second metatarsal using pQCT.\\\\\\\\rMETHODS: Eleven embalmed cadaveric leg specimens were scanned six times; three times with and without repositioning. Each foot was positioned on a custom-designed acrylic foot plate to permit unimpeded scans of the region of interest. Sixty-six scans were obtained at 15% (distal) and 50% (mid shaft) of the second metatarsal. Voxel size and scan speed were reduced to 0.40 mm and 25 mm.sec(-1). The reference line was positioned at the most distal portion of the 2(nd) metatarsal. Repeated measurements of six key variables related to bone properties were subject to reproducibility testing. Data were log transformed and reproducibility of scans were assessed using intraclass correlation coefficients (ICC) and coefficients of variation (CV%).\\\\\\\\rRESULTS: Reproducibility of the measurements without repositioning were estimated as: trabecular area (ICC 0.95; CV% 2.4) trabecular density (ICC 0.98; CV% 3.0) Strength Strain Index (SSI) - distal (ICC 0.99; CV% 5.6) cortical area (ICC 1.0; CV% 1.5) cortical density (ICC 0.99; CV% 0.1) SSI - mid shaft (ICC 1.0; CV% 2.4). Reproducibility of the measurements after repositioning were estimated as: trabecular area (ICC 0.96; CV% 2.4) trabecular density (ICC 0.98; CV% 2.8) SSI - distal (ICC 1.0; CV% 3.5) cortical area (ICC 0.99; CV%2.4) cortical density (ICC 0.98; CV% 0.8) SSI - mid shaft (ICC 0.99; CV% 3.2).\\\\\\\\rCONCLUSIONS: The scanning protocol generated excellent reproducibility for key bone properties measured at the distal and mid-shaft regions of the 2(nd) metatarsal. This protocol extends the capabilities of pQCT to evaluate bone quality in people who may be at an increased risk of metatarsal insufficiency fractures.","","","2014","10.1186/1471-2474-15-242","","","medline-25037451.pdf","medline-25037451"
"Testing the prisma-equity 2012 reporting guideline: the perspectives of systematic review authors","Burford B.j. And Welch V. And Waters E. And Tugwell P. And Moher D. And O'neill J. And Koehlmoos T. And Petticrew M.","Plos One","","Reporting guidelines can be used to encourage standardised and comprehensive reporting of health research. In light of the global commitment to health equity, we have previously developed and published a reporting guideline for equity-focused systematic reviews (prisma-e 2012). The objectives of this study were to explore the utility of the equity extension items included in prisma-e 2012 from a systematic review author perspective, including facilitators and barriers to its use. This will assist in designing dissemination and knowledge translation strategies. We conducted a survey of systematic review authors to expose them to the new items in prisma-e 2012, establish the extent to which they had historically addressed those items in their own reviews, and gather feedback on the usefulness of the new items. Data were analysed using microsoft excel 2008 and stata (version 11.2 for mac). Of 151 respondents completing the survey, 18.5% (95% ci: 12.7% to 25.7%) had not heard of the prisma statement before, although 83.4% (95% ci: 77.5% to 89.3%) indicated that they plan to use prisma-e 2012 in the future, depending on the focus of their review. Most (68.9%;  95% ci: 60.8% to 76.2%) thought that using prisma-e 2012 would lead them to conduct their reviews differently. Important facilitators to using prisma-e 2012 identified by respondents were journal endorsement and incorporation of the elements of the guideline into systematic review software. Barriers identified were lack of time, word limits and the availability of equity data in primary research. This study has been the first to 'road-test' the new prisma-e 2012 reporting guideline and the findings are encouraging. They confirm the acceptability and potential utility of the guideline to assist review authors in reporting on equity in their reviews. The uptake and impact of prisma-e 2012 over time on design, conduct and reporting of primary research and systematic reviews should continue to be examined. © 2013 burford et al.","","","2013","10.1371/journal.pone.0075122","","","embase-370004465.pdf","embase-370004465"
"Reproducibility of a test for the functional evaluation of dynamic balance and agility in elderly people","Fonseca, A.a. And García, C.l.a. And Collante, M.c.b. And Patiño, J.p. And Santisteban, R.n.r. And Carrascal, Y.t.a.","Iatreia","","Background: the 8 foot up & go test assesses the dynamic balance and agility in elderly people. Its reproducibility has been evaluated in american population, but it is unknown whether it would work similarly in a different population like the colombian. Objective: to evaluate the test-retest reliability and agreement level of the 8 foot up & go test in a sample of older adults from bucaramanga, colombia. Materials and methods: an evaluation of diagnostic tests was done in 114 elderly individuals. In the analysis, we assessed the test-retest reliability of the 8 foot up & go test by the intraclass correlation coefficient (icc 2.1) with their respective confidence intervals at 95% (95% ci). The agreement level was established by the bland-altman method. Results: the test-retest reliability of the 8 foot up & go test was very good (icc: 0.98;  95% ci: 0.98- 0.99). The agreement was good in females (mean difference [md] = 0.04 seconds and limits of agreement [la]: -1.27;  1.36 seconds), and in elderly institutionalized (md = 0.04 seconds [la]: -3.18;  3.27 seconds). Conclusion: the 8 foot up & go test has very good reliability and good agreement in colombian local elderly population.","","","2014","","","","scopus-2-s2.0-84903194024.pdf","scopus-2-s2.0-84903194024"
"Two database systems for the access to materials data and for their statistical analysis","Dathe, Gert","Sampe Journal","","The impact of the availability of materials data on economy and innovation and recommendations for the implementation of a materials data system are pointed out. Two operational database systems for standard values and test values, respectively are described. Examples are given for the selection of materials, for the output of properties of selected materials, and for the evaluation of test values. Those examples concern but are not limited to properties of iron and steel. Some problems of standard data and possibilities for their solution are given. Those problems are the structural complexity of standards, the different scales of ranges, influencing parameters, different test methods and units. Available information products and online services are mentioned.","","","1984","","","","scopus-2-s2.0-0021526969.pdf","scopus-2-s2.0-0021526969"
"Attitudes of research participants and the general public towards genomic data sharing: A systematic literature review","Shabani M., Bezuidenhout L., Borry P.","Expert Review of Molecular Diagnostics","","Aim: Introducing data sharing practices into the genomic research arena has challenged the current mechanisms established to protect rights of individuals and triggered policy considerations. To inform such policy deliberations soliciting public and research participants' attitudes with respect to genomic data sharing is a necessity. Method: The main electronic databases were searched in order to retrieve empirical studies investigating the attitudes of research participants and the public towards genomic data sharing through public databases. Results: In the 15 included studies participants' attitudes towards genomic data sharing revealed the influence of a constellation of interrelated factors including the personal perceptions of controllability and sensitivity of data potential risks and benefits of data sharing at individual and social level and also governance level considerations. Conclusion: This analysis indicates that future policy responses and recruitment practices should be attentive to a wide variety of concerns in order to promote both responsible and progressive research. Copyright © 2014 Informa UK Ltd.","","","2014","10.1586/14737159.2014.961917","","","medline-25260013.pdf","medline-25260013"
"Discovery and integrative neuroscience. [Review] [71 refs]","Koslow S. H.","","","Hypothesis driven research has been shown to be an excellent model for pursuing investigations in neuroscience. The Human Genome Project demonstrated the added value of discovery research especially in areas where large amounts of data are produced. Neuroscience has become a data rich field and one that would be enhanced by incorporating the discovery approach. Databases as well as analytical modeling and simulation tools will have to be developed and they will need to be interoperable and federated. This paper presents an overview of the development of the field of neuroscience databases and associate tools: Neuroinformatics. The primary focus is on the impact of NIH funding of this process. The important issues of data sharing as viewed from the perspective of the scientist and private and public funding organizations are discussed. Neuroinformatics will provide more than just a sophisticated array of information technologies to help scientists understand and integrate nervous system data. It will make available powerful models of neural functions and facilitate discovery hypothesis formulation and electronic collaboration. [References: 71]","","","2005","","","","unknown-1787.pdf","unknown-1787"
"EDGE(3): a web-based solution for management and analysis of Agilent two color microarray experiments","Vollrath A. L., Smith A. A., Craven M., Bradfield C. A.","BMC Bioinformatics","","BACKGROUND: The ability to generate transcriptional data on the scale of entire genomes has been a boon both in the improvement of biological understanding and in the amount of data generated. The latter the amount of data generated has implications when it comes to effective storage analysis and sharing of these data. A number of software tools have been developed to store analyze and share microarray data. However a majority of these tools do not offer all of these features nor do they specifically target the commonly used two color Agilent DNA microarray platform. Thus the motivating factor for the development of EDGE(3) was to incorporate the storage analysis and sharing of microarray data in a manner that would provide a means for research groups to collaborate on Agilent-based microarray experiments without a large investment in software-related expenditures or extensive training of end-users.\\\\\\\\rRESULTS: EDGE(3) has been developed with two major functions in mind. The first function is to provide a workflow process for the generation of microarray data by a research laboratory or a microarray facility. The second is to store analyze and share microarray data in a manner that doesn't require complicated software. To satisfy the first function EDGE3 has been developed as a means to establish a well defined experimental workflow and information system for microarray generation. To satisfy the second function the software application utilized as the user interface of EDGE(3) is a web browser. Within the web browser a user is able to access the entire functionality including but not limited to the ability to perform a number of bioinformatics based analyses collaborate between research groups through a user-based security model and access to the raw data files and quality control files generated by the software used to extract the signals from an array image.\\\\\\\\rCONCLUSION: Here we present EDGE(3) an open-source web-based application that allows for the storage analysis and controlled sharing of transcription-based microarray data generated on the Agilent DNA platform. In addition EDGE(3) provides a means for managing RNA samples and arrays during the hybridization process. EDGE(3) is freely available for download at http://edge.oncology.wisc.edu/.","","","2009","10.1186/1471-2105-10-280","","","medline-19732451.pdf","medline-19732451"
"Oddpub – a text-mining algorithm to detect data sharing in biomedical publications","Riedel, N. And Kip, M. And Bobrov, E.","Data Science Journal","","Open research data are increasingly recognized as a quality indicator and an important resource to increase transparency, robustness and collaboration in science. However, no standardized way of reporting open data in publications exists, making it difficult to find shared datasets and assess the prevalence of open data in an automated fashion. We developed oddpub (open data detection in publications), a text-mining algorithm that screens biomedical publications and detects cases of open data. Using english-language original research publications from a single biomedical research institution (n = 8689) and randomly selected from pubmed (n = 1500) we iteratively developed a set of derived keyword categories. Oddpub can detect data sharing through field-specific repositories, general-purpose repositories or the supplement. Additionally, it can detect shared analysis code (open code). To validate oddpub, we manually screened 792 publications randomly selected from pubmed. On this validation dataset, our algorithm detected open data publications with a sensitivity of 0.73 and specificity of 0.97. Open data was detected for 11.5% (n = 91) of publications. Open code was detected for 1.4% (n = 11) of publications with a sensitivity of 0.73 and specificity of 1.00. We compared our results to the linked datasets found in the databases pubmed and web of science. Our algorithm can automatically screen large numbers of publications for open data. It can thus be used to assess open data sharing rates on the level of subject areas, journals, or institutions. It can also identify individual open data publications in a larger publication corpus. Oddpub is published as an r package on github. © 2019 the author(s).","","","2020","10.5334/dsj-2020-042","","","scopus-2-s2.0-85098960465.pdf","scopus-2-s2.0-85098960465"
"Data Governance for Real-World Data Management: A Proposal for a Checklist to Support Decision Making","Sola-Morales O., Sigurardottir K., Akehurst R., Murphy L. A., Mestre-Ferrandiz J., Cunningham D., de Pouvourville G.","Value in Health","","OBJECTIVES: Real-world data (RWD) and real-world evidence (RWE) can provide extensive information on healthcare for use in health technology assessment and decision making. Nevertheless there is a lack of consensus surrounding the appropriate data governance (DG) practices for RWD/RWE. Data sharing is also a large concern especially considering evolving data protection regulations. Our objective is to propose recommendations for international standards of evaluating the acceptability of RWD governance practices.\\\\\\\\rMETHODS: After reviewing the literature we created a checklist targeting DG practices for RWD/RWE. We then carried out a 3-round Delphi panel including European policy makers health technology assessment experts and hospital managers. The consensus for each statement was measured and the checklist adjusted accordingly.\\\\\\\\rRESULTS: The literature review identified the main topics regarding RWD/RWE DG practices: data privacy and security data management and linkage data access management and the generation and use of RWE. Members of the Delphi panel (21 experts/25 invited) were presented a total of 24 statements related to each of the topics. Experts demonstrated a progressive level of consensus and importance ratings in all topics and to most statements. We suggest a refined checklist in which the statements rated less important or with less consensus have been removed.\\\\\\\\rCONCLUSIONS: This study suggests how the DG of RWD/RWE could be qualitatively evaluated. We propose checklists that could be used by all RWD/RWE users to help ensure the quality and integrity of RWD/RWE governance and complement data protection law. Copyright © 2023 International Society for Pharmacoeconomics and Outcomes Research Inc. Published by Elsevier Inc. All rights reserved.","","","2023","10.1016/j.jval.2023.02.012","","","medline-36870678.pdf","medline-36870678"
"Optimization of Revision Hip Arthroplasty Workflow by Means of Detailed Pre-Surgical Planning Using Computed Tomography Data Open-Source Software and Three-Dimensional-Printed Models","Andrzejewski K., Domzalski M., Komorowski P., Poszepczynski J., Rokita B., Elgalal M.","Diagnostics","","BACKGROUND: In revision hip arthroplasty (RHA) establishing the center of rotation (COR) can be technically challenging due to the acetabular bone destruction that is usually present particularly in severe cases such as Paprosky type II and III defects. The aim of this study was to demonstrate the use of open-source medical image reconstruction software and low-cost 3D anatomical models in pre-surgical planning of RHA.\\\\\\\\rMETHODS: A total of 10 patients underwent RHA and were included in the study. Computed tomography (CT) scans were performed for all cases before surgery and approximately 1 week after the procedure. The reconstruction of CT data 3D virtual planning of the COR and positioning of acetabular cups including their inclination and anteversion angles was carried out using the free open source software platform 3D Slicer. In addition anatomical models of the pelvis were built on a desktop 3D printer from polylactic acid (PLA). Preoperative and postoperative reconstructed imaging data were compared for each patient and the position of the acetabular cups as well as the COR were evaluated for each case.\\\\\\\\rRESULTS: Analysis of the pre- and post-op center of rotation position data indicated statistically insignificant differences for the location of the COR on the X-axis (1.5 mm t = 0.5741 p = 0.5868) with a fairly strong correlation of the results (r = -0.672 p = 0.0982) whilst for the location of the COR in the Y and Z-axes there was statistical dependence (Y axis 4.7 mm t = 3.168 and p = 0.0194; Z axis 1.9 mm t = 1.887 and p = 0.1081). A strong correlation for both axes was also observed (Y and Z) (Y-axis r = 0.9438 and p = 0.0014; Z-axis r = 0.8829 and p = 0.0084). Analysis of inclination angle values showed a statistically insignificant difference between mean values (3.9 degrees t = 1.111 p = 0.3092) and a moderate correlation was found between mean values (r = -0.4042 p = 0.3685). Analysis of the anteversion angle showed a statistically insignificant difference between mean values (1.9 degrees t = 0.8671 p = 0.4192) while a moderate correlation between mean values was found (r = -0.4782 p = 0.2777).\\\\\\\\rCONCLUSIONS: Three-dimensional reconstruction software together with low-cost anatomical models are very effective tools for pre-surgical planning which have great potential use in orthopedic surgery particularly RHA. In up and in- and up and out-type defects it is essential to establish a new COR and to identify three support points within the revision acetabulum in order to correctly position acetabular cups.","","","2023","10.3390/diagnostics13152516","","","medline-37568878.pdf","medline-37568878"
"Sharing data improves monitoring of trans-boundary populations: the case of wolverines in central scandinavia","Gervasi, V. And Brøseth, H. And Gimenez, O. And Nilsen, E.b. And Odden, J. And Flagstad, . And Linnell, J.d.c.","Wildlife Biology","","Populations of wide-ranging species are likely to extend across multiple jurisdictions, including national and international borders. This requires that local institutions implement data sharing and a standardization of monitoring designs. However, a formal evaluation of the benefits of integrated monitoring systems had not, of yet, been performed. Using the wolverines in central scandinavia as a study case, we assessed the benefits of data sharing for the monitoring of trans-boundary populations. We also assessed the performance of two demographic monitoring systems, one relying on a count of reproductive units, the other resulting from non-invasive genetic sampling and capture-recapture modeling. Sharing data across the border between norway and sweden allowed a strong increase in the precision of population size, population growth rate and vital rates estimates. It also allowed revealing that the probability to emigrate from sweden to norway was significantly higher than in the opposite direction, a required condition for the existence of a source--sink dynamics. These findings would have been impossible without trans-boundary data sharing. While the den count monitoring provided an estimated population growth of 138% over the 12-year period, the dna-based estimate was only 72%. A positive trend likely occurred in the detectability of wolverine dens during the first years of the study, and the index was not able to separate the actual demographic trend from the trend in the system's ability to detect reproductions, thus providing positively biased estimates of population growth rate during the initial phase of the study. Data sharing is a crucial need for the study of the processes occurring in trans-boundary populations. It should be enhanced wherever trans-boundary ecological processes occur. Also, managers should be aware that count-based monitoring has a risk of overestimating population growth during the first years after its implementation. © 2016 norwegian institute for nature research.","","","2016","10.2981/wlb.00142","","","scopus-2-s2.0-84969168238.pdf","scopus-2-s2.0-84969168238"
"A multi-dimensional evaluation of synthetic data generators","Dankar, F.k. And Ibrahim, M.k. And Ismail, L.","Ieee Access","","Synthetic datasets are gradually emerging as solutions for data sharing. Multiple synthetic data generators have been introduced in the last decade fueled by advancement in machine learning and by the increased demand for fast and inclusive data sharing, yet their utility is not well understood. Prior research tried to compare the utility of synthetic data generators using different evaluation metrics. These metrics have been found to generate conflicting conclusions making direct comparison of synthetic data generators very difficult. This paper identifies four criteria (or dimensions) for masked data evaluation by classifying available utility metrics into different categories based on the measure they attempt to preserve: attribute fidelity, bivariate fidelity, population fidelity, and application fidelity. A representative metric from each category is chosen based on popularity and consistency, and the four metrics are used to compare the overall utility of four recent data synthesizers across 19 datasets of different sizes and feature counts. The paper also examines correlations between the selected metrics in an attempt to streamline synthetic data utility. © 2013 ieee.","","","2022","10.1109/access.2022.3144765","","","scopus-2-s2.0-85123384612.pdf","scopus-2-s2.0-85123384612"
"Embracing research in nursing practice","Jones, S.","British Journal Of Nursing","","The establishment of an integrated research culture in medicine has contributed to the ongoing success of medical science in treating disease. Research examining the effectiveness of services and care is of increasing priority, and is a branch of research open to all health professionals. This article provides an overview of the research process aimed at health professionals with novice research skills. It will provide tools and resources to guide health professionals in the development of research concepts leading to a feasible protocol.","","","2014","10.12968/bjon.2014.23.18.994","","","scopus-2-s2.0-84910035792.pdf","scopus-2-s2.0-84910035792"
"A fair and rational data sharing strategy toward two-stage industrial internet of things","Zheng, X. And Tian, L. And Cai, Z.","Ieee Transactions On Industrial Informatics","","The easy and pervasive involvement of devices in industrial internet of things has greatly benefited the implementation and adoption of various smart services. One prominent prerequisite of such trends is the extensive and continuous support and sharing of data and resources among devices. However, previous efforts usually treat the data sharing as one-time task among devices, which are incapable when the data are applied for the distributed and iterative training task of machine learning models. Therefore, this article proposes a novel framework for continuous data sharing in industrial internet of things. The system consists of different system owners, each brings devices and participate the distributed training of models. Specifically, system owners hold different scales of devices, data, and resources, while devices own heterogeneous availability in different time periods. In this case, the goal is to properly assign devices for qualified model training process in different rounds, such that no devices will devote unlimited resources and the overall efforts and consumptions among different owners are balanced. Accordingly, three algorithms for device allocation are proposed, based on whether the availability of devices in each training round are known at the beginning of the training procedure. The analysis shows that all algorithms can achieve a rational allocation for devices and balance the performance among system owners. Finally, evaluation results reveal that the proposed solutions outperform baseline methods in providing better data sharing plans. © 2005-2012 ieee.","","","2023","10.1109/tii.2022.3179361","","","scopus-2-s2.0-85131724144.pdf","scopus-2-s2.0-85131724144"
"Replication crisis p-hacking and open science. An inquiry into questionable research practices in student projects and impulses for the teaching environment","Brachem Johannes, Frank Maximilian, Kvetnaya Tatiana, Schramm Leonhard F., Volz Leonhard","Psychologische Rundschau","","(German) In den letzten Jahren gab es innerhalb der Psychologie eine intensive Auseinandersetzung mit den Auswirkungen der Replikationskrise sowie dem hieraus entstandenen Diskurs uber die Weiterentwicklung der Disziplin. Als ein Grund fur die mangelnde Replizierbarkeit psychologischer Forschung wurde die Verwendung fragwurdiger Forschungspraktiken (eng. QRPs) identifiziert. Wahrend es umfangreiche Untersuchungen zur Pravalenz von QRPs unter Wissenschaftler*innen gibt ist bisher wenig uber die Verbreitung dieser Praktiken unter Studierenden bekannt. Mit der hier vorgestellten Arbeit wurde erstmals eine grosere Befragung unter 1397 Psychologie- Studierenden im deutschsprachigen Raum durchgefuhrt um die Verbreitung von QRPs in studentischen Projekten sowie den aktuellen Stand der akademischen Lehre in Bezug auf die Replikationskrise und Open Science zu erheben. Die gemeinsame Betrachtung der Lehre und des Einsatzes fragwurdiger Forschungspraktiken versprechen Aufschluss daruber wie die psychologische Lehre mit dem empirischen Vorgehen der Studierenden zusammenhangt. Die Ergebnisse zeigen dass QRPs auch in studentischen Projekten vorkommen wobei grose Unterschiede in der Verbreitung einzelner QRPs bestehen. Auch zwischen den verschiedenen Projekttypen zeigten sich Unterschiede so war die Anwendung von QRPs in Experimentalpraktika am starksten und in Masterarbeiten am schwachsten ausgepragt. Unsere Daten weisen insgesamt darauf hin dass die selbstberichtete Verbreitung von QRPs uber den Studienverlauf abnimmt. Zudem scheint ein Grosteil der Studierenden bereits mit der Thematik der Replikationskrise in der Lehre in Beruhrung gekommen zu sein. Deren Behandlung findet grostenteils in der Methodenlehre und weniger in inhaltlich spezialisierten Lehrveranstaltungen statt. Wir geben abschliesend Impulse zur Weiterentwicklung der psychologischen Lehre in denen die Prinzipien der Offenheit Transparenz und Kollaboration beim Hervorbringen inhaltlich robuster Forschung bereits wahrend des Studiums im Vordergrund stehen. (PsycInfo Database Record (c) 2022 APA all rights reserved)","","","2022","10.1026/0033-3042/a000562","","","wos-000739573100001.pdf","wos-000739573100001"
"A critical evaluation of the 2011 ECHA reports on compliance with the REACH and CLP regulations and on the use of alternatives to testing on animals for compliance with the REACH regulation","Spielmann H., Sauer U. G., Mekenyan O.","ATLA-Alternatives to Laboratory Animals","","On 30 June 2011 the European Chemicals Agency published two reports one on the functioning of the REACH system the other on the use of alternatives to animal testing in compliance with that system. The data presented are based on information gained during the first registration period under the REACH system which included high production volume chemicals and substances of very high concern which have the most extensive information requirements. A total of 25460 registration dossiers were received covering 3400 existing so-called 'phase-in' substances and 900 new so-called 'non-phase-in' substances. Data sharing and the joint submission of data are reported to have worked successfully. In the registration dossiers for these substances results from new animal tests were included for less than 1% of all the endpoints; testing proposals (required for 'higher-tier' information requirements) were submitted for 711 in vivo tests involving vertebrate animals. The registrants mainly used old existing experimental data or options for the adaptation (waiving) of information requirements before collecting new information. For predicting substance toxicity 'read-across' was the second most-used approach followed by 'weight-of-evidence'. In vitro toxicity tests played a minor role and were only used when the respective test methods had gained the status of regulatory acceptance. All in all a successful start to the REACH programme was reported particularly since in contrast to most predictions it did not contribute to a significant increase in toxicity testing in animals. Copyright 2011 FRAME.","","","2011","10.1177/026119291103900509","","","medline-22103941.pdf","medline-22103941"
"Reducing sources of variance in experimental procedures in in vitro research","Fischer I. And Martinez-Dominguez M.v. And Hanggi D. And Kahlert U.","F1000 Res","","Background: lack of reproducibility in preclinical research poses ethical and economic challenges for biomedical science. Various institutional activities by society stakeholders of leading industrialised nations are currently underway with the aim of improving the situation. Such initiatives are usually concerned with high-level organisational issues and typically do not focus on improving experimental approaches per se. Addressing these is necessary in order to increase consistency and success rates of lab-to-lab repetitions. Method(s): in this project, we statistically evaluated repetitive data of a very basic and widely applied lab procedure, namely quantifying the number of viable cells. The purpose of this was to assess the impact of different parameters and instrumentations which may constitute sources of variance in this procedure. Conclusion(s): by comparing the variability of data acquired under two different procedures, featuring improved stringency of protocol adherence, our project attempts to identify the sources and propose guidelines on how to reduce such fluctuations. We believe our work can contribute to tackling the repeatability crisis in biomedical research.copyright © 2022 fischer i et al.","","","2022","10.12688/f1000research.73497.2","","","embase-636890039.pdf","embase-636890039"
"Similar Outcomes of Web-Based and Face-to-Face Training of the GRADE Approach for the Certainty of Evidence: Randomized Controlled Trial","Tokalic R., Poklepovic Pericic T., Marusic A.","Journal of Medical Internet Research","","BACKGROUND: The GRADE (Grading of Recommendations Assessment Development and Evaluation) approach is a system for transparent evaluation of the certainty of evidence used in clinical practice guidelines and systematic reviews. GRADE is a key part of evidence-based medicine (EBM) training of health care professionals.\\\\\\\\rOBJECTIVE: This study aimed to compare web-based and face-to-face methods of teaching the GRADE approach for evidence assessment.\\\\\\\\rMETHODS: A randomized controlled trial was conducted on 2 delivery modes of GRADE education integrated into a course on research methodology and EBM with third-year medical students. Education was based on the Cochrane Interactive Learning ""Interpreting the findings"" module which had a duration of 90 minutes. The web-based group received the web-based asynchronous training whereas the face-to-face group had an in-person seminar with a lecturer. The main outcome measure was the score on a 5-question test that assessed confidence interval interpretation and overall certainty of evidence among others. Secondary outcomes included writing a recommendation for practice and course satisfaction.\\\\\\\\rRESULTS: In all 50 participants received the web-based intervention and 47 participants received the face-to-face intervention. The groups did not differ in the overall scores for the Cochrane Interactive Learning test with a median of 2 (95% CI 1.0-2.0) correct answers for the web-based group and 2 (95% CI 1.3-3.0) correct answers for the face-to-face group. Both groups gave the most correct answers to the question about rating a body of evidence (35/50 70% and 24/47 51% for the web-based and face-to-face group respectively). The face-to-face group better answered the question about the overall certainty of evidence question. The understanding of the Summary of Findings table did not differ significantly between the groups with a median of 3 correct answers to 4 questions for both groups (P=.352). The writing style for the recommendations for practice also did not differ between the 2 groups. Students' recommendations mostly reflected the strengths of the recommendations and focused on the target population but they used passive words and rarely mentioned the setting for the recommendation. The language of the recommendations was mostly patient centered. Course satisfaction was high in both groups.\\\\\\\\rCONCLUSIONS: Training in the GRADE approach could be equally effective when delivered asynchronously on the web or face-to-face.\\\\\\\\rTRIAL REGISTRATION: Open Science Framework akpq7; https://osf.io/akpq7/. Copyright ©Ruzica Tokalic Tina Poklepovic Pericic Ana Marusic. Originally published in the Journal of Medical Internet Research (https://www.jmir.org) 06.06.2023.","","","2023","10.2196/43928","","","medline-37279050.pdf","medline-37279050"
"Penetrating neck trauma: lack of universal reporting guidelines","Atta H. M., Walker M. L.","American Surgeon","","Penetrating neck injuries constitute a heterogeneous group. Two different classifications of zones of the neck exist in trauma literature. Injuries crossing the midline are not accurately reported. Records of 50 patients with stab wounds (30) gunshot wounds (GSWs; 17) and shotgun wounds (SGWs; 3) were reviewed. Injuries involved zone I in 8 patients zone II in 37 patients zone III in 8 patients posterior triangle in 6 patients and multiple zones in 5 patients. All 11 patients with transcervical GSWs and SGWs sustained vascular or aerodigestive injuries and had longer hospital stays (14.0 +/- 2.6 days) compared with patients with other GSWs (6.6 +/- 2.0 days) and stab wounds (3.6 +/- 0.5 days). We emphasize the lethal potential of transcervical GSWs and SGWs. We suggest that these particular injuries be reported separately. We recommend the universal adoption of one system of classification of neck zones.","","","1998","","","","medline-9520810.pdf","medline-9520810"
"Medical guidelines physician density and quality of care: evidence from German SHARE data","Jurges H., Pohl V.","European Journal of Health Economics","","We use German SHARE data to study the relationship between district general practitioner density and the quality of preventive care provided to older adults. We measure physician quality of care as the degree of adherence to medical guidelines (for the management of risk factors for cardiovascular disease and the prevention of falls) as reported by patients. Contrary to theoretical expectations we find only weak and insignificant effects of physician density on quality of care. Our results shed doubt on the notion that increasing physician supply will increase the quality of care provided in Germany's present health care system.","","","2012","10.1007/s10198-011-0372-5","","","medline-22203268.pdf","medline-22203268"
"Systematic reviews are rarely used to contextualise new results-a systematic review and meta-analysis of meta-research studies","Draborg E., Andreasen J., Norgaard B., Juhl C. B., Yost J., Brunnhuber K., Robinson K. A., Lund H.","Systematic Reviews","","BACKGROUND: Results of new studies should be interpreted in the context of what is already known to compare results and build the state of the science. This systematic review and meta-analysis aimed to identify and synthesise results from meta-research studies examining if original studies within health use systematic reviews to place their results in the context of earlier similar studies.\\\\\\\\rMETHODS: We searched MEDLINE (OVID) EMBASE (OVID) and the Cochrane Methodology Register for meta-research studies reporting the use of systematic reviews to place results of original clinical studies in the context of existing studies. The primary outcome was the percentage of original studies included in the meta-research studies using systematic reviews or meta-analyses placing new results in the context of existing studies. Two reviewers independently performed screening and data extraction. Data were synthesised using narrative synthesis and a random-effects meta-analysis was performed to estimate the mean proportion of original studies placing their results in the context of earlier studies. The protocol was registered in Open Science Framework.\\\\\\\\rRESULTS: We included 15 meta-research studies representing 1724 original studies. The mean percentage of original studies within these meta-research studies placing their results in the context of existing studies was 30.7% (95% CI [23.8% 37.6%] I2=87.4%). Only one of the meta-research studies integrated results in a meta-analysis while four integrated their results within a systematic review; the remaining cited or referred to a systematic review. The results of this systematic review are characterised by a high degree of heterogeneity and should be interpreted cautiously.\\\\\\\\rCONCLUSION: Our systematic review demonstrates a low rate of and great variability in using systematic reviews to place new results in the context of existing studies. On average one third of the original studies contextualised their results. Improvement is still needed in researchers' use of prior research systematically and transparently-also known as the use of an evidence-based research approach to contribute to the accumulation of new evidence on which future studies should be based.\\\\\\\\rSYSTEMATIC REVIEW REGISTRATION: Open Science registration number https://osf.io/8gkzu/. Copyright © 2022. The Author(s).","","","2022","10.1186/s13643-022-02062-8","","","medline-36064741.pdf","medline-36064741"
"Challenges of research assessment oriented towards knowledge mobilization in the transition to open science: an analysis based on the case of the Working Groups from the Latin American Council of Social Sciences","Vommaro P., Rovelli L.","Analecta Politica","","Various recent debates and initiatives have once again brought to the forefront the need to reform research evaluation and its greater link with society based on a growing openness collaboration and participation in the field of knowledge following some of the principles of transition to open science. The foregoing entails multiple challenges for the design of research promotion instruments and their evaluation. Taking as a sample case an experience in the Research Area of the Latin American Council of Social Sciences (Clacso) this article aims to describe and analyze the challenges of evaluating research aimed at mobilizing knowledge based on the configuration of Clacso's Groups of Work with a focus on the criteria of the 2019-2022 call. In particular the specific characteristics of the evaluation criteria involved in this program and its scope in relation to inclusion openness collaboration and interaction with society are explored. The approach is qualitative in nature and benefits from the collection of mainly documentary data related to the bases of the call for the 2019-2022 Working Groups the evaluation grids used and the organizational actions promoted around some of the principles of open access and open science.","","","2022","10.18566/apolit.v12n22.a02","","","wos-001017290500002.pdf","wos-001017290500002"
"Guidelines for reporting the results of experiments on fish","Brattelid T., Smith A. J.","Laboratory Animals","","A detailed account of experimental design including an accurate description of the animals used is an essential part of good research practice. Without these details the reader will be unable not only to form an opinion on the significance of the findings but also to repeat the experiment in another laboratory. This paper presents suggested guidelines for reporting experimental studies using fish.","","","2000","10.1258/002367700780457590","","","medline-10817451.pdf","medline-10817451"
"An adaptive secure and practical data sharing system with verifiable outsourced decryption","Xu, S. And Han, X. And Xu, G. And Ning, J. And Huang, X. And Deng, R.h.","Ieee Transactions On Services Computing","","Cloud computing is the widespread acceptance of a promising paradigm offering a substantial amount of storage and data services on demand. To preserve data confidentiality, many cryptosystems have been introduced. However, current solutions are incompatible with the resource-constrained end-devices because of a variety of vulnerabilities in terms of practicality and security. In this paper, we propose a practical and secure data-sharing system by introducing a new design of attribute-based encryption with verifiable outsourced decryption (vo-abe for short). Our system offers: (1) data sharing at a fine-grained level;  (2) a scalable key issuing protocol without any secure channel;  (3) a verifiable outsourced decryption mechanism for resource-constrained end-devices against the malicious cloud service provider;  and (4) adaptive security against the real-world attacks. To formalize our solution with cryptographic analysis, we present the formal definition of vo-abe and its concrete construction with provable security. In particular, our design leverages the techniques of the traditional abe, verifiable outsourced decryption, and randomness extractor to support fine-grained access control, cost-effective data sharing, and security assurance with high entropy. Moreover, our design is provably secure in the adaptive model under the standard assumption, which offers a stronger security guarantee since the state-of-the-art solution is selectively secure under the non-standard assumption and suffers from a variety of real-world attacks. The implementation and evaluation demonstrate that our solution enjoys superior functionality and better performance than the relevant solutions. More importantly, our solution is compatible with the resource-constrained end-devices since the decryption mechanism takes around 1.1ms and is 22.7x faster than the state-of-the-art solution. Ieee","","","2023","10.1109/tsc.2023.3321314","","","scopus-2-s2.0-85174804290.pdf","scopus-2-s2.0-85174804290"
"Longitudinal reproducibility of optical coherence tomography measurements in children","Prakalapakorn S. G., Freedman S. F., Lokhnygina Y., Gandhi N. G., Holgado S., Chen B. B., El-Dairi M. A.","Journal of Aapos: American Association for Pediatric Ophthalmology & Strabismus","","PURPOSE: To evaluate the longitudinal reproducibility of optical coherence tomography (OCT) measurements in normal and glaucomatous eyes of children.\\\\\\\\rMETHODS: In this 2-setting prospective study OCT-3 was used to obtain fast retinal nerve fiber layer (RNFL) and macular thickness scans. In the first study setting the normal eyes of healthy children were scanned on presentation at 2 weeks and 3 years with axial length measured at the first and last examinations. In the second setting OCT scans of patients in the pediatric glaucoma clinic were performed over 4 years as clinically indicated. Eyes were classified as ""normal"" (normal eyes and those with physiologic cupping but normal intraocular pressure [IOP]); ""mild glaucoma"" (increased IOP and a normal optic nerve appearance); or ""advanced glaucoma"" (severe cupping or progressive glaucoma). Intraclass correlation coefficients were used to evaluate the reproducibility of measurements on the same day and over time.\\\\\\\\rRESULTS: In the first setting 8 normal eyes were included. Axial length increased 0.11 +/- 0.04 mm/year over an average of 3.3 years (P = 0.03); there was no statistically significant change in RNFL thickness (P = 0.30). In our second setting 27 normal eyes and 37 eyes with glaucoma were included. Intraclass correlation coefficients across the 3 visits for total macular volume were 0.80-0.91 and for average RNFL were 0.73-0.95.\\\\\\\\rCONCLUSIONS: Global OCT measurements in children were reproducible over years and were not affected by normal increase in axial length. OCT shows promise as an objective tool for longitudinal assessment of children. Copyright © 2012 American Association for Pediatric Ophthalmology and Strabismus. Published by Mosby Inc. All rights reserved.","","","2012","10.1016/j.jaapos.2012.08.011","","","medline-23237748.pdf","medline-23237748"
"Announcing open science badges and reaching for the sky","Grahe Jon E.","The Journal of Social Psychology","","The Journal of Social Psychology is the longest running journal publishing research focused on this sub-discipline. JSP's mission has always been to provide empirically based theoretical contributions to the discipline of the highest quality. Though our tradition is rich we have not always met these idealistic standards. Beginning in 2000 with the significant changes to the executive editorial and consulting editorial review boards we have worked toward the goal of increasing the publication standards of the Journal and thus the quality and impact of research published in JSP. We are ready to take another leap forward. As you hold this issue in your hands you will notice that it has a new size and layout. The timing of this external change also marks an announcement aimed at taking another leap forward in our goal toward again being a leading social psychological journal. (PsycInfo Database Record (c) 2021 APA all rights reserved)","","","2014","10.1080/00224545.2014.853582","","","medline-24689331.pdf","medline-24689331"
"Reforms to improve reproducibility and quality must be coordinated across the research ecosystem: the view from the UKRN Local Network Leads","Stewart S. L. K., Pennington C. R., da Silva G. R., Ballou N., Butler J., Dienes Z., Jay C., Rossit S., Samara A.","BMC Research Notes","","Many disciplines are facing a ""reproducibility crisis"" which has precipitated much discussion about how to improve research integrity reproducibility and transparency. A unified effort across all sectors levels and stages of the research ecosystem is needed to coordinate goals and reforms that focus on open and transparent research practices. Promoting a more positive incentive culture for all ecosystem members is also paramount. In this commentary we-the Local Network Leads of the UK Reproducibility Network-outline our response to the UK House of Commons Science and Technology Committee's inquiry on research integrity and reproducibility. We argue that coordinated change is needed to create (1) a positive research culture (2) a unified stance on improving research quality (3) common foundations for open and transparent research practice and (4) the routinisation of this practice. For each of these areas we outline the roles that individuals institutions funders publishers and Government can play in shaping the research ecosystem. Working together these constituent members must also partner with sectoral and coordinating organisations to produce effective and long-lasting reforms that are fit-for-purpose and future-proof. These efforts will strengthen research quality and create research capable of generating far-reaching applications with a sustained impact on society. Copyright © 2022. The Author(s).","","","2022","10.1186/s13104-022-05949-w","","","medline-35168675.pdf","medline-35168675"
"Timely access to trial data in the context of a pandemic: the time is now","Li R. And Wood J. And Baskaran A. And Neumann S. And Graham E. And Levenstein M. And Sim I.","Bmj Open","","Objective clinical trial data sharing has the potential to accelerate scientific progress, answer new lines of scientific inquiry, support reproducibility and prevent redundancy. Vivli, a non-profit organisation, operates a global platform for sharing of individual participant-level trial data and associated documents. Sharing of these data collected from each trial participant enables combining of these data to drive new scientific insights or assess reproducibility-not possible with the aggregate or summary data tables historically made available. We report on our initial experience including key metrics, lessons learned and how we see our role in the data sharing ecosystem. We also describe how vivli is addressing the needs of the covid-19 challenge through a new dedicated portal that provides a direct search function for covid-19 studies, availability for fast-tracked request review and data sharing. Data summary the vivli platform was established in 2018 and has partnered with 28 diverse members from industry, academic institutions, government platforms and non-profit foundations. Currently, 5400 trials representing 3.6 million participants are shared on the platform. From july 2018 to september 2020, vivli received 201 requests. To date, 106 of 201 requests received approval, 5 have been declined, 27 withdrew and 27 are in the revision stage. Conclusions the pandemic has only magnified the necessity for data sharing. If most data are shared and in a manner that allows interoperability, then we have hope of moving towards a cohesive scientific understanding more quickly not only for covid-19 but also for all diseases. Conversely, if only isolated pockets of data are shared then society loses the opportunity to close vital gaps in our understanding of this rapidly evolving epidemic. This current challenge serves to highlight the value of data sharing platforms-critical enablers that help researchers build on prior knowledge.copyright © 2020 author(s) (or their employer(s)). Re-use permitted under cc by-nc. No commercial re-use. See rights and permissions. Published by bmj.","","","2020","10.1136/bmjopen-2020-039326","","","embase-633256439.pdf","embase-633256439"
"Survey study of research integrity officers' perceptions of research practices associated with instances of research misconduct","Kalichman M.","Research Integrity & Peer Review","","BACKGROUND: Research on research integrity has tended to focus on frequency of research misconduct and factors that might induce someone to commit research misconduct. A definitive answer to the first question has been elusive but it remains clear that any research misconduct is too much. Answers to the second question are so diverse it might be productive to ask a different question: What about how research is done allows research misconduct to occur?\\\\\\\\rMETHODS: With that question in mind research integrity officers (RIOs) of the 62 members of the American Association of Universities were invited to complete a brief survey about their most recent instance of a finding of research misconduct. Respondents were asked whether one or more good practices of research (e.g. openness and transparency keeping good research records) were present in their case of research misconduct.\\\\\\\\rRESULTS: Twenty-four (24) of the respondents (39% response rate) indicated they had dealt with at least one finding of research misconduct and answered the survey questions. Over half of these RIOs reported that their case of research misconduct had occurred in an environment in which at least nine of the ten listed good practices of research were deficient.\\\\\\\\rCONCLUSIONS: These results are not evidence for a causal effect of poor practices but it is arguable that committing research misconduct would be more difficult if not impossible in research environments adhering to good practices of research.","","","2020","10.1186/s41073-020-00103-1","","","medline-33303039.pdf","medline-33303039"
"Validation of a new assessment tool for qualitative research articles","Schou, L. And Høstrup, H. And Lyngsø, E.e. And Larsen, S. And Poulsen, I.","Journal Of Advanced Nursing","","Aim. This paper presents the development and validation of a new assessment tool for qualitative research articles, which could assess trustworthiness of qualitative research articles as defined by guba and at the same time aid clinicians in their assessment. Background. There are more than 100 sets of proposals for quality criteria for qualitative research. However, we are not aware of an assessment tool that is validated and applicable, not only for researchers but also for clinicians with different levels of training and experience in reading research articles. Method. In three phases from 2007 to 2009 we delevoped and tested such an assessment tool called vaks, which is the danish acronym for appraisal of qualitative studies. Phase 1 was to develop the tool based on a literature review and on consultation with qualitative researchers. Phase 2 was an inter-rater reliability test in which 40 health professionals participated. Phase 3 was an inter-rater reliability test among the five authors by means of five qualitative articles. Results. The new assessment tool was based on guba's four criteria for assessing the trustworthiness of qualitative inquiries. The nurses found the assessment tool simple to use and helpful in assessing the quality of the articles. The inter-rater agreement was acceptable, but disagreement was seen for some items. Conclusion. We have developed an assessment tool for appraisal of qualitative research studies. Nurses with a range of formal education and experience in reading research articles are able to appraise, relatively consistently, articles based on different qualitative research designs. We hope that vaks will be used and further developed. © 2011 blackwell publishing ltd.","","","2012","10.1111/j.1365-2648.2011.05898.x","","","scopus-2-s2.0-84864304918.pdf","scopus-2-s2.0-84864304918"
"Availability of Clinical Trial Data From Industry-Sponsored Cardiovascular Trials","Murugiah K., Ritchie J. D., Desai N. R., Ross J. S., Krumholz H. M.","Journal of the American Heart Association","","BACKGROUND: Industry-sponsored clinical trials produce high-quality data sets that can be used by researchers to generate new knowledge. We assessed the availability of individual participant-level data (IPD) from large cardiovascular trials conducted by major pharmaceutical companies and compiled a list of available trials.\\\\\\\\rMETHODS AND RESULTS: We identified all randomized cardiovascular interventional trials registered on ClinicalTrials.gov with >5000 enrollment sponsored by 1 of the top 20 pharmaceutical companies by 2014 global sales. Availability of IPD for each trial was ascertained by searching each company's website/data-sharing portal. If availability could not be determined each company was contacted electronically. Of 60 included trials IPD are available for 15 trials (25%) consisting of 204 452 patients. IPD are unavailable for 15 trials (25%). Reasons for unavailability were: cosponsor did not agree to make IPD available (4 trials) and trials were not conducted within a specific time (5 trials); for the remaining 6 trials no specific reason was provided. For 30 trials (50%) availability of IPD could not be definitively determined either because of no response or requirements for a full proposal (23 trials).\\\\\\\\rCONCLUSIONS: IPD from 1 in 4 large cardiovascular trials conducted by major pharmaceutical companies are confirmed available to researchers for secondary research a valuable opportunity to enhance science. However IPD from 1 in 4 trials are not available and data availability could not be definitively determined for half of our sample. For several of these trials companies require a full proposal to determine availability making use of the IPD by researchers less certain. Copyright © 2016 The Authors. Published on behalf of the American Heart Association Inc. by Wiley Blackwell.","","","2016","10.1161/jaha.116.003307","","","medline-27098969.pdf","medline-27098969"
"Assessing the impact of transdisciplinary research: the usefulness of relevance, credibility, and legitimacy for understanding the link between process and impact","Hansson, S. And Polk, M.","Research Evaluation","","There is a call for more transdisciplinary (td) research, from academia, society, and funding agencies. Consequently, the field of td research is searching for ways of proving the value and providing evidence to support the effectiveness of such research. The main challenge for evaluating td research is attribution, that is how to link societal change to the td research process. However, little attention has been paid to the relationship between the quality of the research process and the effects and impacts that are being evaluated. Building upon earlier attempts at evaluating td research, this article tests three key aspects of effective sustainability research: its relevance, credibility, and legitimacy. To explore the link between the quality of process and societal effects, we analyze and compare outputs, outcomes, and impact of five td projects. Overall, our analysis shows that while relevance, credibility, and legitimacy gave important insights regarding the links between process and impacts, they are not adequate for evaluating td research impact. Process qualities such as practitioner motivation and perceived importance of the project, together with breadth of perspectives, the openness/flexibility of participants, and in-depth exchanges of expertise and knowledge, contributed to producing internally relevant, credible, and legitimate results. However, we also saw a need to develop the relevance, credibility, and legitimacy framework, in relation to the external dynamics of the project process, heterogeneous stakeholder groups, and the credibility of practice-based knowledge, which together with institutional factors and the political context significantly shape the possibility of impact. ©the author(s) 2018. Published by oxford university press.","","","2018","10.1093/reseval/rvy004","","","scopus-2-s2.0-85045514978.pdf","scopus-2-s2.0-85045514978"
"An Open Conversation on Using Eye-Gaze Methods in Studies of Neurodevelopmental Disorders","Venker C. E., Kover S. T.","Journal of Speech Language & Hearing Research","","PURPOSE: Eye-gaze methods have the potential to advance the study of neurodevelopmental disorders. Despite their increasing use challenges arise in using these methods with individuals with neurodevelopmental disorders and in reporting sufficient methodological detail such that the resulting research is replicable and interpretable.\\\\\\\\rMETHOD: This tutorial presents key considerations involved in designing and conducting eye-gaze studies for individuals with neurodevelopmental disorders and proposes conventions for reporting the results of such studies.\\\\\\\\rRESULTS: Methodological decisions (e.g. whether to use automated eye tracking or manual coding implementing strategies to scaffold children's performance defining valid trials) have cascading effects on the conclusions drawn from eye-gaze data. Research reports that include specific information about procedures missing data and selection of participants will facilitate interpretation and replication.\\\\\\\\rCONCLUSIONS: Eye-gaze methods provide exciting opportunities for studying neurodevelopmental disorders. Open discussion of the issues presented in this tutorial will improve the pace of productivity and the impact of advances in research on neurodevelopmental disorders.","","","2015","10.1044/2015_jslhr-l-14-0304","","","medline-26363412.pdf","medline-26363412"
"Mapping the mooc research landscape: insights from empirical studies","Costello, E. And Soverino, T. And Bolger, R.","International Journal Of Emerging Technologies In Learning","","Several reviews have been conducted of empirical studies of mooc learners and teachers. The scope and foci of such reviews has varied, as has the reporting of the details of how they were conducted. This study analysed 1,435 published articles, determining 922 to be empirical studies. We analysed the full text of 826 of these articles to which the research team had access using the scientometric tool scival, manual researcher evaluation and topic modelling to determine: the impact as measured by citations;  geographic and institutional publishing patterns;  and the themes and types of mooc research. We found that mooc research is mostly clustered in the discipline of computer science. Learner persistence and self-regulated learning continue to be a focus of study and most impactful finding respectively as studies of previous periods have found. Research is carried out worldwide, with the most influential studies and researchers clustered in particular institutions and countries. Implications of this study are that mooc research is clustered in certain ways which may give rise to particular biases, that researchers should consider more interdisciplinary approaches in their research and greater awareness and use of open science principles and practices in their work. © 2022. All rights reserved.","","","2022","10.3991/ijet.v17i14.28721","","","scopus-2-s2.0-85135263766.pdf","scopus-2-s2.0-85135263766"
"Data replication in mobile edge computing systems to reduce latency in internet of things","Saranya, N. And Geetha, K. And Rajan, C.","Wireless Personal Communications","","The progress in the development in the field of information technology has brought the internet of things (iot) into existence to play a crucial role in our daily lives. There are interconnected sensors or devices that can both collect and also exchange various data among themselves by employing a modern network of communication as an infrastructure that has been connected by many millions of the iot nodes. After this, there are various applications of the iot that may be able to provide accurate and fine-grained services to the users. Using this as a strategy which can mitigate an escalation to the congestion of resources, edge computing is emerging as the new paradigm that solves the needs of localized computing and the iot. The mobile edge computing (mec) has been emerging to handle the volume of data produced and this can reach a latency of demand of the iot applications that are intensive in terms of computation. Even though the mec has advanced in terms of latency of service and has been solidly investigated, the efficiency of data usage and security are not identified clearly. Replication of data is well suited for improving the time taken for a response, global traffic and data sharing as even at the time of server disconnection this can be done. In this work, efficient techniques of data replication for the mobile ad hoc networks (manet) like the simple and the random applications are evaluated for improving availability of data which considers all the issues that are related to the manet like consumption of power, availability of resource, time taken for response and consistency management. The results of the experiment have shown that a random algorithm for replication can achieve a bandwidth that is better in terms of savings compared to a simple replication algorithm. © 2020, springer science+business media, llc, part of springer nature.","","","2020","10.1007/s11277-020-07168-7","","","scopus-2-s2.0-85078365184.pdf","scopus-2-s2.0-85078365184"
"Industrial methodology for process verification in research (IMPROVER): toward systems biology verification. [Review]","Meyer P., Hoeng J., Rice J. J., Norel R., Sprengel J., Stolle K., Bonk T., Corthesy S., Royyuru A., Peitsch M. C., Stolovitzky G.","","","MOTIVATION: Analyses and algorithmic predictions based on high-throughput data are essential for the success of systems biology in academic and industrial settings. Organizations such as companies and academic consortia conduct large multi-year scientific studies that entail the collection and analysis of thousands of individual experiments often over many physical sites and with internal and outsourced components. To extract maximum value the interested parties need to verify the accuracy and reproducibility of data and methods before the initiation of such large multi-year studies. However systematic and well-established verification procedures do not exist for automated collection and analysis workflows in systems biology which could lead to inaccurate conclusions. RESULTS: We present here a review of the current state of systems biology verification and a detailed methodology to address its shortcomings. This methodology named 'Industrial Methodology for Process Verification in Research' or IMPROVER consists on evaluating a research program by dividing a workflow into smaller building blocks that are individually verified. The verification of each building block can be done internally by members of the research program or externally by 'crowd-sourcing' to an interested community. www.sbvimprover.com IMPLEMENTATION: This methodology could become the preferred choice to verify systems biology research workflows that are becoming increasingly complex and sophisticated in industrial and academic settings.","","","2012","","","","unknown-1954.pdf","unknown-1954"
"Effect of impact factor and discipline on journal data sharing policies","Resnik D.b. And Morales M. And Landrum R. And Shi M. And Minnier J. And Vasilevsky N.a. And Champieux R.e.","Account Res","","Data sharing is crucial to the advancement of science because it facilitates collaboration, transparency, reproducibility, criticism, and re-analysis. Publishers are well-positioned to promote sharing of research data by implementing data sharing policies. While there is an increasing trend toward requiring data sharing, not all journals mandate that data be shared at the time of publication. In this study, we extended previous work to analyze the data sharing policies of 447 journals across several scientific disciplines, including biology, clinical sciences, mathematics, physics, and social sciences. Our results showed that only a small percentage of journals require data sharing as a condition of publication, and that this varies across disciplines and impact factors. Both impact factors and discipline are associated with the presence of a data sharing policy. Our results suggest that journals with higher impact factors are more likely to have data sharing policies;  use shared data in peer review;  require deposit of specific data types into publicly available data banks;  and refer to reproducibility as a rationale for sharing data. Biological science journals are more likely than social science and mathematics journals to require data sharing.","","","2019","10.1080/08989621.2019.1591277","","","embase-627030068.pdf","embase-627030068"
"Why they shared: recovering early arguments for sharing social scientific data","Hauptmann E.","Sci Context","","Most social scientists today think of data sharing as an ethical imperative essential to making social science more transparent, verifiable, and replicable. But what moved the architects of some of the u.s.'s first university-based social scientific research institutions, the university of michigan's institute for social research (isr), and its spin-off, the inter-university consortium for political and social research (icpsr), to share their data? Relying primarily on archived records, unpublished personal papers, and oral histories, i show that angus campbell, warren miller, philip converse, and others understood sharing data not as an ethical imperative intrinsic to social science but as a useful means to the diverse ends of financial stability, scholarly and institutional autonomy, and epistemological reproduction. I conclude that data sharing must be evaluated not only on the basis of the scientific ideals its supporters affirm, but also on the professional objectives it serves.","","","2020","10.1017/s0269889720000204","","","embase-634573802.pdf","embase-634573802"
"A guideline for reporting performance metrics with electrochemical capacitors: from electrode materials to full devices","Balducci, A. And Belanger, D. And Brousse, T. And Long, J.w. And Sugimoto, W.","Journal Of The Electrochemical Society","","Over the past decade, interest in electrochemical capacitors as an energy-storage technology has increased enormously, spurring the development and evaluation of a large number of new materials and device configurations. This perspective article aims to propose guidelines by which new materials and devices should be evaluated, and how resulting data should be reported with respect to critical metrics such as capacitance, energy and power. © the author(s) 2017. Published by ecs. All rights reserved.","","","2017","10.1149/2.0851707jes","","","scopus-2-s2.0-85019985604.pdf","scopus-2-s2.0-85019985604"
"A parallel and forward private searchable public-key encryption for cloud-based data sharing","Chen, B. And Wu, L. And Li, L. And Choo, K.-K.r. And He, D.","Ieee Access","","Data sharing through the cloud is flourishing with the development of cloud computing technology. The new wave of technology will also give rise to new security challenges, particularly the data confidentiality in cloud-based sharing applications. Searchable encryption is considered as one of the most promising solutions for balancing data confidentiality and usability. However, most existing searchable encryption schemes cannot simultaneously satisfy requirements for both high search efficiency and strong security due to lack of some must-have properties, such as parallel search and forward security. To address this problem, we propose a variant searchable encryption with parallelism and forward privacy, namely the parallel and forward private searchable public-key encryption (pfp-spe). Pfp-spe scheme achieves both the parallelism and forward privacy at the expense of slightly higher storage costs. Pfp-spe has similar search efficiency with that of some searchable symmetric encryption schemes but no key distribution problem. The security analysis and the performance evaluation on a real-world dataset demonstrate that the proposed scheme is suitable for practical application. © 2013 ieee.","","","2020","10.1109/access.2020.2971089","","","scopus-2-s2.0-85081105298.pdf","scopus-2-s2.0-85081105298"
"Sharing voxelwise neuroimaging results from rhesus monkeys and other species with neurovault","Fox, A.s. And Holley, D. And Klink, P.c. And Arbuckle, S.a. And Barnes, C.a. And Diedrichsen, J. And Kwok, S.c. And Kyle, C. And Pruszynski, J.a. And Seidlitz, J. And Zhou, X. And Poldrack, R.a. And Gorgolewski, K.j.","Neuroimage","","Animal neuroimaging studies can provide unique insights into brain structure and function, and can be leveraged to bridge the gap between animal and human neuroscience. In part, this power comes from the ability to combine mechanistic interventions with brain-wide neuroimaging. Due to their phylogenetic proximity to humans, nonhuman primate neuroimaging holds particular promise. Because nonhuman primate neuroimaging studies are often underpowered, there is a great need to share data amongst translational researchers. Data sharing efforts have been limited, however, by the lack of standardized tools and repositories through which nonhuman neuroimaging data can easily be archived and accessed. Here, we provide an extension of the neurovault framework to enable sharing of statistical maps and related voxelwise neuroimaging data from other species and template-spaces. Neurovault, which was previously limited to human neuroimaging data, now allows researchers to easily upload and share nonhuman primate neuroimaging results. This promises to facilitate open, integrative, cross-species science while affording researchers the increased statistical power provided by data aggregation. In addition, the neurovault code-base now enables the addition of other species and template-spaces. Together, these advances promise to bring neuroimaging data sharing to research in other species, for supplemental data, location-based atlases, and data that would otherwise be relegated to a ""file-drawer"". As increasing numbers of researchers share their nonhuman neuroimaging data on neurovault, this resource will enable novel, large-scale, cross-species comparisons that were previously impossible. © 2020 the authors","","","2021","10.1016/j.neuroimage.2020.117518","","","scopus-2-s2.0-85095425122.pdf","scopus-2-s2.0-85095425122"
"The MetaboLights repository: curation challenges in metabolomics","Salek R. M., Haug K., Conesa P., Hastings J., Williams M., Mahendraker T., Maguire E., Gonzalez-Beltran A. N., Rocca-Serra P., Sansone S. A., Steinbeck C.","Database: The Journal of Biological Databases and Curation","","MetaboLights is the first general-purpose open-access curated repository for metabolomic studies their raw experimental data and associated metadata maintained by one of the major open-access data providers in molecular biology. Increases in the number of depositions number of samples per study and the file size of data submitted to MetaboLights present a challenge for the objective of ensuring high-quality and standardized data in the context of diverse metabolomic workflows and data representations. Here we describe the MetaboLights curation pipeline its challenges and its practical application in quality control of complex data depositions. Database URL: http://www.ebi.ac.uk/metabolights.","","","2013","10.1093/database/bat029","","","medline-23630246.pdf","medline-23630246"
"[From record keeping to scientific research: obstacles and opportunities for research with electronic health records]","Scholte R. A., Opmeer B. C., Ploem M. C.","Nederlands Tijdschrift voor Geneeskunde","","As a result of increasing digitisation of medical record keeping electronic health records (EHRs) are an attractive source for data reuse. However such record-based research is still suffering from poor quality of data stored in EHRs. Lack of consent for reuse of data also plays an impeding role especially in retrospective record-based research. That said increasing cooperation between healthcare institutions and current attention for EHR organisation also offer opportunities for record-based research. Patient data can be recorded in more standardised ways and in increasingly harmonised EHRs. In addition if healthcare institutions were to establish a generic consent procedure - preferably with national scope - the potential of EHRs for scientific research could be exploited in considerably better ways.","","","2017","","","","medline-29192569.pdf","medline-29192569"
"Shoulder Arthroplasty Trials Are Infrequently Registered: A Systematic Review of Trials","Sims M. T., Sanchez Z. C., Herrington J. M., Hensel J. B., Henning N. M., Scheckel C. J., Vassar M.","PLoS ONE [Electronic Resource]","","INTRODUCTION: With the intent of improving transparency in clinical research the International Committee of Medical Journal Editors (ICMJE) established guidelines in 2005 regarding prospective clinical trial registration. This action worked to address bias related to selective outcome reporting in the medical literature. The objective of this study was to assess and characterize the quality of registration of clinical trials appearing in shoulder arthroplasty-related medical journals. METHODS: All randomized trials involving human subjects pertaining to shoulder arthroplasty published between July 1 2005 and December 31 2015 and indexed in either PubMed or SportDISCUS were analyzed. We assessed the prevalence of registration the timing of registration relative to patient enrollment periods and the variable rates of orthopedic journal compliance with ICMJE and Food and Drug Administration clinical registration standards for our study. RESULTS: Of the 382 articles identified 345 (90.3%) were excluded due to failure to meet inclusion criteria. From the remaining 37 only 12 (32.4%) studies were found to be registered in a trial registry. Ten (10/12 83.3%) of these provided their registration information within the body of the article. None of the included studies from ICMJE-recognized journals were registered. From 34 included studies from non-ICMJE recognized journals 12 (35.3%) were registered. CONCLUSION: The level of compliance with clinical trial registration guidelines in the decade since their release among shoulder arthroplasty trials in orthopedic journals is poor. Given the importance of the issue the prevalence of the problem and the fact that many other medical specialties have already made efforts to improve ICMJE compliance further work on the part of orthopedic surgery journal authors and editors is needed to ensure the publication of unbiased results. Trial registration: Umin000022487.","","","2016","10.1371/journal.pone.0164984","","","medline-27764210.pdf","medline-27764210"
"Sharing individual patient and parasite-level data through the WorldWide Antimalarial Resistance Network platform: A qualitative case study","Pisani E., Botchway S.","Wellcome Open Research","","BACKGROUND: Increasingly biomedical researchers are encouraged or required by research funders and journals to share their data but there's very little guidance on how to do that equitably and usefully especially in resource-constrained settings. We performed an in-depth case study of one data sharing pioneer: the WorldWide Antimalarial Resistance Network (WWARN). METHODS: The case study included a records review a quantitative analysis of WAARN-related publications in-depth interviews with 47 people familiar with WWARN and a witness seminar involving a sub-set of 11 interviewees. RESULTS: WWARN originally aimed to collate clinical in vitro pharmacological and molecular data into linked open-access databases intended to serve as a public resource to guide antimalarial drug treatment policies. Our study describes how WWARN navigated challenging institutional and academic incentive structures alongside funders' reluctance to invest in capacity building in malaria-endemic countries which impeded data sharing. The network increased data contributions by focusing on providing free online tools to improve the quality and efficiency of data collection and by inviting collaborative authorship on papers addressing policy-relevant questions that could only be answered through pooled analyses. By July 1 2016 the database included standardised data from 103 molecular studies and 186 clinical trials representing 135000 individual patients. Developing the database took longer and cost more than anticipated and efforts to increase equity for data contributors are on-going. However analyses of the pooled data have generated new methods and influenced malaria treatment recommendations globally. Despite not achieving the initial goal of real-time surveillance WWARN has developed strong data governance and curation tools which are now being adapted relatively quickly for other diseases. CONCLUSIONS: To be useful data sharing requires investment in long-term infrastructure. To be feasible it requires new incentive structures that favour the generation of reusable knowledge.","","","2017","10.12688/wellcomeopenres.12259.1","","","pubmed-29018840.pdf","pubmed-29018840"
"Competitive communities of practice, knowledge sharing, and machiavellian participation: a case study","Schofield, K. And Analoui, B. And Brooks, J. And Hussain, S.f.","International Journal Of Training And Development","","This paper explores the emergence of machiavellian behaviour in a community of practice (cop). The cop was initiated by the top management team (tmt) as a management development initiative. Participants in a manufacturing setting were encouraged to engage in a series of problem-solving tasks with counterparts from across the organization in a short-term cop. A qualitative case study, using in-depth interviews, was conducted in a large processing plant in the middle eastern kingdom of bahrain. This is an empirical case study that explores employee participation in a short-term management development programme which sought to create cops to enable knowledge sharing. A competitive element was introduced, and we contend this promoted behaviour which served the individuals rather than the cop. The findings indicate that tmt intervention changes the dynamics of cops, reducing knowledge sharing and collaboration among community members. Recommendations are made to practitioners to be cognizant of the possibility of machiavellian participation in cops. © 2018 brian towers (britow) and john wiley & sons ltd.","","","2018","10.1111/ijtd.12129","","","scopus-2-s2.0-85050454802.pdf","scopus-2-s2.0-85050454802"
"Confidentiality in participatory research: challenges from one study","Petrova, E. And Dewing, J. And Camilleri, M.","Nursing Ethics","","Aim: this article presents key ethical challenges that were encountered when conducting a participatory qualitative research project with a very specific, small group of nurses, in this case with practice development nurses in malta. Background: with the small number of nurses employed in practice development roles in malta, there are numerous difficulties of maintaining confidentiality. Poorly constructed interventions by the researcher could have resulted in detrimental effects to research participants and the overall trustworthiness of the research. Generally, ethical guidelines for research exist to reinforce validity of research;  however, there is not an established consensus on how these strategies can be utilised in some types of qualitative field work. Research design: the researcher used an exploratory case study methodology. The sample consisted of 10 participants who were interviewed twice using face-to-face interviews, over a period of 2 months. Ethical considerations: the study was ethically reviewed by the university research ethics committee and the faculty research ethics committee, university of malta. The participants referred to in this article have been given adequate information about the study and their consent has been obtained. Discussion: numerous strategies for ensuring confidentiality during recruitment of the participants, during data collection, during transcription and data analysis and during dissemination of research results assisted the researcher in responding to potential and actual ethical issues. Conclusion: this article emphasises the main strategies that can be used to respond to ethical challenges when researching with a small easily identifiable group. The learning discussed here may be relevant to or even transferable to other similar research studies or research contexts. These methods fostered a greater credibility throughout the research process and predisposed the participants to greater trust, and thus, they disclosed their experiences and speak more freely, thus enhancing the quality of the study. © 2014, © the author(s) 2014.","","","2016","10.1177/0969733014564909","","","scopus-2-s2.0-84973496081.pdf","scopus-2-s2.0-84973496081"
"Impact of new Food and Drug Administration regulations on college university and experiment station researchers","Willett L. B.","Journal of Dairy Science","","The Good Laboratory Practice regulations adopted by the Food and Drug Administration describe specific procedures to assure the integrity of the research results. Those studies conducted with the intent to provide data on the safety of drugs and chemicals will be required to comply with the published relations. The process of bringing research laboratories into compliance with the regulations may be either arduous or fairly routine depending on the organization goals and type of research. Typically the Good Laboratory Practice regulations will increase sharply the cost of health safety information. Hiring more and better trained technical and professional personnel will be much of this expense. If university and experiment station researchers choose to avoid compliance with these regulations then agricultural research science may not continue to be recognized as an authority on the safety of products used for production of human food. Irrespective of whether universities choose to conduct regulated research or delegate this role to other segments of society academic institutions must assume the role of training those individuals needed to conduct toxicity research.","","","1981","10.3168/jds.s0022-0302(81)82781-6","","","medline-7320313.pdf","medline-7320313"
"Clap-pre: certificateless autonomous path proxy re-encryption for data sharing in the cloud","Ren, C. And Dong, X. And Shen, J. And Cao, Z. And Zhou, Y.","Applied Sciences (Switzerland)","","In e-health systems, patients encrypt their personal health data for privacy purposes and upload them to the cloud. There exists a need for sharing patient health data with doctors for healing purposes in one’s own preferred order. To achieve this fine-gained access control to delegation paths, some researchers have designed a new proxy re-encryption (pre) scheme called autonomous path proxy re-encryption (ap-pre), where the delegator can control the whole delegation path in a multi-hop delegation process. In this paper, we introduce a certificateless autonomous path proxy re-encryption (clap-pre) using multilinear maps, which holds both the properties (i.e., certificateless, autonomous path) of certificateless encryption and autonomous path proxy re-encryption. In the proposed scheme, (a) each user has two public keys (user’s identity and traditional public key) with corresponding private keys, and (b) each ciphertext is first re-encrypted from a public key encryption (pke) scheme to an identity-based encryption (ibe) scheme and then transformed in the ibe scheme. Our scheme is an ind-cpa secure clap-pre scheme under the k-multilinear decisional diffie–hellman (k-mddh) assumption in the random oracle model. © 2022 by the authors. Licensee mdpi, basel, switzerland.","","","2022","10.3390/app12094353","","","scopus-2-s2.0-85129001034.pdf","scopus-2-s2.0-85129001034"
"Multicenter data banking in management of dizzy patients: first results from the dizzynet registry project","Grill, E. And Akdal, G. And Becker-Bense, S. And Hübinger, S. And Huppert, D. And Kentala, E. And Strobl, R. And Zwergal, A. And Celebisoy, N.","Journal Of Neurology","","Purpose: comprehensive phenotypical data across countries is needed to understand the determinants, prognosis and consequences of vestibular disease. The registry is a data repository for the members of the european dizzynet. We report results from a pilot study using data from turkey and germany. Methods: the pilot study included a convenience sample of patients aged 18 or above referred to ege university medical school hospital, dokuz eylül university hospital, izmir, turkey, and the german center for german center for vertigo and balance disorders, university on munich, germany, with symptoms of vertigo or dizziness. Health-related quality of life was assessed with the eq5-d and the dizziness handicap inventory (dhi). To obtain comparable groups we matched data from the two countries for age, sex and diagnosis by propensity score. Results: we included 80 adult patients, 40 from each country (60% female, mean age 54.1, sd 12.4). Matching was successful. Vestibular migraine (34%) was the most frequent diagnosis, followed by benign paroxysmal positional vertigo (29%) and menière’s disease (12%). Clinical signs and symptoms were comparable in both countries. Patients from turkey were more likely to report headaches (65 vs. 32%) and to show gait unsteadiness (51 vs. 5%). Patients from germany reported significantly higher quality of life and lower values of the dhi score. Conclusions: sharing data facilitates research, enhances translation from basic science into clinical applications, and increases transparency. The dizzynet registry is a first step to data sharing in vestibular research across europe. © 2018, springer-verlag gmbh germany, part of springer nature.","","","2018","10.1007/s00415-018-8864-1","","","scopus-2-s2.0-85045481769.pdf","scopus-2-s2.0-85045481769"
"Value of open data: a geoscience perspective","Wildman, G. And Lewis, E.","Geoscience Data Journal","","We are living in a data-centric society, with governments and businesses increasingly looking at what they can do to gain insight and improve the flow of data. Encouraging the release of data as ‘open data’ is one measure that would remove barriers to access, increase use and facilitate downstream data innovation. Using examples from firstly the non-geoscience and then geoscience sectors, this paper outlines three factors that can lead to a successful open data programme. These are (1) having a clear strategy with a well-articulated vision;  (2) ensuring that data are not only free but also technically accessible and delivered under an open licence;  and (3) continued investment in the programme to ensure its long-term success. However, not all data can or should be open, and organizations and governments must be careful that their interventions do not have unintended consequences that might reduce incentives to collect, maintain and share data. A primary concern is the financial sustainability of a dataset, but this also extends to other risks that would prevent the data being widely shared such as the inclusion of personal data or third-party intellectual property. In these cases, use of a data-sharing risk assessment framework, and the application of the fair principles of findable, accessible, interoperable and reusable can be used to increase data sharing and maximize the benefits that can be realized from geoscience data. © 2021 british geological survey, a component of ukri. Geoscience data journal published by royal meteorological society and john wiley & sons ltd.","","","2022","10.1002/gdj3.138","","","scopus-2-s2.0-85122960641.pdf","scopus-2-s2.0-85122960641"
"Exploring COVID-19 research credibility among Spanish scientists","Garcia-Garzon E., Angulo-Brunet A., Lecuona O., Barrada J. R., Corradi G.","Current Psychology","","Amidst a worldwide vaccination campaign trust in science plays a significant role when addressing the COVID-19 pandemic. Given current concerns regarding research standards we were interested in how Spanish scholars perceived COVID-19 research and the extent to which questionable research practices and potentially problematic academic incentives are commonplace. We asked researchers to evaluate the expected quality of their COVID-19 projects and other peers' research and compared these assessments with those from scholars not involved in COVID-19 research. We investigated self-admitting and estimated rates of questionable research practices and attitudes towards current research status. Responses from 131 researchers suggested that COVID-19 evaluations followed partisan lines with scholars being more pessimistic about others' colleagues' research than their own. Additionallyresearchers not involved in COVID-19 projects were more negative than their participating peers. These differences were particularly notable for areas such as the expected theoretical foundations or overall quality of the research among others. Most Spanish scholars expected questionable research practices and inadequate incentives to be widespread. In these two aspects researchers tended to agree regardless of their involvement in COVID-19 research. We provide specific recommendations for improving future meta-science studies such as redefining QRPs as inadequate research practices (IRP). This change could help avoid key controversies regarding QRPs' definition while highlighting their detrimental impact. Lastly we join previous calls to improve transparency and academic career incentives as a cornerstone for generating trust in science.\\\\\\\\rSUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007/s12144-022-02797-6. Copyright © The Author(s) under exclusive licence to Springer Science+Business Media LLC part of Springer Nature 2022.","","","2022","10.1007/s12144-022-02797-6","","","medline-35250242.pdf","medline-35250242"
"A scoping review: synthesizing evidence on data management instruction in academic libraries","Xu, Z. And Zhou, X. And Kogut, A. And Watts, J.","Journal Of Academic Librarianship","","The present scoping review examines empirical rdm instruction-related studies in academic libraries between 2010 and 2021. We searched three databases (lista, eric, and medline complete) and two journals (journal of escience librarianship and international journal of digital curation) and identified 124 articles for inclusion. Cohen's kappa indicated a strong to perfect inter-rater reliability on the coding between the authors. Overall, the findings indicate an increasing trend in empirical research regarding the topic of rdm instruction across many countries and regions after 2010. Also, faculty, researchers, and librarians in the higher education field were the primary audiences for the rdm instruction with few studies addressing rdm instruction for the undergraduate and graduate levels. In terms of the rdm aspects, rdm needs assessments were investigated the most among the reviewed studies, followed by data sharing and data management plans. Additionally, the face-to-face learning context was the most popular for rdm instruction, followed by online and hybrid contexts. However, few studies used an intervention research design while delivering instruction to the target audience. This study highlights the substantial characteristics and methodological designs of the rdm instruction empirical research and provides implications for approaches and techniques used to study rdm instruction in academic libraries. © 2022","","","2022","10.1016/j.acalib.2022.102508","","","scopus-2-s2.0-85126023531.pdf","scopus-2-s2.0-85126023531"
"A web-based pilot study of inter-pathologist reproducibility using the ISHLT 2004 working formulation for biopsy diagnosis of cardiac allograft rejection: the European experience","Angelini A., Andersen C. B., Bartoloni G., Black F., Bishop P., Doran H., Fedrigo M., Fries J. W., Goddard M., Goebel H., Neil D., Leone O., Marzullo A., Ortmann M., Paraf F., Rotman S., Turhan N., Bruneval P., Frigo A. C., Grigoletto F., Gasparetto A., Mencarelli R., Thiene G., Burke M.","Journal of Heart & Lung Transplantation","","BACKGROUND: The aim of this study was to assess at the European level and using digital technology the inter-pathologist reproducibility of the ISHLT 2004 system and to compare it with the 1990 system We also assessed the reproducibility of the morphologic criteria for diagnosis of antibody-mediated rejection detailed in the 2004 grading system.\\\\\\\\rMETHODS: The hematoxylin-eosin-stained sections of 20 sets of endomyocardial biopsies were pre-selected and graded by two pathologists (A.A. and M.B.) and digitized using a telepathology digital pathology system (Aperio ImageScope System; for details refer to http://aperio.com/). Their diagnoses were considered the index diagnoses which covered all grades of acute cellular rejection (ACR) early ischemic lesions Quilty lesions late ischemic lesions and (in the 2005 system) antibody-mediated rejection (AMR). Eighteen pathologists from 16 heart transplant centers in 7 European countries participated in the study. Inter-observer reproducibility was assessed using Fleiss's kappa and Krippendorff's alpha statistics.\\\\\\\\rRESULTS: The combined kappa value of all grades diagnosed by all 18 pathologists was 0.31 for the 1990 grading system and 0.39 for the 2005 grading system with alpha statistics at 0.57 and 0.55 respectively. Kappa values by grade for 1990/2005 respectively were: 0 = 0.52/0.51; 1A/1R = 0.24/0.36; 1B = 0.15; 2 = 0.13; 3A/2R = 0.29/0.29; 3B/3R = 0.13/0.23; and 4 = 0.18. For the 2 cases of AMR 6 of 18 pathologists correctly suspected AMR on the hematoxylin-eosin slides whereas in each of 17 of the 18 AMR-negative cases a small percentage of pathologists (range 5% to 33%) overinterpreted the findings as suggestive for AMR.\\\\\\\\rCONCLUSIONS: Reproducibility studies of cardiac biopsies by pathologists in different centers at the international level were feasible using digitized slides rather than conventional histology glass slides. There was a small improvement in interobserver agreement between pathologists of different European centers when moving from the 1990 ISHLT classification to the ""new"" 2005 ISHLT classification. Morphologic suspicion of AMR in the 2004 system on hematoxylin-eosin-stained slides only was poor highlighting the need for better standardization of morphologic criteria for AMR. Ongoing educational programs are needed to ensure standardization of diagnosis of both acute cellular and antibody-mediated rejection. Copyright 2011 International Society for Heart and Lung Transplantation. All rights reserved.","","","2011","10.1016/j.healun.2011.05.011","","","medline-21816625.pdf","medline-21816625"
"Mentoring and the impact of the research climate","Roberts G. C., Kavussanu M., Sprague R. L.","Science & Engineering Ethics","","In this article we focus on the mentoring process and we argue that the internal and external pressures extant at research universities may create a research culture that may be antithetical to appropriate mentoring. We developed a scale based on motivation theory to determine the perceived research culture in departments and research laboratories and a mentoring scale to determine approaches to mentoring graduate students. Participants were 610 faculty members across 49 departments at a research oriented university. The findings were that a mastery-oriented research climate and an outcome-oriented research climate were manifested at the university. More importantly each research climate had its own unique impact on how the faculty approached mentoring graduate students. A mastery research climate was related to a more supportive approach to mentoring than the outcome research climate. We concluded by suggesting that the outcome research climate may have an adverse effect on effective mentoring and on maintaining research ethics.","","","2001","","","","medline-11697009.pdf","medline-11697009"
"Length matters: Improved high field EEG-fMRI recordings using shorter EEG cables","Assecondi S., Lavallee C., Ferrari P., Jovicich J.","Journal of Neuroscience Methods","","BACKGROUND: The use of concurrent EEG-fMRI recordings has increased in recent years allowing new avenues of medical and cognitive neuroscience research; however currently used setups present problems with data quality and reproducibility.\\\\\\\\rNEW METHOD: We propose a compact experimental setup for concurrent EEG-fMRI at 4T and compare it to a more standard reference setup. The compact setup uses short EEG cables connecting to the amplifiers which are placed right at the back of the head RF coil on a form-fitting extension force-locked to the patient MR bed. We compare the two setups in terms of sensitivity to MR-room environmental noise interferences between measuring devices (EEG or fMRI) and sensitivity to functional responses in a visual stimulation paradigm.\\\\\\\\rRESULTS: The compact setup reduces the system sensitivity to both external noise and MR-induced artefacts by at least 60% with negligible EEG noise induced from the mechanical vibrations of the cryogenic cooling compression pump.\\\\\\\\rCOMPARISON WITH EXISTING METHODS: The compact setup improved EEG data quality and the overall performance of MR-artifact correction techniques. Both setups were similar in terms of the fMRI data with higher reproducibility for cable placement within the scanner in the compact setup.\\\\\\\\rCONCLUSIONS: This improved compact setup may be relevant to MR laboratories interested in reducing the sensitivity of their EEG-fMRI experimental setup to external noise sources setting up an EEG-fMRI workplace for the first time or for creating a more reproducible configuration of equipment and cables. Implications for safety and ergonomics are discussed. Copyright © 2016 Elsevier B.V. All rights reserved.","","","2016","10.1016/j.jneumeth.2016.05.014","","","medline-27222442.pdf","medline-27222442"
"Learning to ""share your science"": the open notebook as textual object and dynamic rhetorical space","Wickman, Chad","","","Laboratory notebooks have historically provided scientists with an important resource for documenting, warranting, and communicating the outcomes of their day-to-day research. Over the past decade, however, the genre has undergone some interesting changes: once a situated, print-bound activity, the practice of note making has begun to develop into a highly distributed, multimedia undertaking-a development that is beginning to reconfigure the traditional relationship between the laboratory, scientific community, and broader public sphere. The growing movement toward ""open notebook science"" (see bradley, 2006;  bradley, owens, & williams, 2008) specifically raises questions about .the ways in which notebooks function as a proprietary safeguard for individual scientists and as a dynamic rhetorical space within which groups of scientists and other stakeholders can present, discuss, revise, and circulate information. Through analysis of openwetware, a web-based scientific community, this chapter examines how scientists use familiar genres to balance the relationship between stability and change as they make the transition from traditional rhetorical practices that focus on individual ownership to emerging rhetorical practices that focus on the open sharing of information through digital infi^tmctures. I will argue specifically that open notebooks reinforce a type of ""shared praxis"" (nielsen, 2011) that is necessary for open access to take hold in the scientific community and that may be contributing to a broader shift in the rhetorical and epistemological dimensions of scientific research culture. (Psycinfo database record (c) 2023 apa, all rights reserved)","","","2016","","","","psychinfo-2016-17989-002.pdf","psychinfo-2016-17989-002"
"What is the best available science? A comparison of marine scientists, managers, and interest groups in the united states","Wolters, E.a. And Steel, B.s. And Lach, D. And Kloepfer, D.","Ocean And Coastal Management","","In recent years there have been calls among decision makers, interest groups, citizens, and scientists alike for the use of the ""best available science"" when making environmental policy and managing natural resources. The assumption is that including scientists and the best available scientific information will improve the quality of complex policy decisions. Others have argued, however, that science and scientists are just one source of expertise concerning environmental management and increasing involvement will not necessarily lead to better policy. We report on a study examining the attitudes and orientations of marine scientists, resource managers, and interest group representatives concerning factors that may affect scientific credibility, the credibility of scientific research produced by various organizations, and perceptions of the ability of certain groups to understand scientific research. Using national random sample surveys and interviews of marine scientists, marine managers, and interest groups involved in marine policy issues conducted in 2011, we examine indicators of scientific credibility, data, research and reputation;  the ability of scientists to communicate findings;  and the role of scientists in the policy process. Further, we explore what factors contribute to credible science, the credibility of the science produced by various organizations, and the scientific literacy of various policy actors. © 2016 elsevier ltd.","","","2016","10.1016/j.ocecoaman.2016.01.011","","","scopus-2-s2.0-84961346547.pdf","scopus-2-s2.0-84961346547"
"When good intentions-Open-Access Publishing-Take a wrong turn","Hinds Pamela S.","Cancer Nursing","","Discovering new knowledge in a trustworthy manner -so trustworthy that we would be willing to alter our care of the sick and injured-is precisely the purpose of research. Although infractions of credible research processes do occur adhering to established research strategies and to acknowledging infractions if they occur contributes to research findings being trustworthy. Such findings have the potential of (a) improving care and thereby care outcomes for children and adults with cancer and (b) sustaining support for the caregiving efforts of their family and professional care providers. Following the discovery of new knowledge and the acknowledgement of infractions that could have possibly influenced findings is the sharing of research findings. Sharing of findings also needs to occur in a trustworthy manner. Highly valued by researchers is sharing trustworthy findings as quickly as possible to benefit others as soon as possible. One approach to making credible research findings quickly available to all is open-access publishing. These motives for open-access publishing are respectable and important. The particularly concerning outcome of this wrong turn of good intentions is that peer review is commonly sacrificed. Peer review is one of the cornerstones of scientific publishing's services. Minus that review process trustworthiness of the overall publication process is immediately diminished. Academic systems have been described as overwhelmed by the sudden surge of publishing by faculty who did not understand the predatory nature of this business wrong turn and who relied on the claimed (fabricated) impact factors for their academic promotion. (PsycInfo Database Record (c) 2021 APA all rights reserved)","","","2015","10.1097/ncc.0000000000000219","","","medline-25479242.pdf","medline-25479242"
"A comparison of agree and right: which clinical practice guideline reporting checklist should be followed by guideline developers?","Yao, X. And Ma, J. And Wang, Q. And Kanters, D. And Ali, M.u. And Florez, I.d.","Journal Of General Internal Medicine","","Background: a clinical practice guideline (cpg) reporting checklist is used to assist cpg developers in recording what content should be provided in a cpg report. Recently, two checklists have become available on the enhancing the quality and transparency of health research network website: agree (appraisal of guidelines, research and evaluation) published in 2016 and right (reporting items for practice guidelines in healthcare) published in 2017. The objective of this study was to describe the advantages and disadvantages of these two cpg reporting checklists. Methods: two epidemiologists who lacked experience using both agree and right but were familiar with evidence-based medicine methodology independently compared agree with right on an item-by-item basis. Their assessments were compiled on a pre-designed data form and any disagreements were resolved through discussion. Three other co-authors independently compared agree with right and decided if they agreed with the results of comparison of the two cpg reporting checklists from the first two co-authors. Finally, another co-author reviewed the comparison results to ensure that the description was clear and understandable. Results: the following six relationships between the two checklists were observed: (1) 11 items from agree completely matched with 12 items from right;  (2) four items were listed in agree only;  (3) 12 items were listed in right only;  (4) three items in agree were partially covered by three items in right;  (5) six items in right were partially covered by three items in agree;  and (6) two items intersected across agree and right. Based on the comparison results, the potential impact analysis of selecting either checklist is described. Discussion: we recommend that cpg developers use either agree plus items unique to right or right plus items unique to agree. © 2019, society of general internal medicine.","","","2020","10.1007/s11606-019-05508-3","","","scopus-2-s2.0-85075179654.pdf","scopus-2-s2.0-85075179654"
"The quality of reporting of orthopaedic randomized trials with use of a checklist for nonpharmacological therapies","Chan S., Bhandari M.","Journal of Bone & Joint Surgery - American Volume","","BACKGROUND: The Consolidated Standards of Reporting Trials statement for the reporting of randomized controlled trials has been limited by its applicability to surgical trials. In response a Checklist to Evaluate a Report of a Nonpharmacological Trial was recently developed by the Consolidated Standards of Reporting Trials group to address reporting issues in surgical trials. We aimed (1) to apply the checklist for nonpharmacological therapies to orthopaedic randomized controlled trials across multiple journals from 2004 through 2005 and (2) to survey authors when methodological safeguards itemized in the checklist were not reported to determine whether they actually had been performed. We hypothesized that lack of reporting of a methodological safeguard did not necessarily mean it had not been conducted.\\\\\\\\rMETHODS: We searched for relevant orthopaedic randomized controlled trials across eight journals in the period from January 2004 through December 2005. We applied the Checklist to Evaluate a Report of a Nonpharmacological Trial to all eligible studies. We contacted authors to determine what methodological safeguards were actually used especially when details remained unclear from the publication.\\\\\\\\rRESULTS: We included eighty-seven randomized controlled trials from eighty-five scientific reports. In assessing the randomized controlled trials with the checklist for nonpharmacological therapies seventy-three studies (84%) had unclear reporting of treatment allocation concealment. Only seventeen studies (20%) mentioned surgeon skill or experience. The blinding of patients ward staff rehabilitation staff clinical outcome assessors and nonclinical outcome assessors was unclear in forty-eight (55%) sixty-three (72%) sixty-four (74%) forty (46%) and thirty-three studies (38%) respectively. Authors from forty-three randomized controlled trials responded to our survey. The results of the survey showed that 41% (95% confidence interval 25% to 58%) of the trials had adequate allocation concealment when this had been unclear from the report. Although the surgical experience of the investigators was rarely reported most authors (70%) acknowledged that they had defined ""surgical expertise criteria"" such as minimum case criteria specialized training and clinical performance. The survey also showed that 28% to 40% of the trials had blinding of relevant groups despite the fact that the reporting of such blinding had been unclear in the publications.\\\\\\\\rCONCLUSIONS: The quality of reporting in the orthopaedic literature was highly variable. Readers should not assume that bias-reducing safeguards that were not reported in a randomized controlled trial did not occur. Our study reinforces the need for the consistent use of a tool like the Checklist to Evaluate a Report of a Nonpharmacological Trial to assess the methodology of surgical trials.","","","2007","10.2106/jbjs.f.01591","","","medline-17768194.pdf","medline-17768194"
"The impact of the national heart, lung, and blood institute data: analyzing published articles that used biolincc open access data","Alryalat S.a. And El Khatib O. And Al-Qawasmi O. And Alkasrawi H. And Al Zu'bi R. And Abu-Halaweh M. And Alkanash Y. And Habash I.","F1000res","","Background: data sharing is now a mandatory prerequisite for several major funders and journals, where researchers are obligated to deposit the data resulting from their studies in an openly accessible repository. Biomedical open data are now widely available in almost all disciplines, where researchers can freely access and reuse these data in new studies. We aim to assess the impact of open data in terms of publications generated using open data and citations received by these publications, where we will analyze publications that used the biologic specimen and data repository information coordinating center (biolincc) as an example. Method(s): as of july 2019, there was a total of 194 datasets stored in biolincc repository and accessable through their portal. We requested the full list of publications that used these datasets from biolincc, and we also performed a supplementary pubmed search for other publications. We used web of science (wos) to analyze the characteristics of publications and the citations they received. Result(s): 1,086 published articles used data from biolincc repository, but only 987 (90.88%) articles were wos indexed. The number of publications has steadily increased since 2002 and peaked in 2018 with a total number of 138 publications on that year. The 987 open data publications received a total of 34,181 citations up to 1 st october 2019. The average citation per item for the open data publications was 34.63. The total number of citations received by open data publications per year has increased from only 2 citations in 2002, peaking in 2018 with 2361 citations. Conclusion(s): the vast majority of studies that used biolincc open data were published in wos indexed journals and are receiving an increasing number of citations.copyright: © 2020 alryalat sa et al.","","","2020","10.12688/f1000research.21884.1","","","embase-636323017.pdf","embase-636323017"
"Data integration for materials research","Carey, N.s. And Budavári, T. And Daphalapurkar, N. And Ramesh, K.t.","Integrating Materials And Manufacturing Innovation","","Introduction: a new data science initiative in materials research has been launched at the johns hopkins university within the materials in extreme dynamic environments (mede) collaborative research alliance (cra). Our first goal is to build a solution that facilitates seamless data sharing among mede scientists. We expect to shorten the design and development cycle of new materials by providing integrated storage, database, and analysis services, building on proven components of the sciserver project developed at the institute for data intensive engineering and science (idies). Case description: here we present our system design and demonstrate the power of our approach through a use-case that enables easy comparison of simulations and measurements. This prototype effort, focusing on boron carbide (bc), brings together multiple materials research elements in the ceramics group within the mede cra. Discussion and evaluation: the sciserver platform offers single-sign on access to various general purpose data analysis tools familiar to materials scientists in mede. During the case study deployment, users appreciated the simple data file upload process, automated database ingestion, and platform applicability to both students of the art and power users. Conclusions: from our case study experience in aggregating data from both simulations and physical experiments, we developed a template workflow from which a user may run a common data comparison task outright or customize to another purpose. Next, we turn to acquiring data from more mede groups and expanding the user base to the metals group. © 2016, carey et al.","","","2016","10.1186/s40192-016-0049-0","","","scopus-2-s2.0-85051593274.pdf","scopus-2-s2.0-85051593274"
"Parental perspectives on consent for participation in large-scale, non-biological data repositories","Manhas K.p. And Page S. And Dodd S.x. And Letourneau N. And Ambrose A. And Cui X. And Tough S.c.","Life Sci Soc Policy","","Background: data sharing presents several challenges to the informed consent process. Unique challenges emerge when sharing pediatric or pregnancy-related data. Here, parent preferences for sharing non-biological data are examined., methods: groups (n=4 groups, 18 participants) and individual interviews (n=19 participants) were conducted with participants from two provincial, longitudinal pregnancy cohorts (aob and apron). Qualitative content analysis was applied to transcripts of semi-structured interviews., results: participants were supportive of a broad, one-time consent model or a tiered consent model. These preferences were grounded in the perceived obligations for reciprocity and accuracy. Parents want reciprocity among participants, repositories and researchers regarding respect and trust. Furthermore, parents' worry about the interrelationships between the validity of the consent processes and secondary data use., conclusions: though parent participants agree that their research data should be made available for secondary use, they believe their consent is still required. Given their understanding that obtaining and informed consent can be challenging in the case of secondary use, parents agreed that a broad, one-time consent model was acceptable, reducing the logistical burden while maintaining respect for their contribution. This broad model also maintained participant trust in the research and secondary use of their data. The broad, one-time model also reflected parents' perspectives surrounding child involvement in the consent process. The majority of parents felt decision made during childhood were the parents responsibility and should remain in parental purview until the child reaches the age of majority.","","","2016","10.1186/s40504-016-0034-6","","","embase-615999061.pdf","embase-615999061"
"Expanding Perspectives on Open Science: Communities Cultures and Diversity in Concepts and Practices - Proceedings of the 21st International Conference on Electronic Publishing","Chan L., Loizides F.","","","The proceedings contain 28 papers. The topics discussed include: benefits of open science: an analytical framework illustrated with case study; grey literature publishing in public policy: production and management costs and benefits; imparting knowledge in humanities. about some practices of scientific blogging on hypotheáses; rethinking openness: challenges and new approaches to open scholarly journals; alternative metrics for the evaluation of scholarly activities: an analysis of articles authored by Greek researchers; claims about benefits of open access to society (beyond academia); framing a situated and inclusive open science: emerging lessons from the open and collaborative science in development network; openness in scholarship: a return to core values?; OpenAIRE: supporting the H2020 open access mandate; new toolkits on the block: peer review alternatives in scholarly communication; open access policy and funding in Cyprus University of Technology a case study; Arxiv-based commenting resources by and for astrophysicists and physicists: an initial survey; and open science and accelerating discovery in rare and neglected diseases.","","","2017","","","","scopus-2-s2.0-85020758009.pdf","scopus-2-s2.0-85020758009"
"Geenar: a web tool for reproducible maldi-tof analysis","Del Prete E. And Facchiano A. And Profumo A. And Angelini C. And Romano P.","Front. Genet","","Mass spectrometry is a widely applied technology with a strong impact in the proteomics field. Maldi-tof is a combined technology in mass spectrometry with many applications in characterizing biological samples from different sources, such as the identification of cancer biomarkers, the detection of food frauds, the identification of doping substances in athletes' fluids, and so on. The massive quantity of data, in the form of mass spectra, are often biased and altered by different sources of noise. Therefore, extracting the most relevant features that characterize the samples is often challenging and requires combining several computational methods. Here, we present geenar, a novel web tool that provides a complete workflow for pre-processing, analyzing, visualizing, and comparing maldi-tof mass spectra. Geenar is user-friendly, provides many different functionalities for the analysis of the mass spectra, and supports reproducible research since it produces a human-readable report that contains function parameters, results, and the code used for processing the mass spectra. First, we illustrate the features available in geenar. Then, we describe its internal structure. Finally, we prove its capabilities in analyzing oncological datasets by presenting two case studies related to ovarian cancer and colorectal cancer. Geenar is available at http://proteomics.hsanmartino.it/geenar/.© copyright © 2021 del prete, facchiano, profumo, angelini and romano.","","","2021","10.3389/fgene.2021.635814","","","embase-634734080.pdf","embase-634734080"
"A protocol for the development of reporting guidelines for IDEAL stage studies","Agha R. A., Hirst A., Khachane A., McCulloch P.","International Journal of Surgery Protocols","","BACKGROUND: New surgical procedures devices and other complex interventions need robust evaluation for safety efficacy and effectiveness. The IDEAL Framework and Recommendations lay out a pathway to achieve this and offer general guidance on how studies at each stage should be reported. However researchers require some assistance in translating theory into practice. We will develop a set of reporting guidelines for each IDEAL stage where deemed necessary through Delphi consensus methodology. METHODS: For each IDEAL stage requiring a new set of reporting guidelines we will use the following process. We will search for the relevant reporting guidelines already in existence and use principles developed by the IDEAL Collaboration to compile the initial long list of potential checklist items. In each round the participants will rate the importance of reporting each element on a nine-point Likert scale as proposed by the GRADE group. Sequential rounds and questionnaire administration and completion will take place until a final set of items is produced. There will then be a final consensus meeting of a working group to condense and refine the final recommendations for the reporting guidelines.","","","2018","10.1016/j.isjp.2018.04.001","","","pubmed-31851736.pdf","pubmed-31851736"
"Dynamics of cumulative advantage and threats to equity in open science: a scoping review","Ross-Hellauer T., Reichmann S., Cole N. L., Fessl A., Klebel T., Pontika N.","Royal Society Open Science","","Open Science holds the promise to make scientific endeavours more inclusive participatory understandable accessible and re-usable for large audiences. However making processes open will not per se drive wide reuse or participation unless also accompanied by the capacity (in terms of knowledge skills financial resources technological readiness and motivation) to do so. These capacities vary considerably across regions institutions and demographics. Those advantaged by such factors will remain potentially privileged putting Open Science's agenda of inclusivity at risk of propagating conditions of 'cumulative advantage'. With this paper we systematically scope existing research addressing the question: 'What evidence and discourse exists in the literature about the ways in which dynamics and structures of inequality could persist or be exacerbated in the transition to Open Science across disciplines regions and demographics?' Aiming to synthesize findings identify gaps in the literature and inform future research and policy our results identify threats to equity associated with all aspects of Open Science including Open Access Open and FAIR Data Open Methods Open Evaluation Citizen Science as well as its interfaces with society industry and policy. Key threats include: stratifications of publishing due to the exclusionary nature of the author-pays model of Open Access; potential widening of the digital divide due to the infrastructure-dependent highly situated nature of open data practices; risks of diminishing qualitative methodologies as 'reproducibility' becomes synonymous with quality; new risks of bias and exclusion in means of transparent evaluation; and crucial asymmetries in the Open Science relationships with industry and the public which privileges the former and fails to fully include the latter.","","","2022","10.1098/rsos.211032","","","pubmed-35116143.pdf","pubmed-35116143"
"Data quality evaluation in open data in the municipality context:a systematic literature review","Troncoso-González, A. And Rodríguez, A. And Caro, A.","Ingeniare","","Nowadays, governments often share their data with the citizens through diverse digital platforms. This sharing of data, which has no restrictions or costs, has been called open government data. The increased use of this kind of data poses several challenges, such as the quality of the provided data. It becomes necessary to evaluate factors like the reliability, integrity, and accuracy of data and apply guidelines and metrics to ensure its quality for the intended use. This way, it is no longer sufficient to make data available, as having guidelines to evaluate its quality for other public agencies and citizens becomes a necessity. This article presents the realization of a systematic literature review to research existing proposals focused on evaluating open government data quality at a municipal level. As a result of this review, 56 articles have been identified. These are presented based on 3 posed research questions. © 2022, universidad de tarapaca. All rights reserved.","","","2022","10.4067/s0718-33052022000200255","","","scopus-2-s2.0-85137874155.pdf","scopus-2-s2.0-85137874155"
"The Resource Identification Initiative: a cultural shift in publishing","Bandrowski A., Brush M., Grethe J. S., Haendel M. A., Kennedy D. N., Hill S., Hof P. R., Martone M. E., Pols M., Tan S. C., Washington N., Zudilova-Seinstra E., Vasilevsky N.","Brain and Behavior","","A central tenet in support of research reproducibility is the ability to uniquely identify research resources that is reagents tools and materials that are used to perform experiments. However current reporting practices for research resources are insufficient to identify the exact resources that are reported or to answer basic questions such as ""How did other studies use resource X?"" To address this issue the Resource Identification Initiative was launched as a pilot project to improve the reporting standards for research resources in the methods sections of papers and thereby improve identifiability and scientific reproducibility. The pilot engaged over 25 biomedical journal editors from most major publishers as well as scientists and funding officials. Authors were asked to include Research Resource Identifiers (RRIDs) in their manuscripts prior to publication for three resource types: antibodies model organisms and tools (i.e. software and databases). RRIDs are assigned by an authoritative database for example a model organism database for each type of resource. To make it easier for authors to obtain RRIDs resources were aggregated from the appropriate databases and their RRIDs made available in a central web portal ( http://scicrunch.org/resources). RRIDs meet three key criteria: they are machine readable free to generate and access and are consistent across publishers and journals. The pilot was launched in February of 2014 and over 300 papers have appeared that report RRIDs. The number of journals participating has expanded from the original 25 to more than 40 with RRIDs appearing in 62 different journals to date. Here we present an overview of the pilot project and its outcomes to date. We show that authors are able to identify resources and are supportive of the goals of the project. Identifiability of the resources post-pilot showed a dramatic improvement for all three resource types suggesting that the project has had a significant impact on identifiability of research resources.","","","2016","10.1002/brb3.417","","","medline-27110440.pdf","medline-27110440"
"An open web-based module developed to advance data-driven hydrologic process learning","Lane, B. And Garousi-Nejad, I. And Gallagher, M.a. And Tarboton, D.g. And Habib, E.","Hydrological Processes","","The era of ‘big data’ promises to provide new hydrologic insights, and open web-based platforms are being developed and adopted by the hydrologic science community to harness these datasets and data services. This shift accompanies advances in hydrology education and the growth of web-based hydrology learning modules, but their capacity to utilize emerging open platforms and data services to enhance student learning through data-driven activities remains largely untapped. Given that generic equations may not easily translate into local or regional solutions, teaching students to explore how well models or equations work in particular settings or to answer specific problems using real data is essential. This article introduces an open web-based module developed to advance data-driven hydrologic process learning, targeting upper level undergraduate and early graduate students in hydrology and engineering. The module was developed and deployed on the hydrolearn open educational platform, which provides a formal pedagogical structure for developing effective problem-based learning activities. We found that data-driven learning activities utilizing collaborative open web platforms like cuahsi hydroshare and jupyterhub to store and run computational notebooks allowed students to access and work with datasets for systems of personal interest and promoted critical evaluation of results and assumptions. Initial student feedback was generally positive, but also highlighted challenges including trouble-shooting and future-proofing difficulties and some resistance to programming and new software. Opportunities to further enhance hydrology learning include better articulating the benefits of coding and open web platforms upfront, incorporating additional user-support tools, and focusing methods and questions on implementing and adapting notebooks to explore fundamental processes rather than tools and syntax. The profound shift in the field of hydrology toward big data, open data services and reproducible research practices requires hydrology instructors to rethink traditional content delivery and focus instruction on harnessing these datasets and practices in the preparation of future hydrologists and engineers. © 2021 the authors. Hydrological processes published by john wiley & sons ltd.","","","2021","10.1002/hyp.14273","","","scopus-2-s2.0-85111375087.pdf","scopus-2-s2.0-85111375087"
"Reproducible Research Practices and Transparency across the Biomedical Literature","Iqbal S. A., Wallach J. D., Khoury M. J., Schully S. D., Ioannidis J. P. A.","PLoS Biology","","There is a growing movement to encourage reproducibility and transparency practices in the scientific community including public access to raw data and protocols the conduct of replication studies systematic integration of evidence in systematic reviews and the documentation of funding and potential conflicts of interest. In this survey we assessed the current status of reproducibility and transparency addressing these indicators in a random sample of 441 biomedical journal articles published in 2000-2014. Only one study provided a full protocol and none made all raw data directly available. Replication studies were rare (n = 4) and only 16 studies had their data included in a subsequent systematic review or meta-analysis. The majority of studies did not mention anything about funding or conflicts of interest. The percentage of articles with no statement of conflict decreased substantially between 2000 and 2014 (94.4% in 2000 to 34.6% in 2014); the percentage of articles reporting statements of conflicts (0% in 2000 15.4% in 2014) or no conflicts (5.6% in 2000 50.0% in 2014) increased. Articles published in journals in the clinical medicine category versus other fields were almost twice as likely to not include any information on funding and to have private funding. This study provides baseline data to compare future progress in improving these indicators in the scientific literature. Copyright © 2016 Public Library of Science. All Rights Reserved.","","","2016","10.1371/journal.pbio.1002333","","","medline-26726926.pdf","medline-26726926"
"Water, water, everywhere: defining and assessing data sharing in academia","Van Tuyl S. And Whitmire A.l.","Plos One","","Sharing of research data has begun to gain traction in many areas of the sciences in the past few years because of changing expectations from the scientific community, funding agencies, and academic journals. National science foundation (nsf) requirements for a data management plan (dmp) went into effect in 2011, with the intent of facilitating the dissemination and sharing of research results. Many projects that were funded during 2011 and 2012 should now have implemented the elements of the data management plans required for their grant proposals. In this paper we define 'data sharing' and present a protocol for assessing whether data have been shared and how effective the sharing was. We then evaluate the data sharing practices of researchers funded by the nsf at oregon state university in two ways: by attempting to discover project-level research data using the associated dmp as a starting point, and by examining data sharing associated with journal articles that acknowledge nsf support. Sharing at both the project level and the journal article level was not carried out in the majority of cases, and when sharing was accomplished, the shared data were often of questionable usability due to access, documentation, and formatting issues. We close the article by offering recommendations for how data producers, journal publishers, data repositories, and funding agencies can facilitate the process of sharing data in a meaningful way.copyright © 2016 van tuyl, whitmire. This is an open access article distributed under the terms of the creative commons attribution license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","","","2016","10.1371/journal.pone.0147942","","","embase-608829276.pdf","embase-608829276"
"How to protect the credibility of articles published in predatory journals","Yamada, Y.","Publications","","Predatory journals often prey on innocent researchers who are unaware of the threat they pose. This paper discusses what researchers can do if they unintentionally publish a paper in a predatory journal, including measures to take before submission, during peer review, and after the journal has accepted a manuscript. The specific recommendations discussed are pre-registration, pre-submission peer-review, open peer-review, topping up reviewers, post-publication peer review, open recommendation, and treatment as unrefereed. These measures may help to ensure the credibility of the article, even if it is published in a predatory journal. The present article suggests that an open and multi-layered assessment of research content enhances the credibility of all research articles, even those published in non-predatory journals. If applied consistently by researchers in various fields, the suggested measures may enhance reproducibility and promote the advancement of science. © 2021 by the authors.","","","2021","10.3390/publications9010004","","","scopus-2-s2.0-85100340503.pdf","scopus-2-s2.0-85100340503"
"Safe sharing sites","Austin, L.m. And Lie, D.","New York University Law Review","","In this article we argue that data sharing is an activity that sits at the crossroads of privacy concerns and the broader challenges of data governance surrounding access and use. Using the sidewalk toronto “smart city” proposal as a starting point for discussion, we outline these concerns to include resistance to data monopolies, public control over data collected through the use of public infrastructure, public benefit from the generation of intellectual property, the desire to broadly share data for innovation in the public interest, social—rather than individual— surveillance and harms, and that data use be held to standards of fairness, justice, and accountability. Data sharing is sometimes the practice that generates these concerns and sometimes the practice that is involved in the solution to these concerns. Our safe sharing site approach to data sharing focuses on resolving key risks associated with data sharing, including protecting the privacy and security of data subjects, but aims to do so in a manner that is independent of the various legal contexts of regulation and governance. Instead, we propose that safe sharing sites connect with these different contexts through a legal interface consisting of a registry that provides transparency in relation to key information that supports different forms of regulation. Safe sharing sites could also offer assurances and auditability regarding the data sharing, further supporting a range of regulatory interventions. It is therefore not an alternative to these interventions but an important tool that can enable effective regulation. A central feature of a safe sharing site is that it offers an alternative to the strategy of de-identifying data and then releasing it, whether within an “open data” context or in a more controlled environment. In a safe sharing site, computations may be performed on the data in a secure and privacy-protective manner without releasing the raw data, and all data sharing is transparent and auditable. Transparency does not mean that all data sharing becomes a matter of “public” view, but rather that there is the ability to make these activities visible to organizations and regulators in appropriate circumstances while recognizing the potential confidentiality interests in data uses. In this way, safe sharing sites facilitate data sharing in a manner that manages the complexities of sharing while reducing the risks and enabling a variety of forms of governance and regulation. As such, the safe sharing site offers a flexible and modular piece of legal-technical infrastructure for the new economy. © 2019 by lisa m. Austin & david lie.","","","2019","","","","scopus-2-s2.0-85074029232.pdf","scopus-2-s2.0-85074029232"
"Frequent exposure to varied home cage sizes alters pain sensitivity and some key inflammation-related biomarkers","Oyewole, A.l. And Oyafemi, K.o. And Badmus, K.s. And Omoleye, J.o. And Abubakar, M.f. And Adeniyi-Raheem, O. And Amedu, A.-H. And Lawal, D.l. And Ijiyode, A.o. And Yussuf, A.o. And Ishola, S.s. And Sulaimon, F.a. And Alli-Oluwafuyi, A.o. And Nafiu, A.b. And Akinola, O. And Olajide, O.j. And Amin, A. And Abdulmajeed, W.i. And Michael, O.s. And Adeyanju, O.a. And Ogunjimi, G.l.","Journal Of Neuroscience Methods","","Background: nature and size of rodent cages vary from one laboratory or country to another. Little is however known about the physiological implications of exposure to diverse cage sizes in animal-based experiments. Method: here, two groups of male swiss mice (control group – cage stationed, and test group – cage migrated) were used for this study. The cage-migrated mice were exposed daily to various cage sizes used across laboratories in nigeria while the cage-stationed mice exposed daily to different but the same cage size and shape. At the end of the 30 days exposure, top-rated paradigms were used to profile changes in physiological behaviours, and this was followed by evaluation of histological and biochemical metrics. Results: the study showed a significant (p < 0.05) decrease in blood glucose levels (at 60 and 120 min of oral glucose tolerance test) in the cage-migrated mice compared to cage-stationed mice. Strikingly, peripheral oxidative stress (plasma malondialdehyde) and pain sensitivity (formalin test, hot-and-cold plate test, and von frey test) decreased significantly in cage-migrated mice compared to cage-stationed animals. Also, the pro-inflammation mediators (il-6 and nf-κb) increased significantly in cage-migrated mice compared to cage-stationed mice. However, emotion-linked behaviours, neurotransmitters (serotonin, noradrenaline and gaba), brain and plasma electrolytes were not significantly difference in cage-migrated animals compared to cage-stationed mice. Conclusion: taken together, these results suggest that varied size cage-to-cage exposure of experimental mice could affect targeted behavioural and biomolecular parameters of pain and inflammation, thus diminishing research reproducibility, precipitating false negative/positive results and leading to poor translational outcomes. © 2020 elsevier b.v.","","","2020","10.1016/j.jneumeth.2020.108890","","","scopus-2-s2.0-85089415533.pdf","scopus-2-s2.0-85089415533"
"Data as social capital and the gift culture in research","Klump, J.","Data Science Journal","","The value of making research data available is broadly accepted. Policies concerning the open access to research data try to implement new norms calling for researchers to make their data more openly available. These policies either appeal to the common good or focus on publication and citation as an incentive to bring about a cultural change in how researchers share their data with their peers. But when we compare the total number of publications in the fields of science, technology and medicine with the number data publications from the same time period, the number of openly available datasets is rather small. This indicates that current policies on data sharing are not effective in changing behaviours and bringing about the wanted cultural change. By looking at research communities that are more open to data sharing we can study the social patterns that influence data sharing and point us to possible points for intervention and change. © 2017 the author(s).","","","2017","10.5334/dsj-2017-014","","","scopus-2-s2.0-85024364121.pdf","scopus-2-s2.0-85024364121"
"Alaska native people's perceptions understandings and expectations for research involving biological specimens","Hiratsuka V. Y., Brown J. K., Hoeft T. J., Dillard D. A.","International Journal of Circumpolar Health","","OBJECTIVES: Members of racially and ethnically diverse groups have been persistently underrepresented in biomedical research in general possibly due to mistrust with the medical and research community. This article describes the perceptions understandings and expectations of Alaska Native people about research involving the collection and storage of biological specimens.\\\\\\\\rSTUDY DESIGN: Stratified focus groups.\\\\\\\\rMETHODS: Twenty-nine focus groups with Alaska Native people (n = 178) were held in 14 locations using a semi-structured moderator guide. ATLAS.ti was used for thematic analysis through iterative readings and coding. Alaska Native peoples' perceptions understandings and expectations of researcher beneficence informed consent processes and provision of research findings were elicited.\\\\\\\\rRESULTS AND CONCLUSIONS: Alaska Native people desired extensive disclosure of information beyond that typically provided in consent and results dissemination processes. Information germane to the motivation and intent of researchers and specifics of specimen storage and destruction were specifically requested. A clear and extensive process of informed consent and continued improvements in sharing results may enhance the transparency of research intent conduct and use of obtained results among Alaska Native people. Meeting expectations may improve relationships between researchers and the Alaska Native population which could result in increased research participation. Our findings offer a guide for researchers and communities when planning and implementing research with biological specimens.","","","2012","10.3402/ijch.v71i0.18642","","","medline-22663942.pdf","medline-22663942"
"Vpl-based big data analysis system: udas","Choi, H. And Gim, J. And Seo, Y.-D. And Baik, D.-K.","Ieee Access","","Over the past five years, research on big data analysis has been actively conducted, and many services have been developed to find valuable data. However, low quality of raw data and data loss problem during data analysis make it difficult to perform accurate data analysis. With the enormous generation of both unstructured and structured data, refinement of data is becoming increasingly difficult. As a result, data refinement plays an important role in data analysis. In addition, as part of efforts to ensure research reproducibility, the importance of reuse of researcher data and research methods is increasing;  however, the research on systems supporting such roles has not been conducted sufficiently. Therefore, in this paper, we propose a big data analysis system named the unified data analytics suite (udas) that focuses on data refinement. Udas performs data refinement based on the big data platform and ensures the reusability and reproducibility of refinement and analysis through the visual programming language interface. It also recommends open source and visualization libraries to users for statistical analysis. The qualitative evaluation of udas using the functional evaluation factor of the big data analysis platform demonstrated that the average satisfaction of the users is significantly high. © 2013 ieee.","","","2018","10.1109/access.2018.2857845","","","scopus-2-s2.0-85050407551.pdf","scopus-2-s2.0-85050407551"
"Perspectives on Open Science and Scholarly Publishing: a Survey Study Focusing on Early Career Researchers in Europe","Berezko O., Medina L. M. P., Malaguarnera G., Almeida I., Zyra A., Seang S., Bjornmalm M., Hnatkova E., Tata M.","F1000Research","","Background: The value of Open Science (OS) for the academic community and society has been becoming more evident recently especially during the COVID-19 pandemic. Nevertheless significant challenges regarding its implementation arise that are likely to affect researchers especially those in early career stages. Hence monitoring early-career researchers' views knowledge and skills on OS and related policies is crucial for its advancement. The main aim of this exploratory study was to gain new perspectives regarding the awareness of and attitudes towards OS and related practices having in consideration geographical economic and research career variables. Method(s): The survey was conducted during May-August 2020 as part of a collaboration between Eurodoc and the Open Research Europe project. The data from the survey were analyzed by European region Gross domestic product Gross domestic expenditure on research and development as a percentage of gross domestic product field of study and career stage. Result(s): The awareness and positive attitude regarding OS specifically among early-career researchers is high in Europe. However there are significant career stage group differences in views and knowledge about OS. Generally awareness and positive attitude tend to increase with increasing career seniority. Regarding European regions we spotted three main groups sharing similar awareness levels and attitudes: researchers in Western Europe - the most informed group towards OS; researchers in northern central and southern Europe - a moderately informed group with some minor differences; and researchers in eastern Europe - the least informed group whose opinions deviate the most. Conclusion(s): We found that there is an ""evolution of needs and focus"" regarding scientific publishing: researchers in most European regions are in different stages of transition from the competitive to collaborative levels while researchers in eastern Europe are largely beginning their transition to the competitive level. Copyright: © 2021 Berezko O et al.","","","2021","10.12688/f1000research.74831.1","","","unknown-1154.pdf","unknown-1154"
"Data trusted sharing delivery: a blockchain-assisted software-defined content delivery network","Shao, S. And Gong, W. And Yang, H. And Guo, S. And Chen, L. And Xiong, A.","Ieee Internet Of Things Journal","","The 6g wireless network aims to forge a new spectrum, high technical standards of high time and phase synchronization accuracy, and 100% geographic coverage to connect trillions of devices flexibly and efficiently in the future. However, as connectivity increases and applications become novel, it is a challenge to ensure the privacy and security of networks and applications. Blockchain is seen as a promising technology that can improve efficiency, reduce costs, mitigate security, and privacy threats, and establish a trusted data-sharing environment. This article presents a trusted framework based on blockchain technology from the perspective of how to build a trusted software-defined content delivery network. As the peer node of the blockchain, the software-defined network (sdn) controller establishes trust between different regions and a wide range of participants, realizing peer autonomy and flexible business orchestration. The two main purposes of the architecture are to enhance the security of network communications and establish trust relationships between entities in different domains. It includes trusted communication based on routing sandbox, service choreography based on blockchain, proxy server selection strategy based on model predictive control (mpc), and optimization consensus based on practical byzantine fault tolerance. Some simulation experiments verify the effectiveness of the theoretical method. © 2014 ieee.","","","2023","10.1109/jiot.2021.3124091","","","scopus-2-s2.0-85118570315.pdf","scopus-2-s2.0-85118570315"
"Consequences of common data analysis inaccuracies in CNS trauma injury basic research","Burke D. A., Whittemore S. R., Magnuson D. S.","Journal of Neurotrauma","","The development of successful treatments for humans after traumatic brain or spinal cord injuries (TBI and SCI respectively) requires animal research. This effort can be hampered when promising experimental results cannot be replicated because of incorrect data analysis procedures. To identify and hopefully avoid these errors in future studies the articles in seven journals with the highest number of basic science central nervous system TBI and SCI animal research studies published in 2010 (N=125 articles) were reviewed for their data analysis procedures. After identifying the most common statistical errors the implications of those findings were demonstrated by reanalyzing previously published data from our laboratories using the identified inappropriate statistical procedures then comparing the two sets of results. Overall 70% of the articles contained at least one type of inappropriate statistical procedure. The highest percentage involved incorrect post hoc t-tests (56.4%) followed by inappropriate parametric statistics (analysis of variance and t-test; 37.6%). Repeated Measures analysis was inappropriately missing in 52.0% of all articles and among those with behavioral assessments 58% were analyzed incorrectly. Reanalysis of our published data using the most common inappropriate statistical procedures resulted in a 14.1% average increase in significant effects compared to the original results. Specifically an increase of 15.5% occurred with Independent t-tests and 11.1% after incorrect post hoc t-tests. Utilizing proper statistical procedures can allow more-definitive conclusions facilitate replicability of research results and enable more accurate translation of those results to the clinic.","","","2013","10.1089/neu.2012.2704","","","medline-23186206.pdf","medline-23186206"
"But what do participants want? Comment on the ""data sharing in psychology"" special section (2018)","Cummings J.a. And Day T.e.","Am Psychol","","This commentary addresses a recent special section on data sharing (i.e., open data) in the february-march 2018 american psychologist. In 4 articles, the authors outline how open data can positively impact psychology and provide guidelines for adopting open data practices, which we believe is to be commended. However, this special issue has not acknowledged a crucial concern in the open data debate: the views and desires of participants. Participants are the backbone of psychological research and an important stakeholder in open data issues. We review research that has studied participants' opinions of open data and outline concerns regarding open data raised by some groups of participants. We conclude with recommendations, including a call to psychological researchers to move beyond opinion and instead to empirically examine the impact of open data. We believe psychology is a discipline uniquely poised to execute these recommendations and guide researchers' understandings of how to appropriately and ethically implement open data practices across multiple disciplines. (Psycinfo database record (c) 2019 apa, all rights reserved).","","","2019","10.1037/amp0000408","","","embase-626376832.pdf","embase-626376832"
"Reducing publication bias through trial registration","Abaid L. N., Grimes D. A., Schulz K. F.","Obstetrics & Gynecology","","Publication bias is the systematic preferential publication of studies with statistically significant positive results over indeterminate studies (frequently researchers inappropriately term these ""negative"" studies) or studies that show a statistically significant negative outcome. Over time this practice distorts the medical literature potentially compromising the validity of systematic reviews. Publication bias primarily stems from investigators but data suppression can occur by pharmaceutical companies universities and regulatory agencies. Registration at inception of all clinical trials in a centralized searchable database can reduce publication bias by enabling researchers to identify all studies related to a particular intervention. Prior attempts to encourage voluntary trial registration have been largely unsuccessful. Hence the International Committee of Medical Journal Editors recently adopted a policy of mandatory clinical trial registration before consideration of manuscripts for publication. Trial registration and the development of comprehensive computerized databases will promote transparency in research and help reduce publication bias.","","","2007","","","","medline-17540818.pdf","medline-17540818"
"NSF Anticipates Pushing Boundaries on Open-Access Plan","Basken Paul","Chronicle of Higher Education. Feb","","The National Science Foundation (NSF) in carrying out the Obama administration's new push for greater public access to research published in scientific journals will consider exclusivity periods shorter than the 12-month standard in the White House directive as well as trade-offs involving data-sharing and considerations of publishers' financial sustainability. The administration's directive announced on Friday after two years of deliberation asks agencies that sponsor research to impose a 12-month upper limit on how long journals can hold subscription-only rights to articles describing research that was financed with federal funds. The National Institutes of Health (NIH) adopted such a requirement almost five years ago and now all other federal agencies that spend at least $100-million a year on research and development are being given six months to draft a similar policy. The NIH announced this past November that it would soon begin enforcement by blocking the renewal of grant awards in cases where journal publications arising from the awards do not comply with its open-access rule. The NSF the largest provider of federal money for basic scientific research after the NIH will very likely follow the NIH in setting a 12-month period of exclusivity as its general rule. The White House science adviser John P. Holdren in announcing the new policy on Friday described an expansion of public access to federally financed research as important to economic growth. Scientific research supported by the federal government spurs scientific breakthroughs and economic advances when research results are made available to innovators. Demands for open-access research have generated years of heated debate involving publishers universities researchers and various advocacy groups. The NIH instituted its 12-month policy in April 2008 but only after strenuous objections from private publishing companies that fought back against an original proposal for six months. Congress has refused to pass a government-wide mandate despite several years of attempts by some lawmakers. And only a year ago the Obama administration appeared to have given up on the idea after a year of studying the question. In the end the plan outlined by Mr. Holdren does ""a very good job of balancing interests"" of libraries universities researchers and publishers. Industry representatives appeared to agree. In a statement issued Friday the Association of American Publishers said the new policy ""outlines a reasonable balanced resolution of issues around public access to research funded by federal agencies."" (ERIC)","","","2013","","","","unknown-1135.pdf","unknown-1135"
"Data flows during public health emergencies in LMICs: A people-centered mapping of data flows during the 2018 ebola epidemic in Equateur DRC","Abramowitz S., Stevens L. A., Kyomba G., Mayaka S., Grepin K. A.","Social Science & Medicine","","In infectious outbreaks rapid case detection and reporting coordination and context-specific strategies are needed for rapid containment. Data sharing between actors and the speed and content of data flows is essential for expediting epidemic response. In this study researchers mapped data flows during the 2018 Ebola Virus Disease (EVD) outbreak in Equateur Province in the Democratic Republic of the Congo using semi-structured interviews ethnographic research and focus groups with EVD response actors. During this research we mapped and tracked data collection transmission storage sharing and use patterns. Target participants included: key organizational actors in the EVD outbreaks responses including local (primary health community-based hospital) provincial (MoPH DRC Red Cross) and international (WHO UN organizations international first-responders) stakeholders. We found that a community-based surveillance system enabled the rapid detection of a hemorrhagic fever outbreak resulting in the rapid laboratory confirmation of EVD. With the arrival of international organizations to provide support to the EVD response routine surveillance systems continued to function robustly. However the establishment of a vertical EVD response architecture created challenges for the response. Data flows during the Equateur outbreak were hampered by numerous challenges in the domains of early warning line lists of cases and contact tracing which impeded surveillance and data flows. We therefore argue that structuring health information systems for preparedness requires taking a person-centered approach to data production flow and analysis. Copyright © 2022. Published by Elsevier Ltd.","","","2023","10.1016/j.socscimed.2022.115116","","","medline-36610244.pdf","medline-36610244"
"Reproducibility of Automated Breast Ultrasonography and Handheld Ultrasonography for Breast Lesion Size Measurement","Park K. W., Ko E. Y., Park S., Han B. K., Ko E. S., Choi J. S., Kwon M. R.","Ultrasound Quarterly","","ABSTRACT: The purpose of our study was to evaluate the reproducibility of size measurement of breast lesions using automated breast ultrasonography (ABUS) compared with that with handheld ultrasonography (HHUS). Three breast radiologists performed HHUS and measured the lesions size in 2 different phantoms: lesions with various shape size and same stiffness (phantom 1) and lesions with same shape size and various stiffness (phantom 2). After 1 month the same radiologists measured the lesion size of the same breast phantoms in the images obtained using ABUS. We evaluated interobserver variability between 3 radiologists in ABUS and HHUS and intraobserver variability of radiologists between ABUS and HHUS. Intraclass correlation coefficient (ICC) was used in statistical analysis. The measured size of lesions on HHUS was slightly larger than that on ABUS in both phantom 1 and 2 although not statistically significant (P = 0.314 P = 0.858). There were no significant differences in size measurements between the radiologists' measurements and the reference size in phantom 2 (P = 0.862). The ICCs for the interobserver agreement between the 3 radiologists were 0.98 to 0.99 on ABUS and 0.99 to 1.00 on HHUS respectively. The ICCs for the intraobserver agreement between ABUS and HHUS were 0.97 to 0.97 in phantom 1 and 0.98 to 0.99 in phantom 2. In conclusion ABUS showed excellent interobserver and intraobserver agreement with HHUS in measuring size of the lesions regardless of shape size and stiffness. Therefore ABUS mixed with HHUS can be used reliably in following up breast lesions size. Copyright © 2022 Wolters Kluwer Health Inc. All rights reserved.","","","2022","10.1097/ruq.0000000000000568","","","medline-35001027.pdf","medline-35001027"
"Shared care for diabetes: supporting communication between primary and secondary care","Branger P. J., van't Hooft A., van der Wouden J. C., Moorman P. W., van Bemmel J. H.","International Journal of Medical Informatics","","OBJECTIVE: To assess the effects on information exchange of electronic communication between physicians co-treating diabetic patients.\\\\\\\\rDESIGN: Comparison of traditional paper-based communication for reporting and electronic communication.\\\\\\\\rSETTING: General practitioners and an internal medicine outpatient clinic of an urban public hospital.\\\\\\\\rSUBJECTS: A total of 275 diabetic patients and the 32 general practitioners and one internal medicine consultant who cared for them.\\\\\\\\rINTERVENTION: An electronic communication network linking up the computer-based patient records of the physicians thus enabling electronic data interchange.\\\\\\\\rMAIN OUTCOME MEASURES: Number of letters sent and received per year by the general practitioners the number of diabetes-related parameters (e.g. results of laboratory tests) in the patient records and HBA1C levels.\\\\\\\\rRESULTS: INTERVENTION GPs received more messages per year (1.6 per patient) than control GPs (0.5 per patient P<0.05). Significant higher availability (P<0.05) was achieved for data on HBA1C levels fructosamine levels blood pressure measurements cholesterol levels triglyceride levels and weight measurements. INTERVENTION patients showed a slight but significant decrease of HBA1C levels in the second semester of 1994 (from 7.0 to 6.8 P = 0.03) control patients also showed a slightly decreased group mean but this change was not significant (from 6.6 to 6.5 P = 0.52). The magnitudes of these mean differences however were not significantly different (intervention group: 0.21; control group: 0.12 P = 0.68).\\\\\\\\rCONCLUSIONS: The electronic communication network for exchanging consultation outcomes significantly increased frequency of communication and the availability of data to the general practitioner on diagnostic procedures performed in the hospital thus providing more complete information about the care that patients are receiving. A large-scale experiment over a longer period of time is needed to assess the effects of improved communication on quality of care.","","","1999","10.1016/s1386-5056(98)00154-3","","","medline-10193883.pdf","medline-10193883"
"Rethinking success integrity and culture in research (part 2) - a multi-actor qualitative study on problems of science","Aubert Bonn N., Pinxten W.","Research Integrity & Peer Review","","BACKGROUND: Research misconduct and questionable research practices have been the subject of increasing attention in the past few years. But despite the rich body of research available few empirical works also include the perspectives of non-researcher stakeholders.\\\\\\\\rMETHODS: We conducted semi-structured interviews and focus groups with policy makers funders institution leaders editors or publishers research integrity office members research integrity community members laboratory technicians researchers research students and former-researchers who changed career to inquire on the topics of success integrity and responsibilities in science. We used the Flemish biomedical landscape as a baseline to be able to grasp the views of interacting and complementary actors in a system setting.\\\\\\\\rRESULTS: Given the breadth of our results we divided our findings in a two-paper series with the current paper focusing on the problems that affect the integrity and research culture. We first found that different actors have different perspectives on the problems that affect the integrity and culture of research. Problems were either linked to personalities and attitudes or to the climates in which researchers operate. Elements that were described as essential for success (in the associate paper) were often thought to accentuate the problems of research climates by disrupting research culture and research integrity. Even though all participants agreed that current research climates need to be addressed participants generally did not feel responsible nor capable of initiating change. Instead respondents revealed a circle of blame and mistrust between actor groups.\\\\\\\\rCONCLUSIONS: Our findings resonate with recent debates and extrapolate a few action points which might help advance the discussion. First the research integrity debate must revisit and tackle the way in which researchers are assessed. Second approaches to promote better science need to address the impact that research climates have on research integrity and research culture rather than to capitalize on individual researchers' compliance. Finally inter-actor dialogues and shared decision making must be given priority to ensure that the perspectives of the full research system are captured. Understanding the relations and interdependency between these perspectives is key to be able to address the problems of science.\\\\\\\\rSTUDY REGISTRATION: https://osf.io/33v3m.","","","2021","10.1186/s41073-020-00105-z","","","medline-33441167.pdf","medline-33441167"
"A semi-automated pipeline for fulfillment of resource requests from a longitudinal alzheimer's disease registry","Mckenzie K.a. And Hunt S.l. And Hulshof G. And Mudaranthakam D.p. And Meyer K. And Vidoni E.d. And Burns J.m. And Mahnken J.d.","Jamia Open","","Objective: managing registries with continual data collection poses challenges, such as following reproducible research protocols and guaranteeing data accessibility. The university of kansas (ku) alzheimer's disease center (adc) maintains one such registry: curated clinical cohort phenotypes and observations (c3po). We created an automated and reproducible process by which investigators have access to c3po data. Material(s) and method(s): data was input into research electronic data capture. Monthly, data part of the uniform data set (uds), that is data also collected at other adcs, was uploaded to the national alzheimer's coordinating center (nacc). Quarterly, nacc cleaned, curated, and returned the uds to the ku data management and statistics (dms) core, where it was stored in c3po with other quarterly curated site-specific data. Investigators seeking to utilize c3po submitted a research proposal and requested variables via the publicly accessible and searchable data dictionary. The dms core used this variable list and an automated sas program to create a subset of c3po. Result(s): c3po contained 1913 variables stored in 15 datasets. From 2017 to 2018, 38 data requests were completed for several ku departments and other research institutions. Completing data requests became more efficient;  c3po subsets were produced in under 10 seconds. Discussion(s): the data management strategy outlined above facilitated reproducible research practices, which is fundamental to the future of research as it allows replication and verification to occur. Conclusion(s): we created a transparent, automated, and efficient process of extracting subsets of data from a registry where data was changing daily.copyright © the author(s) 2019. Published by oxford university press on behalf of the american medical informatics association. This is an open access article distributed under the terms of the creative commons attribution non-commercial license (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.","","","2019","10.1093/jamiaopen/ooz032","","","embase-631253608.pdf","embase-631253608"
"Physician and stakeholder perceptions of conflict of interest policies in oncology","Lockhart A. C., Brose M. S., Kim E. S., Johnson D. H., Peppercorn J. M., Michels D. L., Storm C. D., Schuchter L. M., Rathmell W. K.","Journal of Clinical Oncology","","PURPOSE: The landscape of managing potential conflicts of interest (COIs) has evolved substantially across many disciplines in recent years but rarely are the issues more intertwined with financial and ethical implications than in the health care setting. Cancer care is a highly technologic arena with numerous physician-industry interactions. The American Society of Clinical Oncology (ASCO) recognizes the role of a professional organization to facilitate management of these interactions and the need for periodic review of its COI policy (Policy).\\\\\\\\rMETHODS: To gauge the sentiments of ASCO members and nonphysician stakeholders two surveys were performed. The first asked ASCO members to estimate opinions of the Policy as it relates to presentation of industry-sponsored research. Respondents were classified as consumers or producers of research material based on demographic responses. A similar survey solicited opinions of nonphysician stakeholders including patients with cancer survivors family members and advocates.\\\\\\\\rRESULTS: The ASCO survey was responded to by 1967 members (1% of those solicited); 80% were producers and 20% were consumers. Most respondents (93% of producers; 66% of consumers) reported familiarity with the Policy. Only a small proportion regularly evaluated COIs for presented research. Members favored increased transparency about relationships over restrictions on presentations of research. Stakeholders (n = 264) indicated that disclosure was ""very important"" to ""extremely important"" and preferred written disclosure (77%) over other methods.\\\\\\\\rCONCLUSION: COI policies are an important and relevant topic among physicians and patient advocates. Methods to simplify the disclosure process improve transparency and facilitate responsiveness are critical for COI management.","","","2013","10.1200/jco.2012.47.5475","","","medline-23530092.pdf","medline-23530092"
"Optimizing the leveraging of real-world data to improve the development and use of medicines","Berger, M.l. And Lipset, C. And Gutteridge, A. And Axelsen, K. And Subedi, P. And Madigan, D.","Value In Health","","Health research, including health outcomes and comparative effectiveness research, is on the cusp of a golden era of access to digitized real-world data, catalyzed by the adoption of electronic health records and the integration of clinical and biological information with other data. This era promises more robust insights into what works in health care. Several barriers, however, will need to be addressed if the full potential of these new data are fully realized;  these will involve both policy solutions and stakeholder cooperation. Although a number of these issues have been widely discussed, we focus on the one we believe is the most important - the facilitation of greater openness among public and private stakeholders to collaboration, connecting information and data sharing, with the goal of making robust and complete data accessible to all researchers. In this way, we can better understand the consequences of health care delivery, improve the effectiveness and efficiency of health care systems, and develop advancements in health technologies. Early real-world data initiatives illustrate both potential and the need for future progress, as well as the essential role of collaboration and data sharing. Health policies critical to progress will include those that promote open source data standards, expand access to the data, increase data capture and connectivity, and facilitate communication of findings. © 2015 international society for pharmacoeconomics and outcomes research (ispor).","","","2015","10.1016/j.jval.2014.10.009","","","scopus-2-s2.0-84920821005.pdf","scopus-2-s2.0-84920821005"
"The operations manual: a mechanism for improving the research process","Bowman A., Wyman J. F., Peters J.","Nursing Research","","BACKGROUND: The development and use of an operations manual has the potential to improve the capacity of nurse scientists to address the complex multifaceted issues associated with conducting research in today's healthcare environment. An operations manual facilitates communication standardizes training and evaluation and enhances the development and standard implementation of clear policies processes and protocols. A 10-year review of methodology articles in relevant nursing journals revealed no attention to this topic.\\\\\\\\rOBJECTIVES: This article will discuss how an operations manual can improve the conduct of research methods and outcomes for both small-scale and large-scale research studies. It also describes the purpose and components of a prototype operations manual for use in quantitative research.\\\\\\\\rCONCLUSION: The operations manual increases reliability and reproducibility of the research while improving the management of study processes. It can prevent costly and untimely delays or errors in the conduct of research.","","","2002","10.1097/00006199-200203000-00011","","","medline-11984385.pdf","medline-11984385"
"Reporting Research","Simera I., Altman D. G.","","","This chapter provides a brief overview of general principles of reporting medical research studies with a particular focus on the following study designs: randomised controlled trials analytical observational studies and systematic reviews and meta-analyses. Health-related research can be divided into two broad groups: experimental and observational. A typical example of experimental research design is a randomised controlled trial (RCT). Reporting guidelines provide structured advice on the minimum information to be included in an article reporting a particular type of medical research. There are three main types of observational design: cohort studies case-control studies and cross-sectional surveys. A minimum set of recommendations for reporting these studies is specified in the STROBE Statement. Similar to the CONSORT Statement. The Enhancing the Quality and Transparency of Health Research (EQUATOR) Network is an international initiative that aims to improve the reliability and value of the medical research literature by promoting transparent and accurate reporting of research studies. © 2016 John Wiley & Sons Ltd. All rights reserved.","","","2012","10.1002/9781118763025.ch39","","","scopus-2-s2.0-85028842848.pdf","scopus-2-s2.0-85028842848"
"Research fatigue in COVID-19 pandemic and post-disaster research: Causes consequences and recommendations","Patel S. S., Webster R. K., Greenberg N., Weston D., Brooks S. K.","Disaster Prevention & Management","","PURPOSE: Research fatigue occurs when an individual or population of interest tires of engaging with research consequently avoiding further participation. This paper considers research fatigue in the context of the current COVID-19 pandemic to identify contributory factors and possible solutions for future post-disaster research.\\\\\\\\rMETHODOLOGY: We draw on examples from the literature and our own observations from the recruitment and data collection phases of qualitative and quantitative studies to provide an overview of possible research fatigue in the current COVID-19 pandemic with implications for future post-disaster research.\\\\\\\\rFINDINGS: People affected by disasters sometimes receive multiple requests for study participation by separate teams who may not necessarily be coordinating their work. Not keeping participants informed of the research process or outcomes can lead to disillusionment. Being overburdened with too many research requests and failing to see any subsequent changes following participation may cause individuals to experience research fatigue.\\\\\\\\rORIGINALITY: Guidelines for researchers wishing to reduce the occurrence of research fatigue include ensuring greater transparency within research; sharing of results; and using oversight or gatekeeper bodies to aid coordination. Failure to restrict the number of times that people are asked to participate in studies risks poor participation rates. This can subsequently affect the quality of information with which to inform policy-makers and protect the health of the public during the COVID-19 pandemic or other public health disasters/emergencies.","","","2020","10.1108/dpm-05-2020-0164","","","medline-33679011.pdf","medline-33679011"
"Essential items for reporting of scaling studies of health interventions (succeed): protocol for a systematic review and delphi process","Gogovor, A. And Zomahoun, H.t.v. And Ben Charif, A. And Mclean, R.k.d. And Moher, D. And Milat, A. And Wolfenden, L. And Prévost, K. And Aubin, E. And Rochon, P. And Ekanmian, G. And Sawadogo, J. And Rheault, N. And Légaré, F.","Systematic Reviews","","Background: the lack of a reporting guideline for scaling of evidence-based practices (ebps) studies has prompted the registration of the standards for reporting studies assessing the impact of scaling strategies of ebps (succeed) with equator network. The development of succeed will be guided by the following main steps recommended for developing health research reporting guidelines. Methods: executive committee. We established a committee composed of members of the core research team and of an advisory group. Systematic review. The protocol was registered with the open science framework on 29 november 2019 (https://osf.io/vcwfx/). We will include reporting guidelines or other reports that may include items relevant to studies assessing the impact of scaling strategies. We will search the following electronic databases: embase, psycinfo, cochrane library, cinahl, web of science, from inception. In addition, we will systematically search websites of equator and other relevant organizations. Experts in the field of reporting guidelines will also be contacted. Study selection and data extraction will be conducted independently by two reviewers. A narrative analysis will be conducted to compile a list of items for the delphi exercise. Consensus process. We will invite panelists with expertise in: development of relevant reporting guidelines, methodologists, content experts, patient/member of the public, implementers, journal editors, and funders. We anticipated that three rounds of web-based delphi consensus will be needed for an acceptable degree of agreement. We will use a 9-point scale (1 = extremely irrelevant to 9 = extremely relevant). Participants' response will be categorized as irrelevant (1-3), equivocal (4-6) and relevant (7-9). For each item, the consensus is reached if at least 80% of the participants' votes fall within the same category. The list of items from the final round will be discussed at face-to-face consensus meeting. Guideline validation. Participants will be authors of scaling studies. We will collect quantitative (questionnaire) and qualitative (semi-structured interview) data. Descriptive analyses will be conducted on quantitative data and constant comparative techniques on qualitative data. Discussion: essential items for reporting scaling studies will contribute to better reporting of scaling studies and facilitate the transparency and scaling of evidence-based health interventions. © 2020 the author(s).","","","2020","10.1186/s13643-019-1258-3","","","scopus-2-s2.0-85077765873.pdf","scopus-2-s2.0-85077765873"
"From blurry space to a sharper sky: keeping twenty-three years of astronomical data alive","Boscoe, Bernadette Marie","Dissertation Abstracts International Section A: Humanities And Social Sciences","","In compute-heavy and data-driven scientific fields, digital data play a central role in the creation of knowledge. For science fields that rely on data that can only be observed once, the preservation of these data is crucial to analytic processes. Time-domain astronomy is one such field, involving celestial observations such as exploding supernovae and passing comets. This dissertation examines the data and code practices of a university-based astronomy research group who have managed to keep their data ""alive"" for twenty-three years with limited resources. Keeping data alive means both understanding the knowledge contained within and having the associated technologies operable. The human aspects necessary to keep data alive are equally as important as the technological elements. This three-year ethnographic research project examines the factors involved in analyzing, preserving, and curating astronomy data as these data wend their way through socio-technical, physical, and digital infrastructures that shape and are shaped by the knowledge contained within the data. Dissertation research took place at six field sites: the case study's university, the observatory of the case study, the archive of the observatory, and three other astronomy archives. The findings of this study show two main ways that data are kept alive in astronomy: 1) publicly: data are curated and preserved with the intention to be made available via web interfaces, engendering relationships among stakeholders, and 2) privately: data are preserved by astronomy research groups at universities that continually reuse them and their associated code, in a collaborative way. Findings suggest how a better understanding of the relationships between public and private astronomy data can inform scientific data preservation and curation practices. (Psycinfo database record (c) 2021 apa, all rights reserved)","","","2019","","","","psychinfo-2019-46352-235.pdf","psychinfo-2019-46352-235"
"Avoiding questionable research practices in applied psychology","O'donohue, William [Ed] And Masuda, Akihiko [Ed] And Lilienfeld, Scott [Ed]","","","This authoritative volume presents a detailed analysis of the replication crisis and the use of questionable research practices (qrps) in psychology, as well as recommended practices for combatting these problems. Ultimately, the book aims to provide a comprehensive, current, and accessible account of the adverse effects of qrps. The replication crisis in psychology and allied fields has exposed critical flaws in the standard views of research methods, which allow for extensive flexibility in data analysis by investigators and permit the widespread use of qrps. Chapters examine the intentional use of qrps such as data fabrication and falsification, along with subtler, unintentional practices such as p-hacking and harking (hypothesizing after results are known). Drawing on the growing awareness of these problems, contributors also highlight potential strategies to detect qrps and minimize their negative impact through open data practices, preregistration of hypotheses and analyses, and adversarial collaborations, in which investigators holding opposing positions on a scientific issue agree to work together on a study in an effort to counteract their respective biases. Among the topics covered: history of controversies in statistics and replication;  embracing intellectual humility while designing research;  confirmatory vs. exploratory analyses;  publication bias and negative results;  promoting honest and transparent report writing. Avoiding questionable research practices in applied psychology provides a deeper understanding of how qrps impede the reliability and trustworthiness of findings in psychology and the social sciences. It will be a practical, useful resource for students and instructors in graduate and advanced undergraduate level research methods classes, along with psychological researchers interested in improving their own research. (Psycinfo database record (c) 2023 apa, all rights reserved)","","","2022","10.1007/978-3-031-04968-2","","","psychinfo-2023-11526-000.pdf","psychinfo-2023-11526-000"
"The use of systematic reviews and reporting guidelines to advance the implementation of the 3Rs","Avey M. T., Fenwick N., Griffin G.","Journal of the American Association for Laboratory Animal Science: JAALAS","","In 1959 Russell and Burch published The Principles of Humane Experimental Technique which included concrete advice on factors that they considered would govern progress in the implementation of these principles (enunciated as the 3Rs [Replacement Reduction and Refinement in animal-based studies]). One challenge to the implementation of the 3Rs was identified as information retrieval. Here we further explore this challenge-the need for 'research on research'-and the role that systematic reviews and reporting guidelines can play in implementation of the 3Rs. First we examine the 2-fold nature of the challenge of information retrieval: 1) the identification of relevant publications spread throughout a large population of nonrelevant publications and 2) the incomplete reporting of relevant details within those publications. Second we evaluate how systematic reviews and reporting guidelines can be used generally to address this challenge. Third we assess the explicit reporting of the 3Rs in a cohort of preclinical animal systematic reviews. Our results show that Reduction methods are the most commonly reported by authors of systematic reviews but that in general reporting on how findings relate to the 3Rs is limited at best. Although systematic reviews are excellent tools for resolving the challenge of information retrieval their utility for making progress in implementation of the 3Rs may be limited unless authors improve their reporting of these principles.","","","2015","","","","medline-25836961.pdf","medline-25836961"
"A manifesto for reproducible science","Munafo M.r. And Nosek B.a. And Bishop D.v.m. And Button K.s. And Chambers C.d. And Du Sert N.p. And Simonsohn U. And Wagenmakers E.-J. And Ware J.j. And Ioannidis J.p.a.","Nat Hum Behav","","Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.","","","2017","10.1038/s41562-016-0021","","","embase-634978887.pdf","embase-634978887"
"Large, open datasets for human connectomics research: considerations for reproducible and responsible data use","Laird A.r.","Neuroimage","","Large, open datasets have emerged as important resources in the field of human connectomics. In this review, the evolution of data sharing involving magnetic resonance imaging is described. A summary of the challenges and progress in conducting reproducible data analyses is provided, including description of recent progress made in the development of community guidelines and recommendations, software and data management tools, and initiatives to enhance training and education. Finally, this review concludes with a discussion of ethical conduct relevant to analyses of large, open datasets and a researcher's responsibility to prevent further stigmatization of historically marginalized racial and ethnic groups. Moving forward, future work should include an enhanced emphasis on the social determinants of health, which may further contextualize findings among diverse population-based samples. Leveraging the progress to date and guided by interdisciplinary collaborations, the future of connectomics promises to be an impressive era of innovative research, yielding a more inclusive understanding of brain structure and function.copyright © 2021","","","2021","10.1016/j.neuroimage.2021.118579","","","embase-2014616454.pdf","embase-2014616454"
"Searching across-cohort relatives in 54092 GWAS samples via encrypted genotype regression","Zhang Q. X., Liu T., Guo X., Zhen J., Yang M. Y., Khederzadeh S., Zhou F., Han X., Zheng Q., Jia P., Ding X., He M., Zou X., Liao J. K., Zhang H., He J., Zhu X., Lu D., Chen H., Zeng C., Liu F., Zheng H. F., Liu S., Xu H. M., Chen G. B.","PLoS Genetics","","Explicitly sharing individual level data in genomics studies has many merits comparing to sharing summary statistics including more strict QCs common statistical analyses relative identification and improved statistical power in GWAS but it is hampered by privacy or ethical constraints. In this study we developed encG-reg a regression approach that can detect relatives of various degrees based on encrypted genomic data which is immune of ethical constraints. The encryption properties of encG-reg are based on the random matrix theory by masking the original genotypic matrix without sacrificing precision of individual-level genotype data. We established a connection between the dimension of a random matrix which masked genotype matrices and the required precision of a study for encrypted genotype data. encG-reg has false positive and false negative rates equivalent to sharing original individual level data and is computationally efficient when searching relatives. We split the UK Biobank into their respective centers and then encrypted the genotype data. We observed that the relatives estimated using encG-reg was equivalently accurate with the estimation by KING which is a widely used software but requires original genotype data. In a more complex application we launched a finely devised multi-center collaboration across 5 research institutes in China covering 9 cohorts of 54092 GWAS samples. encG-reg again identified true relatives existing across the cohorts with even different ethnic backgrounds and genotypic qualities. Our study clearly demonstrates that encrypted genomic data can be used for data sharing without loss of information or data sharing barrier. Copyright: © 2024 Zhang et al. This is an open access article distributed under the terms of the Creative Commons Attribution License which permits unrestricted use distribution and reproduction in any medium provided the original author and source are credited.","","","2024","10.1371/journal.pgen.1011037","","","medline-38206971.pdf","medline-38206971"
"Distinct antigen delivery systems induce dendritic cells' divergent transcriptional response: new insights from a comparative and reproducible computational analysis","Costa V. And Righelli D. And Russo F. And De Berardinis P. And Angelini C. And D'apice L.","Int. J. Mol. Sci","","Vaccination is the most successful and cost-effective method to prevent infectious diseases. However, many vaccine antigens have poor in vivo immunogenic potential and need adjuvants to enhance immune response. The application of systems biology to immunity and vaccinology has yielded crucial insights about how vaccines and adjuvants work. We have previously characterized two safe and powerful delivery systems derived from non-pathogenic prokaryotic organisms: e2 and fd filamentous bacteriophage systems. They elicit an in vivo immune response inducing cd8+ t-cell responses, even in absence of adjuvants or stimuli for dendritic cells' maturation. Nonetheless, a systematic and comparative analysis of the complex gene expression network underlying such activation is missing. Therefore, we compared the transcriptomes of ex vivo isolated bone marrow-derived dendritic cells exposed to these antigen delivery systems. Significant differences emerged, especially for genes involved in innate immunity, co-stimulation, and cytokine production. Results indicate that e2 drives polarization toward the th2 phenotype, mainly mediated by irf4, ccl17, and ccr4 over-expression. Conversely, fd-scalphadec-205 triggers th1 t cells' polarization through the induction of il12b, il12rb, il6, and other molecules involved in its signal transduction. The data analysis was performed using rnaseqgui, hence, addressing the increasing need of transparency and reproducibility of computational analysis.copyright © 2017 by the authors. Licensee mdpi, basel, switzerland.","","","2017","10.3390/ijms18030494","","","embase-614569174.pdf","embase-614569174"
"Biomedical conferences' author instructions rarely mention guidelines for reporting abstracts of trials and systematic reviews","Saric L., Dosenovic S., Mihanovic J., Puljak L.","Journal of Comparative Effectiveness Research","","<b>Aim:</b> To analyze whether instructions for authors of biomedical conference abstracts mention guidelines for writing randomized controlled trial and systematic review abstracts and to evaluate reasons for their absence from instructions. <b>Materials & methods:</b> We analyzed instructions for authors of biomedical conferences advertized in 2019 and assessed whether they mentioned Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Abstracts and Consolidated Standards of Reporting Trials for Abstracts guidelines. We surveyed contact persons from abstract/publication committees of selected conferences to analyze why relevant guidelines were missing.","","","2020","10.2217/cer-2019-0158","","","pubmed-31950848.pdf","pubmed-31950848"
"An integrated framework for de-identifying unstructured medical data","Gardner, J. And Xiong, L.","Data And Knowledge Engineering","","While there is an increasing need to share medical information for public health research, such data sharing must preserve patient privacy without disclosing any information that can be used to identify a patient. A considerable amount of research in data privacy community has been devoted to formalizing the notion of identifiability and developing techniques for anonymization but are focused exclusively on structured data. On the other hand, efforts on de-identifying medical text documents in medical informatics community rely on simple identifier removal or grouping techniques without taking advantage of the research developments in the data privacy community. This paper attempts to fill the above gaps and presents a framework and prototype system for de-identifying health information including both structured and unstructured data. We empirically study a simple bayesian classifier, a bayesian classifier with a sampling based technique, and a conditional random field based classifier for extracting identifying attributes from unstructured data. We deploy a k-anonymization based technique for de-identifying the extracted data to preserve maximum data utility. We present a set of preliminary evaluations showing the effectiveness of our approach. © 2009 elsevier b.v. All rights reserved.","","","2009","10.1016/j.datak.2009.07.006","","","scopus-2-s2.0-71749103414.pdf","scopus-2-s2.0-71749103414"
"Forecast evaluation with shared data sets","Sullivan, R. And Timmermann, A. And White, H.","International Journal Of Forecasting","","Data sharing is common practice in forecasting experiments in situations where fresh data samples are difficult or expensive to generate. This means that forecasters often analyze the same data set using a host of different models and sets of explanatory variables. This practice introduces statistical dependencies across forecasting studies that can severely distort statistical inference. Here we examine a new and inexpensive recursive bootstrap procedure that allows forecasters to account explicitly for these dependencies. The procedure allows forecasters to merge empirical evidence and draw inference in the light of previously accumulated results. In an empirical example, we merge results from predictions of daily stock prices based on (1) technical trading rules and (2) calendar rules, demonstrating both the significance of problems arising from data sharing and the simplicity of accounting for data sharing using these new methods. © 2001 international institute of forecasters. Published by elsevier science b.v. All rights reserved.","","","2003","10.1016/s0169-2070(01)00140-6","","","scopus-2-s2.0-1842680450.pdf","scopus-2-s2.0-1842680450"
"How Reproducible Research Leads to Non-Rote Learning within Socially Constructivist Statistics Education","Wessa Patrick","Electronic Journal of e-Learning","","This paper discusses the implementation of a new e-learning environment that supports non-rote learning of exploratory and inductive statistics within the pedagogical paradigm of social constructivism. The e-learning system is based on a new computational framework that allows us to create an electronic research environment where students are empowered to interact with reproducible computations from peers and the educator. The underlying technology effectively supports social interaction (communication) knowledge construction collaboration and scientific experimentation even if the student population is very large. In addition the system allows us to measure important aspects of the actual learning process which are otherwise unobservable. With this new information it is possible to explore (and investigate) the effectiveness of e-based learning the impact of software usability and the importance of knowledge construction through various feedback and communication mechanisms. Based on a preliminary empirical analysis from two courses (with large student populations) it is shown that there are strong relationships between actual constructivist learning activities and scores on objective examinations in which the questions assess conceptual understanding. It is also explained that non-rote learning is supported by the fact that the system allows users to reproduce results and reuse them in derived research that can be easily communicated. (Contains 2 tables.)","","","2009","","","","eric-ej867115.pdf","eric-ej867115"
"Evaluation of open science for co-creation of social innovations: a conceptual framework","Maciuliene, M.","European Public And Social Innovation Review","","Open science is a rapidly expanding and diversifying field of social innovation with significant implications for and potential benefits to society, policy and various academic research areas. However, much is still unknown about the co-creation processes in open science and an overall conceptual framework which aids such understanding is missing. The article aims to address these limitations and identify the key dimensions of an ecosystem allowing co-creation in open science to unfold its social and economic impact. The research presented integrates the literature analysis on co-creation in multi-stakeholder ecosystems and suggest that three important dimensions have to be considered in evaluation of open science ecosystems: framework conditions, system conditions and outcomes. The proposed model was applied in qualitative analysis of thirty-three open science case studies. Based on the results of evaluation, it can be concluded that open science landscape is highly heterogenous, fragmented and not fully coordinated. The fragmentation appeared in all dimensions of evaluation. The outcomes of the research provide a first exploratory step in proposing innovative measures to determine the elements of co-creation practices within open science context. © 2022, sinnergiak social innovation. All rights reserved.","","","2022","10.31637/epsir.22-1.1","","","scopus-2-s2.0-85135198079.pdf","scopus-2-s2.0-85135198079"
"A health informatics transformation model based on intelligent cloud computing - exemplified by type 2 diabetes mellitus with related cardiovascular diseases","Lin H.-C. And Kuo Y.-C. And Liu M.-Y.","Comput. Methods Programs Biomed","","Background and objective: many studies regarding health analysis request structured datasets but the legacy resources provide scattered data. This study aims to establish a health informatics transformation model (hitm) based upon intelligent cloud computing with the self-developed analytics modules by open source technique. The model was exemplified by the open data of type 2 diabetes mellitus (dm2) with related cardiovascular diseases. Method(s): the apache-spark framework was employed to generate the infrastructure of the hitm, which enables the machine learning (ml) algorithms including random forest, multi-layer perceptron classifier, support vector machine, and naive bayes classifier as well as the regression analysis for intelligent cloud computing. The modeling applied the mimic-iii open database as an example to design the health informatics data warehouse, which embeds the pl/sql-based modules to extract the analytical data for the training processes. A coupling analysis flow can drive the ml modules to train the sample data and validate the results. Result(s): the four modes of cloud computation were compared to evaluate the feasibility of the cloud platform in accordance with its system performance for more than 11,500 datasets. Then, the modeling adaptability was validated by simulating the featured datasets of obesity and cardiovascular-related diseases for patients with dm2 and its complications. The results showed that the run-time efficiency of the platform performed in around one minute and the prediction accuracy of the featured datasets reached 90%. Conclusion(s): this study helped contribute the modeling for efficient transformation of health informatics. The hitm can be customized for the actual clinical database, which provides big data for training, with the proper ml modules for a predictable process in the cloud platform. The feedback of intelligent computing can be referred to risk assessment in health promotion.copyright © 2020","","","2020","10.1016/j.cmpb.2020.105409","","","embase-2005113198.pdf","embase-2005113198"
"The Use and Abuse of Transcranial Magnetic Stimulation to Modulate Corticospinal Excitability in Humans","Heroux M. E., Taylor J. L., Gandevia S. C.","PLoS ONE [Electronic Resource]","","The magnitude and direction of reported physiological effects induced using transcranial magnetic stimulation (TMS) to modulate human motor cortical excitability have proven difficult to replicate routinely. We conducted an online survey on the prevalence and possible causes of these reproducibility issues. A total of 153 researchers were identified via their publications and invited to complete an anonymous internet-based survey that asked about their experience trying to reproduce published findings for various TMS protocols. The prevalence of questionable research practices known to contribute to low reproducibility was also determined. We received 47 completed surveys from researchers with an average of 16.4 published papers (95% CI 10.8-22.0) that used TMS to modulate motor cortical excitability. Respondents also had a mean of 4.0 (2.5-5.7) relevant completed studies that would never be published. Across a range of TMS protocols 45-60% of respondents found similar results to those in the original publications; the other respondents were able to reproduce the original effects only sometimes or not at all. Only 20% of respondents used formal power calculations to determine study sample sizes. Others relied on previously published studies (25%) personal experience (24%) or flexible post-hoc criteria (41%). Approximately 44% of respondents knew researchers who engaged in questionable research practices (range 30-81%) yet only 18% admitted to engaging in them (range 6-38%) [corrected]. These practices included screening subjects to find those that respond in a desired way to a TMS protocol selectively reporting results and rejecting data based on a gut feeling. In a sample of 56 published papers that were inspected not a single questionable research practice was reported. Our survey revealed that approximately 50% of researchers are unable to reproduce published TMS effects. Researchers need to start increasing study sample size and eliminating--or at least reporting--questionable research practices in order to make the outcomes of TMS research reproducible.","","","2015","10.1371/journal.pone.0144151","","","medline-26629998.pdf","medline-26629998"
"Mindmap: establishing an integrated database infrastructure for research in ageing, mental well-being, and the urban environment","Beenackers, M.a. And Doiron, D. And Fortier, I. And Noordzij, J.m. And Reinhard, E. And Courtin, E. And Bobak, M. And Chaix, B. And Costa, G. And Dapp, U. And Diez Roux, A.v. And Huisman, M. And Grundy, E.m. And Krokstad, S. And Martikainen, P. And Raina, P. And Avendano, M. And Van Lenthe, F.j.","Bmc Public Health","","Background: urbanization and ageing have important implications for public mental health and well-being. Cities pose major challenges for older citizens, but also offer opportunities to develop, test, and implement policies, services, infrastructure, and interventions that promote mental well-being. The mindmap project aims to identify the opportunities and challenges posed by urban environmental characteristics for the promotion and management of mental well-being and cognitive function of older individuals. Methods: mindmap aims to achieve its research objectives by bringing together longitudinal studies from 11 countries covering over 35 cities linked to databases of area-level environmental exposures and social and urban policy indicators. The infrastructure supporting integration of this data will allow multiple mindmap investigators to safely and remotely co-analyse individual-level and area-level data. Individual-level data is derived from baseline and follow-up measurements of ten participating cohort studies and provides information on mental well-being outcomes, sociodemographic variables, health behaviour characteristics, social factors, measures of frailty, physical function indicators, and chronic conditions, as well as blood derived clinical biochemistry-based biomarkers and genetic biomarkers. Area-level information on physical environment characteristics (e.g. green spaces, transportation), socioeconomic and sociodemographic characteristics (e.g. neighbourhood income, residential segregation, residential density), and social environment characteristics (e.g. social cohesion, criminality) and national and urban social policies is derived from publically available sources such as geoportals and administrative databases. The linkage, harmonization, and analysis of data from different sources are being carried out using piloted tools to optimize the validity of the research results and transparency of the methodology. Discussion: mindmap is a novel research collaboration that is combining population-based cohort data with publicly available datasets not typically used for ageing and mental well-being research. Integration of various data sources and observational units into a single platform will help to explain the differences in ageing-related mental and cognitive disorders both within as well as between cities in europe, the us, canada, and russia and to assess the causal pathways and interactions between the urban environment and the individual determinants of mental well-being and cognitive ageing in older adults. © 2018 the author(s).","","","2018","10.1186/s12889-018-5031-7","","","scopus-2-s2.0-85040723839.pdf","scopus-2-s2.0-85040723839"
"Public and biobank participant attitudes toward genetic research participation and data sharing","Lemke A. A., Wolf W. A., Hebert-Beirne J., Smith M. E.","Public Health Genomics","","Research assessing attitudes toward consent processes for high-throughput genomic-wide technologies and widespread sharing of data is limited. In order to develop a better understanding of stakeholder views toward these issues this cross-sectional study assessed public and biorepository participant attitudes toward research participation and sharing of genetic research data. Forty-nine individuals participated in 6 focus groups; 28 in 3 public focus groups and 21 in 3 NUgene biorepository participant focus groups. In the public focus groups 75% of participants were women 75% had some college education or more 46% were African-American and 29% were Hispanic. In the NUgene focus groups 67% of participants were women 95% had some college education or more and the majority (76%) of participants was Caucasian. Five major themes were identified in the focus group data: (a) a wide spectrum of understanding of genetic research; (b) pros and cons of participation in genetic research; (c) influence of credibility and trust of the research institution; (d) concerns about sharing genetic research data and need for transparency in the Policy for Sharing of Data in National Institutes of Health-Supported or Conducted Genome-Wide Association Studies; (e) a need for more information and education about genetic research. In order to increase public understanding and address potential concerns about genetic research future efforts should be aimed at involving the public in genetic research policy development and in identifying or developing appropriate educational strategies to meet the public's needs.","","","2010","10.1159/000276767","","","medline-20805700.pdf","medline-20805700"
"Dynamic proof of data possession and replication with tree sharing and batch verification in the cloud","Guo, W. And Qin, S. And Gao, F. And Zhang, H. And Li, W. And Jin, Z. And Wen, Q.","Ieee Transactions On Services Computing","","Cloud storage attracts a lot of clients to join the paradise. For a high data availability, some clients require their files to be replicated and stored on multiple servers. Because clients are generally charged based on the redundancy level required by them, it is critical for clients to obtain convincing evidence that all replicas are stored correctly and are updated to the up-to-date version. In this article, we propose a dynamic proof of data possession and replication (dpdpr) scheme, which is proved to be secure in the defined security model. Our scheme shares a single authenticated tree across multiple replicas, which reduces the tree's storage cost significantly. Our scheme allows for batch verification for multiple challenged leaves and can verify multiple replicas in a single batch way, which considerably save bandwidth and computation resources during audit process. We also evaluate the dpdpr's performance and compare it with the most related scheme. The evaluation results show that our scheme saves almost 66 percent tree's storage cost for three replicas, and obtains almost 60 and 80 percent efficiency improvements in terms of the overall bandwidth and computation costs, respectively, when three replicas are checked and each challenged with 460 blocks. © 2022 ieee.","","","2022","10.1109/tsc.2020.3022812","","","scopus-2-s2.0-85090986445.pdf","scopus-2-s2.0-85090986445"
"Propensity Scoring in Plastic Surgery Research: An Analysis and Best Practice Guide","Chu J. J., Shamsunder M. G., Yin S., Rubenstein R. R., Slutsky H., Fischer J. P., Nelson J. A.","Plastic and Reconstructive Surgery - Global Open","","Randomized controlled trials though considered the gold standard in clinical research are often not feasible in plastic surgery research. Instead researchers rely heavily on observational studies leading to potential issues with confounding and selection bias. Propensity scoring-a statistical technique that estimates a patient's likelihood of having received the exposure of interest-can improve the comparability of study groups by either guiding the selection of study participants or generating a covariate that can be adjusted for in multivariate analyses. In this study we conducted a comprehensive review of research articles published in three major plastic surgery journals (Plastic and Reconstructive Surgery Journal of Plastic Reconstructive & Aesthetic Surgery and Annals of Plastic Surgery) to determine the utilization of propensity scoring methods in plastic surgery research from August 2018 to August 2020. We found that propensity scoring was used in only eight (0.8%) of 971 research articles none of which fully reported all components of their propensity scoring methodology. We provide a brief overview of propensity score techniques and recommend guidelines for accurate reporting of propensity scoring methods for plastic surgery research. Improved understanding of propensity scoring may encourage plastic surgery researchers to incorporate the method in their own work and improve plastic surgeons' ability to understand and analyze future research studies that utilize propensity score methods. Copyright © 2022 The Authors. Published by Wolters Kluwer Health Inc. on behalf of The American Society of Plastic Surgeons.","","","2022","10.1097/gox.0000000000004003","","","medline-35169516.pdf","medline-35169516"
"Research on Intelligent Recommendation of Science and Technology Resource Data Based on Semantic Intelligence Analysis","Su H., Di J., Yin X., Li X.","","","Aiming at the practical problems such as imperfect semantic analysis function of recommendation service and low sharing degree among scientific data in the existing science and technology resources sharing the center of knowledge association algorithm with intelligence is built through the mutual reflection evidence among different data subjects such as talents enterprises platforms industries and scientific payoffs to realize multi-angle multi-dimensional and multi-association data portrait. Through business understanding data extraction data processing feature extraction model construction model deduction model application model evaluation and other mining steps the data mining algorithm center with intelligence is established. Based on knowledge graph an intelligent recommendation model for scientific data is proposed and we construct vector model bank through machine learning and realize intelligent recommendation and accurate pushing of scientific resources based on massive data of scientific research such as papers patents projects and scientific reports. The problems of data centralization and unification data systematization mapping data application convenience and data multiform and multi-scene application are effectively solved in the actual research work. By summarizing the theories technologies and methods related to data sharing and application of S&T resources this study aims to provide useful references for the wisdom upgrading of S&T resource sharing services and scientific and technical information service model innovation under the big data environment. © 2022 ACM.","","","2022","10.1145/3545801.3545806","","","scopus-2-s2.0-85138428111.pdf","scopus-2-s2.0-85138428111"
"Protocol for the development of a reporting guideline for clinical trials with integrated Chinese and western medicine interventions: the CONSORT extension for ICWM","Wang J., Zhang X., Wang P., Han F., Li J., Ma Y., Lyu A., Bian Z.","Frontiers in Medicine","","Background: While Integrated Chinese and Western Medicine (ICWM) has become widely accepted as a necessary intervention for treating various diseases key information about ICWM interventions is often missing in published clinical trials. To facilitate complete transparent and consistent reporting of clinical trials with ICWM interventions an extension of the CONSORT guideline is necessary to be developed: the CONSORT-ICWM guideline.\\\\\\\\rMethods: The CONSORT-ICWM guideline will be developed in five stages in accordance with recommendations for the development of reporting guidelines from the EQUATOR (Enhancing the QUAlity and Transparency Of health Research) Network including (1) project launch and registration; (2) literature review and checklist draft; (3) Delphi survey; (4) consensus meeting; and (5) finalization of the guideline. Additionally the working group will be composed of professors with expertise in integrated medicines traditional Chinese medicines biomedical informatics statistics methodology development of reporting guidelines epidemiology health economics and paper publications.\\\\\\\\rDiscussion: The CONSORT-ICWM guideline is to improve the reporting quality of clinical trials with ICWM interventions by ensuring the reports are complete informative clear and transparent. Copyright © 2023 Wang Zhang Wang Han Li Ma Lyu and Bian.","","","2023","10.3389/fmed.2023.1190560","","","medline-37457590.pdf","medline-37457590"
"No study is ever perfectly flawless: exploring research limitations in theses and dissertations of iranian higher education institutes","Shahriari, P. And Rasuli, B.","Iranian Journal Of Information Processing And Management","","Reporting research limitations in academic literature is very important for understanding and utilizing research findings. This study aims to assess the extent to which and how research limitations acknowledged in persian theses and dissertations (tds). 664 persian tds published from 2009 to 2018 selected randomly through cluster sampling technique from irandoc’s tds database and the last chapter of 578 tds analyzed. Through content analysis method, reported limitations analyzed and coded. Around 17% of tds (99/578) addressed at least one limitation in their reports, while 64% of them had a specific heading for this section. The trend of reporting limitations has increased during current decades and students in humanities areas have paid more attentions to this section. Overall, 22 key limitations have been repeated 268 times in the tds and “sampling bias” is more frequent with 29 repeats among them. Addressing research limitations in tds is very incomplete. Transparent addressing of research limitations may help readers and stakeholders understand and apply research findings more effectively. The role of educators and academic institutes to inform newcomer researchers about the importance and structure of research limitations section in their reports is vital. This is the first study to examine research limitations in persian academic literature and can inspire more research on this subject. The stakeholders of research, such as faculties, students, universities, research centers, readers/beneficiaries, and sponsors will be informed about key research limitations in tds. © 2020 iranian research institute for scientific information and documentation. All rights reserved.","","","2020","","","","scopus-2-s2.0-85095785293.pdf","scopus-2-s2.0-85095785293"
"Endorsement of guidelines for reporting economic evaluation studies by spanish biomedical journals","Catalá-López, F. And Ridao, M. And Bernal-Delgado, E. And Moher, D. And Repullo, J.r.","Gaceta Sanitaria","","Objective: to examine the endorsement of reporting guidelines for economic evaluation studies, such as the cheers (consolidated health economic evaluation reporting standards) statement, by spanish biomedical journals. Method: cross-sectional analysis of the instructions to authors of spanish biomedical journals included in the journal citation reports 2017. Two authors examined and extracted the following information: mention of any reporting guideline, the cheers statement, the recommendations of the international committee of medical journal directors (icmje) and the enhancing the quality and transparency of health research (equator) network. Results: of the 28 journals included, 23 (82.1%;  95% confidence interval [95%ci]: 63.1-93.9%) mentioned at least one reporting guideline in the instructions to authors. Only one journal mentioned the cheers statement for health economic evaluations. Twenty-four journals (85.7%;  95%ci: 67.3-96.0%) mentioned the icmje recommendations and 8 (28.6%;  95%ci: 13.2-48.7%) mentioned the equator network. The consort (consolidated standards of reporting trials) statement for clinical trials was the most- mentioned reporting guideline (n = 21;  75.0%;  95%ci: 55.1-89.3%). Discussion: most of the instructions to authors do not provide guidance on how to report economic evaluations. Journals should support compliance with reporting guidelines by authors and peer-reviewers. © 2019 sespas","","","2019","10.1016/j.gaceta.2018.12.006","","","scopus-2-s2.0-85062721470.pdf","scopus-2-s2.0-85062721470"
"Researchers' Perspectives Regarding Ethical Issues of Biobank Research in the Arab Region","Ibrahim M. E., Adarmouch L., Elgamri A., Abd ElHafeez S., Mohammed Z., Abdelgawad F., Elsebaie E. H., Abdelhafiz A. S., Gamel E., El Rhazi K., Abdelnaby A., Ahram M., Silverman H.","Biopreservation and Biobanking","","Background: The recent expansion of genomic biobank research in the Arab region in the Middle East North Africa has raised complex ethical and regulatory issues. However there is a lack of studies regarding the views of Arab researchers involved in such research. We aimed to assess the perceptions and attitudes of Arab researchers regarding these issues in biobank research. Methods: We developed a questionnaire to assess the perceptions and attitudes regarding genetic research of researchers from Egypt Sudan Morocco and Jordan. The questionnaire requested demographic data perceptions and attitudes regarding the collection storage and use of biospecimens and data the use of broad consent data security data sharing and community engagement. We used multiple linear regressions to identify predictors of perceptions and attitudes. Results: We recruited 383 researchers. Researchers favored equally the use of broad and tiered consent (44.1% and 39.1% respectively). Most respondents agreed with the importance of confidentiality protections to ensure data security (91.8%). However lower percentages were seen regarding the importance of community engagement (64.5%) data sharing with national colleagues and international partners (60.9% and 41.1% respectively) and biospecimen sharing with national colleagues and international partners (59.9% and 36.2% respectively). Investigators were evenly split on whether the return of individual research results should depend on the availability or not of a medical intervention that can be offered to address the genetic anomaly (47.5% and 46.4% respectively). Predictors of attitudes toward biospecimen research included serving on Research Ethics Committees prior research ethics training and affiliation with nonacademic institutions. Conclusions: We recommend further exploratory research with researchers regarding the importance of community engagement and to address their concerns about data sharing with researchers within and outside their countries.","","","2023","10.1089/bio.2022.0112","","","medline-36951637.pdf","medline-36951637"
"Evaluation of the Completeness of Interventions Reported in Published Randomized Controlled Trials in Plastic Surgery: A Systematic Review","Evans S., Rauh S., Jellison S., Diener B., Agha R., Vassar M.","Aesthetic Surgery Journal","","BACKGROUND: With the increasing number of randomized control trials being conducted and published in plastic surgery complete reporting of trial information is critical for readers to properly evaluate a trial's methodology and arrive at appropriate conclusions about its merits and applicability to patients. The Template for Intervention Description and Replication (TIDieR) checklist was introduced to address the limited guidance for reporting trial interventions.\\\\\\\\rOBJECTIVES: The authors applied the TIDieR checklist to evaluate the completeness of intervention reporting of randomized control trials in plastic surgery compare the quality of intervention reporting before and after the guideline was published and evaluate characteristics associated with TIDieR compliance.\\\\\\\\rMETHODS: A PubMed search identified 1 cohort published prior to the release of TIDieR and another published after its release. From the final sample the TIDieR checklist was applied to intervention descriptions and relevant study characteristics were extracted in a duplicate blinded manner.\\\\\\\\rRESULTS: In total 130 trials were included for analysis. The mean TIDieR score was 6.4 of 12. Five items were reported 90% of the time and 4 items were reported less than 10% of the time. We found that TIDieR publication did not affect intervention reporting (P = 0.22).\\\\\\\\rCONCLUSIONS: Our study identified areas in which intervention reporting could be improved. The extent of TIDieR adoption by trialists appears to be limited and greater efforts are needed to disseminate this reporting guideline if widespread uptake is to be expected. Alternately it may be beneficial to incorporate TIDieR into the more widely recognized Consolidated Standards of Reporting Trials statement. Copyright © 2020 The Aesthetic Society. Reprints and permission: journals.permissions@oup.com.","","","2021","10.1093/asj/sjaa166","","","medline-32530461.pdf","medline-32530461"
"Data trust framework using blockchain technology and adaptive transaction validation","Rouhani, S. And Deters, R.","Ieee Access","","Trust is the main barrier preventing widespread data sharing. The lack of transparent infrastructures for implementing data trust prevents many data owners from sharing their data and concerns data users regarding the quality of the shared data. Data trust is a paradigm that facilitates data sharing by forcing data users to be transparent about the process of sharing and reusing data. Blockchain technology proposes a distributed and transparent administration by employing multiple parties to maintain consensus on an immutable ledger. This paper presents an end-to-end framework for data trust to enhance trustworthy data sharing utilizing blockchain technology. The framework promotes data quality by assessing input data sets, effectively manages access control, and presents data provenance and activity monitoring. We introduce an assessment model that includes reputation, endorsement, and confidence factors to evaluate data quality. We also suggest an adaptive solution to determine the number of transaction validators based on the computed trust value. The proposed data trust framework addresses both data owners' and data users' concerns by ensuring the trustworthiness and quality of the data at origin and ethical and secure usage of the data at the end. A comprehensive experimental study indicates the presented system effectively handles a large number of transactions with low latency. © 2013 ieee.","","","2021","10.1109/access.2021.3091327","","","scopus-2-s2.0-85117585209.pdf","scopus-2-s2.0-85117585209"
"Evaluating the replicability and specificity of evidence for natural pedagogy theory","Silverstein, Priya","Dissertation Abstracts International: Section B: The Sciences And Engineering","","Do infants understand that they are being communicated to? This thesis first outlines issues facing the field of infancy research that affect confidence in the literature on this (and any) topic to date. Following this, an introductory chapter evaluates evidence for the three core claims of natural pedagogy (np), and the compatibility of this evidence with alternative theories. This is followed by three experimental chapters. In study 1, we attempted two replications of the study with the highest theoretical value for np (yoon et al., 2008). This study has high stakes theoretically, as it is the only study providing evidence for the most specific claim of np that is difficult to explain by low-level mechanisms. Therefore, a replication of this result that included a reduction of possible confounds and a more sophisticated measure of attention throughout the task was of great theoretical value. In this study, we were unable to replicate the original findings. In study 2 we went beyond the evidence for the claims made in the outline of np, and instead generated a new, specific prediction that we believe np would make. This is important, as theories are only useful if they can make clear, testable predictions. In this study, we pitted pedagogically demonstrated actions and simple actions against each other and evaluated infants' transmission of these actions to someone else. We found no evidence for np, finding evidence for preferential transmission of simple actions instead. In study 3 we went beyond np, and tested a clear prediction stemming from an alternative low-level theory for how infants develop gaze-following ability. We found evidence that infants learn to gaze-follow through reinforcement. Overall, this thesis contributes to the vast literature on infants as recipients of communication, as well as highlighting methods for conducting open and reproducible infancy research. (Psycinfo database record (c) 2021 apa, all rights reserved)","","","2021","","","","psychinfo-2020-97496-004.pdf","psychinfo-2020-97496-004"
"Standards for reporting interventions in clinical trials of cupping (strictoc): extending the consort statement","Zhang, X. And Tian, R. And Lam, W.c. And Duan, Y. And Liu, F. And Zhao, C. And Wu, T. And Shang, H. And Tang, X. And Lyu, A. And Bian, Z.","Chinese Medicine (United Kingdom)","","Background: the standards for reporting interventions in clinical trials of cupping (strictoc), in the form of a checklist and explanations for users, were designed to improve reporting of cupping trials, particularly the interventions, and thereby facilitating their interpretation and replication. Methods: a group of clinical experts, methodologists, epidemiologists, and editors has developed this strictoc checklist through a comprehensive process, including registration of this guideline, literature review, solicitation of comments, consensus meeting, revision, and finalization. Results: the strictoc checklist includes 6 items and 16 sub-items, namely cupping rationale, details of cupping, treatment regimen, other components of treatment, treatment provider background, and control or comparator interventions. Illustrative examples of each item are also provided. Conclusions: it is intended that the strictoc, in conjunction with both the main consolidated standards of reporting trials (consort) statement and extension for nonpharmacologic treatment, will raise the reporting quality of clinical trials of cupping. Trial registration we have registered this study on the enhancing the quality and transparency of health research (equator) network: http://www.equator-network.org/library/reporting-guidelines-under-development/reporting-guidelines-under-development-for-clinical-trials/#strictoc. © 2020 the author(s).","","","2020","10.1186/s13020-020-0293-2","","","scopus-2-s2.0-85079064952.pdf","scopus-2-s2.0-85079064952"
"A Bayesian statistics tutorial for clinical research: Prior distributions and meaningful results for small clinical samples","Larson C., Kaplan D., Girolamo T., Kover S. T., Eigsti I. M.","Journal of Clinical Psychology","","OBJECTIVES: Bayesian statistics provides an effective reliable approach for research with small clinical samples and yields clinically meaningful results that can bridge research and practice. This tutorial demonstrates how Bayesian statistics can be effectively and reliably implemented with a small heterogeneous participant sample to promote reproducible and clinically relevant research.\\\\\\\\rMETHODS/RESULTS: We tested example research questions pertaining to language and clinical features in autism spectrum disorder (ASD; n = 20) a condition characterized by significant heterogeneity. We provide step-by-step instructions and visualizations detailing how to (1) identify and develop prior distributions from the literature base (2) evaluate model convergence and reliability and (3) compare models with different prior distributions to select the best performing model. Moreover in step three we demonstrate how to determine whether a sample size is sufficient for reliably interpreting model results. We also provide instructions detailing how to examine results with varied bounds of clinical interest such as the probability that an effect will reflect at least one standard deviation change in scores on a standardized assessment. This information facilitates generalization and application of Bayesian results to a variety of clinical research questions and settings.\\\\\\\\rCONCLUSION: The tutorial concludes with suggestions for future clinical research ensuring the utility of our step-by-step instructions for a broad clinical audience. Copyright © 2023 Wiley Periodicals LLC.","","","2023","10.1002/jclp.23570","","","medline-37477577.pdf","medline-37477577"
"Plan quality evaluation 1994–2012: growth and contributions, limitations, and new directions","Lyles, W. And Stevens, M.","Journal Of Planning Education And Research","","During the last twenty years, more than forty-five publications have sought to measure and evaluate the quality of plans using content analysis methods. We examine reasons for this growth in the literature and its contributions and limitations. We also examine whether the research methods described in these publications conform to recommended practices in the methodological literature on content analysis to determine whether plan quality researchers are likely to be generating reliable and reproducible plan quality data. We provide seven recommendations plan quality researchers can follow to address these weaknesses and improve the reliability and reproducibility of their data. © the author(s) 2014.","","","2014","10.1177/0739456x14549752","","","scopus-2-s2.0-84910663145.pdf","scopus-2-s2.0-84910663145"
"Assessing Open Science practices in physical activity behaviour change intervention evaluations","Norris E., Sulevani I., Finnerty A. N., Castro O.","BMJ Open Sport & Exercise Medicine","","Objectives: Concerns on the lack of reproducibility and transparency in science have led to a range of research practice reforms broadly referred to as 'Open Science'. The extent that physical activity interventions are embedding Open Science practices is currently unknown. In this study we randomly sampled 100 reports of recent physical activity randomised controlled trial behaviour change interventions to estimate the prevalence of Open Science practices.\\\\\\\\rMethods: One hundred reports of randomised controlled trial physical activity behaviour change interventions published between 2018 and 2021 were identified as used within the Human Behaviour-Change Project. Open Science practices were coded in identified reports including: study pre-registration protocol sharing data materials and analysis scripts sharing replication of a previous study open access publication funding sources and conflict of interest statements. Coding was performed by two independent researchers with inter-rater reliability calculated using Krippendorff's alpha.\\\\\\\\rResults: 78 of the 100 reports provided details of study pre-registration and 41% provided evidence of a published protocol. 4% provided accessible open data 8% provided open materials and 1% provided open analysis scripts. 73% of reports were published as open access and no studies were described as replication attempts. 93% of reports declared their sources of funding and 88% provided conflicts of interest statements. A Krippendorff's alpha of 0.73 was obtained across all coding.\\\\\\\\rConclusion: Open data materials analysis and replication attempts are currently rare in physical activity behaviour change intervention reports whereas funding source and conflict of interest declarations are common. Future physical activity research should increase the reproducibility of their methods and results by incorporating more Open Science practices. Copyright © Author(s) (or their employer(s)) 2022. Re-use permitted under CC BY. Published by BMJ.","","","2022","10.1136/bmjsem-2021-001282","","","medline-35722044.pdf","medline-35722044"
"Blockchain-enabled immutable distributed and highly available clinical research activity logging system for federated COVID-19 data analysis from multiple institutions","Kuo T. T., Pham A., Edelson M. E., Kim J., Chan J., Gupta Y., Ohno-Machado L.","Journal of the American Medical Informatics Association","","OBJECTIVE: We aimed to develop a distributed immutable and highly available cross-cloud blockchain system to facilitate federated data analysis activities among multiple institutions.\\\\\\\\rMATERIALS AND METHODS: We preprocessed 9166 COVID-19 Structured Query Language (SQL) code summary statistics and user activity logs from the GitHub repository of the Reliable Response Data Discovery for COVID-19 (R2D2) Consortium. The repository collected local summary statistics from participating institutions and aggregated the global result to a COVID-19-related clinical query previously posted by clinicians on a website. We developed both on-chain and off-chain components to store/query these activity logs and their associated queries/results on a blockchain for immutability transparency and high availability of research communication. We measured run-time efficiency of contract deployment network transactions and confirmed the accuracy of recorded logs compared to a centralized baseline solution.\\\\\\\\rRESULTS: The smart contract deployment took 4.5 s on an average. The time to record an activity log on blockchain was slightly over 2 s versus 5-9 s for baseline. For querying each query took on an average less than 0.4 s on blockchain versus around 2.1 s for baseline.\\\\\\\\rDISCUSSION: The low deployment recording and querying times confirm the feasibility of our cross-cloud blockchain-based federated data analysis system. We have yet to evaluate the system on a larger network with multiple nodes per cloud to consider how to accommodate a surge in activities and to investigate methods to lower querying time as the blockchain grows.\\\\\\\\rCONCLUSION: Blockchain technology can be used to support federated data analysis among multiple institutions. Copyright © The Author(s) 2023. Published by Oxford University Press on behalf of the American Medical Informatics Association.","","","2023","10.1093/jamia/ocad049","","","medline-36916740.pdf","medline-36916740"
"Electronic Lab Notebooks and Experimental Design Assistants","Gerlach B., Untucht C., Stefan A.","Handbook of Experimental Pharmacology","","Documentation of experiments is essential for best research practice and ensures scientific transparency and data integrity. Traditionally the paper lab notebook (pLN) has been employed for documentation of experimental procedures but over the course of the last decades the introduction of electronic tools has changed the research landscape and the way that work is performed. Nowadays almost all data acquisition analysis presentation and archiving are done with electronic tools. The use of electronic tools provides many new possibilities as well as challenges particularly with respect to documentation and data quality. One of the biggest hurdles is the management of data on different devices with a substantial amount of metadata. Transparency and integrity have to be ensured and must be reflected in documentation within LNs. With this in mind electronic LNs (eLN) were introduced to make documentation of experiments more straightforward with the development of enhanced functionality leading gradually to their more widespread use. This chapter gives a general overview of eLNs in the scientific environment with a focus on the advantages of supporting quality and transparency of the research. It provides guidance on adopting an eLN and gives an example on how to set up unique Study-IDs in labs in order to maintain and enhance best practices. Overall the chapter highlights the central role of eLNs in supporting the documentation and reproducibility of experiments. Copyright © The Author(s) 2019.","","","2020","10.1007/164_2019_287","","","medline-31541321.pdf","medline-31541321"
"Review of Research Reporting Guidelines for Radiology Researchers","Cronin P., Rawson J. V.","Academic Radiology","","Prior articles have reviewed reporting guidelines and study evaluation tools for clinical research. However only some of the many available accepted reporting guidelines at the Enhancing the QUAlity and Transparency Of health Research Network have been discussed in previous reports. In this paper we review the key Enhancing the QUAlity and Transparency Of health Research reporting guidelines that have not been previously discussed. The study types include diagnostic and prognostic studies reliability and agreement studies observational studies analytical and descriptive experimental studies quality improvement studies qualitative research health informatics systematic reviews and meta-analyses economic evaluations and mixed methods studies. There are also sections on study protocols and statistical analyses and methods. In each section there is a brief overview of the study type and then the reporting guideline(s) that are most applicable to radiology researchers including radiologists involved in health services research are discussed.","","","2016","10.1016/j.acra.2016.01.004","","","medline-26928069.pdf","medline-26928069"
"Stability of scientific big data sharing mechanism based on two-way principal-agent","Zhang, W. And Zhang, J.","Aims Mathematics","","In the era of big data, facing the data-intensive scientific paradigm shift and the explosion of scientific big data, there is an urgent need for alliance cooperation between heterogeneous research groups to actively open and share scientific big data to support china's economic development, technological innovation and national security. Therefore, the study of scientific big data sharing mechanism has very important practical significance. We think science big data sharing is an ecosystem that is constantly evolving to higher-order ecological evolution. Based on the dual perspectives of psychological contract and contractual contract, the scientific big data sharing strategy evolution mechanism and sharing strategy incentive mechanism are explored .The research finds that the cooperation of scientific research groups is bound by psychological contract and contractual contract;  stochastic evolutionary game has stronger explanatory power for sharing strategy evolution, complementarity is positive indicator, random interference and moral risk are negative indicators;  two-way principal agent can describe alliance members are mutually entrusted, and the shared strategy incentive contract consists of fixed wages and incentive wages, which are proportional to risk. © 2023 the author(s), licensee aims press.","","","2023","10.3934/math.2023955","","","scopus-2-s2.0-85161375955.pdf","scopus-2-s2.0-85161375955"
"Community expectations for research artifacts and evaluation processes","Hermann B., Winter S., Siegmund J., Devanbu P., Cohen M., Zimmermann T.","","","Background. Artifact evaluation has been introduced into the software engineering and programming languages research community with a pilot at ESEC/FSE 2011 and has since then enjoyed a healthy adoption throughout the conference landscape. Objective. In this qualitative study we examine the expectations of the community toward research artifacts and their evaluation processes. Method. We conducted a survey including all members of artifact evaluation committees of major conferences in the software engineering and programming language field since the first pilot and compared the answers to expectations set by calls for artifacts and reviewing guidelines. Results. While we find that some expectations exceed the ones expressed in calls and reviewing guidelines there is no consensus on quality thresholds for artifacts in general. We observe very specific quality expectations for specific artifact types for review and later usage but also a lack of their communication in calls. We also find problematic inconsistencies in the terminology used to express artifact evaluation's most important purpose - replicability. Conclusion. We derive several actionable suggestions which can help to mature artifact evaluation in the inspected community and also to aid its introduction into other communities in computer science. © 2020 ACM.","","","2020","10.1145/3368089.3409767","","","scopus-2-s2.0-85097193422.pdf","scopus-2-s2.0-85097193422"
"A generic methodology for the statistically uniform & comparable evaluation of automated trading platform components","Sokolovsky, A. And Arnaboldi, L.","Expert Systems With Applications","","Introduction: although machine learning approaches have been widely used in the field of finance, to very successful degrees, these approaches remain bespoke to specific investigations and opaque in terms of explainability, comparability, and reproducibility. Objectives: the primary objective of this research was to shed light upon this field by providing a generic methodology that was investigation-agnostic and interpretable to a financial markets’ practitioner, thus enhancing their efficiency, reducing barriers to entry, and increasing the reproducibility of experiments. The proposed methodology is showcased on two automated trading platform components. Namely, price levels, a well-known trading pattern, and a novel 2-step feature extraction method. Methods: this proposed a generic methodology, useable across markets, the methodology relies on hypothesis testing, which is widely applied in other social and scientific disciplines to effectively evaluate the concrete results beyond simple classification accuracy. The first hypothesis was formulated to evaluate whether the selected trading pattern is suitable for use in the machine learning setting. The second hypothesis allows us to systematically assess whether the proposed feature extraction method leads to any statistically significant improvement in the automated trading platform performance. Results: experiments were conducted across, 10 contracts, 3 feature spaces, and 3 rebound configurations (for feature extraction), resulting in 90 experiments. Across the experiments we found that the use of the considered trading pattern in the machine learning setting is only partially supported by statistics, resulting in insignificant effect sizes (rebound 7 - 0.64±1.02, rebound 11 0.38±0.98, and rebound 15 - 1.05±1.16), but allowed the rejection of the null hypothesis based on the outcome of the statistical test. While the results of the proposed 2-step feature extraction looked promising at first sight, statistics did not support this, this demonstrated the usefulness of the proposed methodology. Additionally, we obtained shap values for the considered models, providing insights for adjustments to the feature space. Conclusion: we showcased the generic methodology on a us futures market instrument and provided evidence that with this methodology we could easily obtain informative metrics beyond the more traditional performance and profitability metrics. The interpretability of these results allows the practitioner to construct more effective automated trading pipelines by analysing their strategies using an intuitive and statistically sound methodology. This work is one of the first in applying this rigorous statistically-backed approach to the field of financial markets and we hope this may be a springboard for more research. A full reproducibility package is shared. © 2023 the author(s)","","","2023","10.1016/j.eswa.2023.119836","","","scopus-2-s2.0-85150249736.pdf","scopus-2-s2.0-85150249736"
"The quality of reporting in clinical research: the consort and strobe initiatives","Bolignano, D. And Mattace-Raso, F. And Torino, C. And D'arrigo, G. And Elhafeez, S.a. And Provenzano, F. And Zoccali, C. And Tripepi, G.","Aging Clinical And Experimental Research","","Inaccurate reporting of data hampers the generalizability and the correct interpretation of results of scientific medical papers. The consolidated standards of reporting trials (consort) and strengthening the reporting of observational studies in epidemiology (strobe) initiatives, both included in the enhancing the quality and transparency of health research (equator) international network, have elaborated appropriate guidelines in order to improve the transparence, clearness and completeness of scientific literature. The consort statement consists of a 25 items checklist and a flow-chart diagram which provide guidance to authors on how to report randomized clinical trials. The strobe is a checklist of 22 items which should be addressed when observational studies (case-control, cohort and cross-sectional) are made up. Many editorial committees and prestigious international journals have now embraced these guidelines to improve the quality and methodology of their scientific reports. © 2013 springer international publishing switzerland.","","","2013","10.1007/s40520-013-0007-z","","","scopus-2-s2.0-84887026610.pdf","scopus-2-s2.0-84887026610"
"Observations on the uk transformational government strategy relative to citizen data sharing and privacy","Combe, C.","Transforming Government: People, Process And Policy","","Purpose - one of the key aims of the uk's transformational government strategy is to create a ""joined-up"" government where communications within and between public organisations is improved by the use of information technology. Data sharing is a key enabler of ""joined-up"" government but the implementation of the strategy presents a series of risks. The purpose of this paper is to articulate and assess the nature of those risks in relation to violations of existing laws using the national pupil database (npd) in england as a case study. Design/methodology/approach - the paper investigates examples of violations of eu law relating to rights to privacy of data sharing practices within the uk public sector using an interpretive approach to existing published information. The case of the npd illustrates how certain identified data sharing practices contravene existing laws and exposes this aspect of the transformational government strategy to heightened risk of a legal challenge. Findings - four examples of violations of existing eu laws on privacy are identified from an investigation into the npd for schools in england. The analysis exposes the imbalance between the data sharing practices underpinning the transformational government strategy in the uk and the requirements for fulfilling privacy protection rights to citizens enshrined in eu law. The findings reveal that data sharing practices as a key enabler of the transformational government strategy risks violating existing laws designed to protect privacy. The uk government risks a legal challenge, the outcome of which may seriously undermine the prospects for achieving the stated aim of improving efficiency and effectiveness across the public sector. Research limitations/implications - the paper is largely restricted to the npd for schools in england. The findings would be strengthened by expanding the research into other areas of the public sector where data sharing practices have been implemented. Originality/value - the findings are a significant and timely contribution to understanding the data sharing/privacy tension that ministers and legislators need to address. The work provides an insight into where weaknesses exist within current arrangements that is of value to policymakers, legislators, human rights advocates and government authorities at both central and local levels. © emerald group publishing limited.","","","2009","10.1108/17506160910997892","","","scopus-2-s2.0-70350362715.pdf","scopus-2-s2.0-70350362715"
"Challenges in the evaluation of observational data trustworthiness from a data producers viewpoint (fair+)","Koedel, U. And Schuetze, C. And Fischer, P. And Bussmann, I. And Sauer, P.k. And Nixdorf, E. And Kalbacher, T. And Wichert, V. And Rechid, D. And Bouwer, L.m. And Dietrich, P.","Frontiers In Environmental Science","","Recent discussions in many scientific disciplines stress the necessity of “fair” data. Fair data, however, does not necessarily include information on data trustworthiness, where trustworthiness comprises reliability, validity and provenience/provenance. This opens up the risk of misinterpreting scientific data, even though all criteria of “fair” are fulfilled. Especially applications such as secondary data processing, data blending, and joint interpretation or visualization efforts are affected. This paper intends to start a discussion in the scientific community about how to evaluate, describe, and implement trustworthiness in a standardized data evaluation approach and in its metadata description following the fair principles. It discusses exemplarily different assessment tools regarding soil moisture measurements, data processing and visualization and elaborates on which additional (metadata) information is required to increase the trustworthiness of data for secondary usage. Taking into account the perspectives of data collectors, providers and users, the authors identify three aspects of data trustworthiness that promote efficient data sharing: 1) trustworthiness of the measurement 2) trustworthiness of the data processing and 3) trustworthiness of the data integration and visualization. The paper should be seen as the basis for a community discussion on data trustworthiness for a scientifically correct secondary use of the data. We do not have the intention to replace existing procedures and do not claim completeness of reliable tools and approaches described. Our intention is to discuss several important aspects to assess data trustworthiness based on the data life cycle of soil moisture data as an example. Copyright © 2022 koedel, schuetze, fischer, bussmann, sauer, nixdorf, kalbacher, wichert, rechid, bouwer and dietrich.","","","2022","10.3389/fenvs.2021.772666","","","scopus-2-s2.0-85124087032.pdf","scopus-2-s2.0-85124087032"
"Game over: empower early career researchers to improve research quality","De Herde, V. And Björnmalm, M. And Susi, T.","Insights: The Uksg Journal","","Processes of research evaluation are coming under increasing scrutiny, with detractors arguing that they have adverse effects on research quality, and that they support a research culture of competition to the detriment of collaboration. Based on three personal perspectives, we consider how current systems of research evaluation lock early career researchers and their supervisors into practices that are deemed necessary to progress academic careers within the current evaluation frameworks. We reflect on the main areas in which changes would enable better research practices to evolve;  many align with open science. In particular, we suggest a systemic approach to research evaluation, taking into account its connections to the mechanisms of financial support for the institutions of research and higher education in the broader landscape. We call for more dialogue in the academic world around these issues and believe that empowering early career researchers is key to improving research quality. © 2021 ubiquity press. All rights reserved.","","","2021","10.1629/uksg.548","","","scopus-2-s2.0-85109964563.pdf","scopus-2-s2.0-85109964563"
"Validation and reproducibility of pressure-corrected aortic distensibility measurements using pulse-wave-velocity Doppler ultrasound","Lehmann E. D., Parker J. R., Hopkins K. D., Taylor M. G., Gosling R. G.","Journal of Biomedical Engineering","","A non-invasive Doppler ultrasound technique is described for the assessment of aortic compliance based on the in vivo measurement of pulse wave velocity along the thoraco-abdominal aortic pathway. A structured protocol which has been developed to improve the reproducibility of the technique is validated. A method of correcting for the effect of non-chronic changes in blood pressure on arterial elasticity is considered and applied to compliance measurements performed on 66 normal healthy volunteers. The results of a study to ascertain the overall reproducibility of the method are provided and problems associated with the technique are discussed. Medical disorders such as atherosclerosis diabetes mellitus familial hypercholesterolaemia and growth hormone deficiency have all been shown to affect arterial wall compliance. It is suggested that the in vivo measurement of pressure-corrected aortic distensibility may be a useful non-invasive tool for assessing such patients' susceptibility to atheromatous arterial disease and for monitoring their response to therapy. Measurements in the aorta may be especially pertinent since the natural history of fatty streaks there tends to parallel that in coronary arteries thereby potentially affording a convenient surrogate estimate of coronary heart disease.","","","1993","","","","medline-8320981.pdf","medline-8320981"
"Computational systems biology as an animal-free approach to characterize toxicological effects of persistent organic pollutants","Wu, Q. And Achebouche, R. And Audouze, K.","Altex","","Exposure to persistent organic pollutants (pops), as defined by the stockholm convention, may alter biological systems and cause toxic effects. Computational studies appear to be a relevant approach to increase our understanding of the molecular mechanisms triggered by pops. We investigated the use of a systems toxicology approach to explore the effects of pops on human health. A protein-protein association network (ppan) was developed based on known pop-protein interactions. This model was used to predict protein complexes for several candidate pops, including dicofol, methoxychlor, and perfluorooctanoic acid (pfoa), that are listed or proposed to be listed as pops by the stockholm convention. Integration of multiple data sources (pathways, disease annotations, adverse outcome pathways) involving the identified protein complexes was performed independently in order to reveal putative risk factors for human health. This approach revealed that several systems may be disturbed by these candidate pops, mainly the reproductive, metabolic and nervous systems. This study highlights that a computational systems toxicology approach may help to decipher putative biological mechanisms of poorly studied chemicals and link them to possible adverse effects with the aim to support regulatory assessment and trigger new epidemiological and experimental studies. In order to develop more accurate computational models as alternative methods to animal testing, the next challenge will be to integrate more data according to the findable, accessible, interoperable and reusable (fair) data principles. © the authors, 2020.","","","2020","10.14573/altex.1910161","","","scopus-2-s2.0-85082979276.pdf","scopus-2-s2.0-85082979276"
"Design and implementation of information tracing platform for crop whole industry chain based on csbft-blockchain","Ren, S. And He, Z. And Zhou, Z. And Gu, X. And Xiong, Y. And Yuan, P. And Xu, H.","Nongye Gongcheng Xuebao/Transactions Of The Chinese Society Of Agricultural Engineering","","The safety of agricultural products concerns people's health. To ensure food safety and accountability, it is crucial to establish a credible food traceability system. The blockchain technology can greatly improve the integrity, security and credibility of traceability information of traditional agricultural product traceability system, thanks to its properties such as decentralization, non-tamperability and information traceability. This paper proposes an improved blockchain consensus algorithm, based on the credit-supervisor byzantine fault tolerance (csbft). It mainly includes credit update strategy and supervisor node selection strategy. Various credit update strategies are formulated according to the node types and whether they can actively forward a message, and then the supervisor node selection strategies are made based on the credit of the node. The csbft algorithm uses the supervisor to monitor the behavior of the master node to prevent problems such as sending different messages to different nodes;  while the consensus mechanism of centralized nodes and distributed nodes can improve the supervisory node generating efficiency because the master node does not need to be generated cyclically and it can choose a more reliable node as a supervisory node according to the supervisory node selection strategy. To prove the effectiveness of csbft algorithm, the paper uses the common transfer transaction information as experimental data and the experiments were repeated ten times to compare the consensus delays of pbft (practical byzantine fault tolerance), mbft and csbft with the transaction numbers of 5, 10, 20 and 50 respectively. The average of 10 repeated experiments is used as the final statistical value and the result proves that csbft has higher robustness, lower consensus delay and higher safety. This paper studied and analyzed the information flow of the whole industrial chain of crops from agricultural product purchase, planting management, processing and production, logistics and transportation to grain sales, to build a blockchain alliance chain based on csbft. It analyzed, designed and realized the whole industrial chain information traceability platform based on embedded csbft, via object-oriented software engineering method. The platform can automatically save the key up-chain information through pre-designed smart contracts to generate corresponding traceability codes for consumers to query. Compared with the traditional food traceability system, the csbft algorithm endows the platform with higher security and less delay in information chaining. Future research on data privacy protection will be conducted. To protect the data privacy, users of various levels in the whole industrial chain hesitate to share their data. Therefore, how to collect and share the information without leaking the private data is the future research direction. © 2020, editorial department of the transactions of the chinese society of agricultural engineering. All right reserved.","","","2020","10.11975/j.issn.1002-6819.2020.03.034","","","scopus-2-s2.0-85083338430.pdf","scopus-2-s2.0-85083338430"
"Structural under-reporting of informed consent data handling and sharing ethical approval and application of Open Science principles as proxies for study quality conduct in COVID-19 research: a systematic scoping review","Wilmes N., Hendriks C. W. E., Viets C. T. A., Cornelissen Sjwm, van Mook Wnka, Cox-Brinkman J., Celi L. A., Martinez-Martin N., Gichoya J. W., Watkins C., Bakhshi-Raiez F., Wynants L., van der Horst I. C. C., van Bussel B. C. T.","BMJ Global Health","","BACKGROUND: The COVID-19 pandemic required science to provide answers rapidly to combat the outbreak. Hence the reproducibility and quality of conducting research may have been threatened particularly regarding privacy and data protection in varying ways around the globe. The objective was to investigate aspects of reporting informed consent and data handling as proxies for study quality conduct.\\\\\\\\rMETHODS: A systematic scoping review was performed by searching PubMed and Embase. The search was performed on November 8th 2020. Studies with hospitalised patients diagnosed with COVID-19 over 18 years old were eligible for inclusion. With a focus on informed consent data were extracted on the study design prestudy protocol registration ethical approval data anonymisation data sharing and data transfer as proxies for study quality. For reasons of comparison data regarding country income level study location and journal impact factor were also collected.\\\\\\\\rRESULTS: 972 studies were included. 21.3% of studies reported informed consent 42.6% reported waivers of consent 31.4% did not report consent information and 4.7% mentioned other types of consent. Informed consent reporting was highest in clinical trials (94.6%) and lowest in retrospective cohort studies (15.0%). The reporting of consent versus no consent did not differ significantly by journal impact factor (p=0.159). 16.8% of studies reported a prestudy protocol registration or design. Ethical approval was described in 90.9% of studies. Information on anonymisation was provided in 17.0% of studies. In 257 multicentre studies 1.2% reported on data sharing agreements and none reported on Findable Accessible Interoperable and Reusable data principles. 1.2% reported on open data. Consent was most often reported in the Middle East (42.4%) and least often in North America (4.7%). Only one report originated from a low-income country.\\\\\\\\rDISCUSSION: Informed consent and aspects of data handling and sharing were under-reported in publications concerning COVID-19 and differed between countries which strains study quality conduct when in dire need of answers. Copyright © Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.","","","2023","10.1136/bmjgh-2023-012007","","","medline-37257937.pdf","medline-37257937"
"Strengthen the process report of clinical trials promote full transparency of clinical trials. [Chinese]","Dong C., Yan X., Tian R., Bian Z., Yao C.","Chinese Journal of Evidence-Based Medicine","","The concept of clinical trial transparency has been promoted for more than 40 years. The act of clinical trial registration report guidelines development and data sharing has has been strongly pushed forward and become a common practice. The clinical trial process being the key procedure of trial operation and quality control determines the accuracy of the results. However the process report of clinical trials is insufficient. In this article we summarize the importance of clinical trial process report and provide corresponding suggestions. We propose that medical journals reporting guidelines developers and clinical trial registration platforms should work together to strengthen the process report of clinical trials and promote full transparency of clinical trials. Copyright © 2018 West China University of Medical Science. All rights reserved.","","","2018","10.7507/1672-2531.201806071","","","unknown-1414.pdf","unknown-1414"
"In data science, take nothing on faith","Kobielus, J.","Ibm Data Management Magazine","","Adopting a rigid regimen is required for selection of reproducible computational findings. There is a list of specific data and analytic governance rules for computational scientists to follow to ensure reproducibility. For every result, scientists must keep track of how it was produced. They should avoid manual data manipulation steps. Archive the exact versions of all external programs used. Version control all custom scripts. Computer scientists must record all intermediate results, when possible, in standardized formats. For analyses that include randomness, note underlying random seeds. They must always store raw data behind plots;  generate hierarchical analysis output, allowing layers of increasing detail to be inspected. They must also connect textual statements to underlying results and provide public access to scripts, runs, and results.","","","2014","","","","scopus-2-s2.0-84904314987.pdf","scopus-2-s2.0-84904314987"
"Randomized Clinical Trials of Machine Learning Interventions in Health Care: A Systematic Review","Plana D., Shung D. L., Grimshaw A. A., Saraf A., Sung J. J. Y., Kann B. H.","JAMA Network Open","","Importance: Despite the potential of machine learning to improve multiple aspects of patient care barriers to clinical adoption remain. Randomized clinical trials (RCTs) are often a prerequisite to large-scale clinical adoption of an intervention and important questions remain regarding how machine learning interventions are being incorporated into clinical trials in health care.\\\\\\\\rObjective: To systematically examine the design reporting standards risk of bias and inclusivity of RCTs for medical machine learning interventions.\\\\\\\\rEvidence Review: In this systematic review the Cochrane Library Google Scholar Ovid Embase Ovid MEDLINE PubMed Scopus and Web of Science Core Collection online databases were searched and citation chasing was done to find relevant articles published from the inception of each database to October 15 2021. Search terms for machine learning clinical decision-making and RCTs were used. Exclusion criteria included implementation of a non-RCT design absence of original data and evaluation of nonclinical interventions. Data were extracted from published articles. Trial characteristics including primary intervention demographics adherence to the CONSORT-AI reporting guideline and Cochrane risk of bias were analyzed.\\\\\\\\rFindings: Literature search yielded 19737 articles of which 41 RCTs involved a median of 294 participants (range 17-2488 participants). A total of 16 RCTS (39%) were published in 2021 21 (51%) were conducted at single sites and 15 (37%) involved endoscopy. No trials adhered to all CONSORT-AI standards. Common reasons for nonadherence were not assessing poor-quality or unavailable input data (38 trials [93%]) not analyzing performance errors (38 [93%]) and not including a statement regarding code or algorithm availability (37 [90%]). Overall risk of bias was high in 7 trials (17%). Of 11 trials (27%) that reported race and ethnicity data the median proportion of participants from underrepresented minority groups was 21% (range 0%-51%).\\\\\\\\rConclusions and Relevance: This systematic review found that despite the large number of medical machine learning-based algorithms in development few RCTs for these technologies have been conducted. Among published RCTs there was high variability in adherence to reporting standards and risk of bias and a lack of participants from underrepresented minority groups. These findings merit attention and should be considered in future RCT design and reporting.","","","2022","10.1001/jamanetworkopen.2022.33946","","","medline-36173632.pdf","medline-36173632"
"P-values - a chronic conundrum","Gao, J.","Bmc Medical Research Methodology","","Background: in medical research and practice, the p-value is arguably the most often used statistic and yet it is widely misconstrued as the probability of the type i error, which comes with serious consequences. This misunderstanding can greatly affect the reproducibility in research, treatment selection in medical practice, and model specification in empirical analyses. By using plain language and concrete examples, this paper is intended to elucidate the p-value confusion from its root, to explicate the difference between significance and hypothesis testing, to illuminate the consequences of the confusion, and to present a viable alternative to the conventional p-value. Main text: the confusion with p-values has plagued the research community and medical practitioners for decades. However, efforts to clarify it have been largely futile, in part, because intuitive yet mathematically rigorous educational materials are scarce. Additionally, the lack of a practical alternative to the p-value for guarding against randomness also plays a role. The p-value confusion is rooted in the misconception of significance and hypothesis testing. Most, including many statisticians, are unaware that p-values and significance testing formed by fisher are incomparable to the hypothesis testing paradigm created by neyman and pearson. And most otherwise great statistics textbooks tend to cobble the two paradigms together and make no effort to elucidate the subtle but fundamental differences between them. The p-value is a practical tool gauging the ""strength of evidence""against the null hypothesis. It informs investigators that a p-value of 0.001, for example, is stronger than 0.05. However, p-values produced in significance testing are not the probabilities of type i errors as commonly misconceived. For a p-value of 0.05, the chance a treatment does not work is not 5%;  rather, it is at least 28.9%. Conclusions: a long-overdue effort to understand p-values correctly is much needed. However, in medical research and practice, just banning significance testing and accepting uncertainty are not enough. Researchers, clinicians, and patients alike need to know the probability a treatment will or will not work. Thus, the calibrated p-values (the probability that a treatment does not work) should be reported in research papers. © 2020 the author(s).","","","2020","10.1186/s12874-020-01051-6","","","scopus-2-s2.0-85087097769.pdf","scopus-2-s2.0-85087097769"
"Standard quality criteria in retracted vs nonretracted obstetrical randomized controlled trials","Anderson K. M., Doulaveris G., Bennett C., Mol B. W., Berghella V.","American Journal of Obstetrics & Gynecology MFM","","BACKGROUND: The number of retracted articles in peer-reviewed journals is increasing within the field of obstetrics. The most common reason for article retraction is scientific misconduct. Unfortunately article retraction often occurs years after publication allowing inaccurate data to be widely distributed to readers. There exists a great need for validated screening criteria for obstetric journals to use when reviewing randomized controlled trials for scientific misconduct.\\\\\\\\rOBJECTIVE: This study aimed to compare retracted obstetric randomized controlled trials with nonretracted randomized controlled trials with regard to their inclusion of 7 quality metrics: prospective trial registration trial registration number ethics approval statement name of the approving committee statement of informed consent adherence to the Consolidated Standards of Reporting Trials guidelines and a data sharing statement.\\\\\\\\rSTUDY DESIGN: Obstetric randomized controlled trials retracted between 1995 and 2021 identified through Retraction Watch were compared with nonretracted randomized controlled trials published between 2018 and 2020 with regard to inclusion of the 7 quality metrics. The main outcome was the difference in prospective trial registration. Secondary outcomes were the percentage of individual criteria met and the screening performance of quality criteria in predicting article retraction.\\\\\\\\rRESULTS: A total of 150 randomized controlled trials were identified of which 14 (9.3%) were retracted and 136 (90.7%) nonretracted. Retracted randomized controlled trials were less likely than nonretracted randomized controlled trials to be prospectively registered (14.3% vs 80.1%; P<.001). The median number of quality criteria met was lower for retracted randomized controlled trials (3 vs 6; P<.01). Using a cutoff of <=4 criteria was associated with 85.7% (95% confidence interval 57.2-98.2) sensitivity and 92.0% (95% confidence interval 86.2-96.0) specificity in distinguishing the retracted randomized controlled trials from nonretracted studies.\\\\\\\\rCONCLUSION: Retracted obstetric randomized controlled trials were less likely to include the 7 quality metrics required on submission by most top obstetrics and gynecology journals. Copyright © 2023. Published by Elsevier Inc.","","","2023","10.1016/j.ajogmf.2023.100889","","","medline-36804302.pdf","medline-36804302"
"About the improvement of production and evaluation of scientific journals in brazil","Ponce, B.j. And De Almeida, M.e.b. And Freitas, S.a. And Da Silva, C.b. And Anjos, D. And De Pietri, E. And Prieto, R.g. And Dias, É.s.a.c. And Camargo, E. And Branco, J.c. And Souza, J.s. And Bizelli, J.l. And Siman, L.m.c. And Muzzeti, L.r. And Dos Reis, M. And Martins, E. And Rosito, M.m.b. And Bissoto, M.l. And De Castro, M.r. And Gimenes, N. And Gualtieri, R. And Silva, R. And Ribeiro, R. And Lemes, S.s.","Ensaio","","The document we presented is the result of the reflection of editors of scientific journals connected to fepae southeast and expresses concrete concerns about the material conditions for the realization of its work and about the evaluation criteria that have been used to qualify its journals. Cheaper budgets, dismantling of university structures that supported many academic journals and indicators which everytime complicate even more the operational procedures involved in editorial production, undermine the national journals. Lack of visibility about the evaluation procedures, absence of dialogue about results and questions connected to the qualis frequency bring uncertainty for the authors who have in the journals a vehicle for sharing its practices and theories. This way we want to deepen the discussion with the scientific community and the regulatory governmental agency - capes - about the destinations reserved to brazilian scientific journals.","","","2017","10.1590/s0104-40362017002501032","","","scopus-2-s2.0-85032014563.pdf","scopus-2-s2.0-85032014563"
"Reproducibility of arterial stiffness and wave reflections in chronic obstructive pulmonary disease: the contribution of lung hyperinflation and a comparison of techniques","Stone I. S., John L., Petersen S. E., Barnes N. C.","Respiratory Medicine","","Significant cardiovascular morbidity and mortality exists in chronic obstructive pulmonary disease (COPD). Arterial stiffness is raised in COPD and may be a mechanistic link. Non-invasive assessment of arterial stiffness has the potential to be a surrogate outcome measure although no reproducibility data exists in COPD patients. Two studies (23 and 33 COPD patients) were undertaken to 1) assess the Vicorder reproducibility of carotid-femoral pulse wave velocity and Augmentation index in COPD; 2) compare it to SphygmoCor; and 3) assess the contribution of lung hyperinflation to measurement variability. There were excellent correlations and good agreement between repeat Vicorder measurements for carotid-femoral pulse wave velocity (r = 0.96 (p < 0.001); mean difference +/-SD = -0.03 +/- 0.36 m/s (p = 0.65); co-efficient of reproducibility = 4.02%; limits of agreement = -0.68-0.75 m/s). Augmentation index significantly correlated (r = 0.736 (p < 0.001); mean difference +/-SD = 0.72 +/- 4.86% (p = 0.48) however limits of agreement were only 10.42-9.02% with co-efficient of reproducibility of 27.93%. Comparing devices Vicorder values were lower but there was satisfactory agreement. There were no correlation between lung hyperinflation (as measured by residual volume percent predicted total lung capacity percent predicted or the ratio of inspiratory capacity to residual volume) and variability of measurements in either study. In COPD measurement of carotid-femoral pulse wave velocity is highly reproducible not affected by lung hyperinflation and suitable as a surrogate endpoint in research studies. Day-to-day variation in augmentation index highlights the importance of such studies prior to the planning and undertaking of clinical COPD research. Copyright © 2013 Elsevier Ltd. All rights reserved.","","","2013","10.1016/j.rmed.2013.06.008","","","medline-23920329.pdf","medline-23920329"
"The future of interdisciplinary research in the digital era: obstacles and perspectives of collaboration in social and data sciences - an empirical study","Parti, K. And Szigeti, A.","Cogent Social Sciences","","In the last decade, a transition in research design and methodology is identified in social research methodology;  however, the high entry threshold (i.e., technical knowledge) to utilize computational methods and the ethical concerns seem to slow down the process. A possible way out is that social sciences collaborate with computational or data scientists in interdisciplinary research projects to rely on each other’s skills and to develop jointly accepted ethical principles. In this exploratory study, we collected data from researchers with a variety of academic backgrounds to find out their views of interdisciplinary projects and related methodological or ethical issues. Our findings derived from one-on-one interviews (n = 22) reinforce the importance of interdisciplinary collaboration and highlight the significance of “interpreters,” i.e., individuals able to communicate with and connect various areas of science, education, and academic institutions’ role in enhancing interdisciplinary collaborations of sciences. Additional concerns of participants emerged in terms of research methodology applied in the digital world (i.e., data validity, credibility and research ethics). Finally, participants identified open science and the transparency of research as the key to the future development of social sciences. © 2021 the author(s). This open access article is distributed under a creative commons attribution (cc-by) 4.0 license.","","","2021","10.1080/23311886.2021.1970880","","","scopus-2-s2.0-85114307563.pdf","scopus-2-s2.0-85114307563"
